%File: formatting-instructions-latex-2026.tex
%release 2026.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{amsfonts}
\usepackage{amsmath}
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}


%to be removed packages:
\usepackage{xcolor}
%\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}
\usepackage{booktabs}
% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai2026.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Influencing the Divide: Political Polarization in Indian Influencer Discourse on X}
%\author {
    % Authors
    % Ziv Bosco Barretto\textsuperscript{\rm 1},
    % Agriya Yadav\textsuperscript{\rm 1},
    % Kyra Chettri\textsuperscript{\rm 1},
    % Shailly\textsuperscript{\rm 2},
    % Joyojeet Pal\textsuperscript{\rm 2},
    % Anirban Sen\textsuperscript{\rm 1}
%}
%\affiliations {
    % Affiliations
    % \textsuperscript{\rm 1}Ashoka University\\
    % \textsuperscript{\rm 2}University of Michigan\\
    % {ziv.barretto\_ug23@ashoka.edu.in}, agriya.yadav\_ug2023@ashoka.edu.in, kyra.chettri\_ug23@ashoka.edu.in,\\
    % shailly@affiliation.com, joyojeet@umich.edu, anirban.sen@ashoka.edu.in
%}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
This paper investigates partisan discourse on X (formerly Twitter), examining how political identities and affiliations shape online communication patterns. We analyze a large-scale dataset of tweets retweeted by Indian politicians from 2020-2023 to understand political polarization in social media. Our methodology combines influencer polarity scoring based on retweet behavior, automated keyword extraction using KeyBERT, and stance classification using a LoRA-adapted large language model. We identify politically polar keywords and classify tweet stances toward these topics to reveal patterns in how ruling and opposition parties engage with different political narratives.
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
% You must keep this block between (not within) the abstract and the main body of the paper.
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}

\section{Introduction}


The rise of social media platforms has fundamentally transformed political communication and discourse. X (formerly known as Twitter) has emerged as a critical platform for political discussion, serving as a space where politicians, journalists, activists, and citizens engage in real-time conversations about political issues. However, this democratization of political discourse has coincided with growing concerns about political polarization and the formation of ideological echo chambers as pointed out by several recent research studies \cite{overgaard2024manifestation,pournaki2025influencers,sen2024networks}.

This polarization has several detrimental effects on the information ecosystem, the nature of electoral politics, and by extension, on society at large. Polarization leads to echo chambers that undermine the spread and acceptance of factual information, or even the cross-pollination of opposing viewpoints in communities \cite{cinelli2021echo}. This in turn has been shown to impact the judgment of voters on social issues during elections. \cite{faris2017partisanship}
The problem is further compounded by the more effective online motility of  malinformation and toxic content, resulting in an undesirable impact on democracy \cite{whatsappfake}. More often than not, these echo chambers are fortified when the information is generated by an influential figure (a prominent politician, business person, celebrity, etc.) with a significant network reach and follower count \cite{goodwin2023political}. 



With the increasing accessibility of digital technologies, a second tier of micro-influencers (hereafter referred to as \textit{influencers}) has emerged as a significant contributor to polarization \cite{woolley2022digital}. Many of these individuals primarily derive their livelihoods from social media platforms and, in some cases, attain levels of popularity comparable to or exceeding those of traditional public figures, which has in turn made influencing an important element in the aspirational sphere of young Indians\cite{mukherjee2025aspirational}. 
Content generated or disseminated by these influencers reaches a substantial audience, and consequently, ideological or opinion-based polarization among them further amplifies and compounds broader societal polarization. The political and social impact of these digital influencers can be estimated by the fact that influencer generated content accounted for around 36 \% of all user actions across major social platforms (Facebook, Instagram, X, TikTok) in April 2025, reflecting their mass global engagement \cite{socialsamosa2025influencerreport. Political content alone drove over 30 \% of influencer interactions in India. Additionally, the influencer base seems to be ever-growing on social media, with India alone seeing a significant rise in number of influencers starting 2020 \cite{influencerrise}.

This paper investigates the intersection between  influencing and polarization.  We study the partisan influencer discourse on X, examining if and how political leaning of influencers determine their online communication patterns on various social and political issues. The primary research question we intend to address in this work is: \textit{How does the political discourse of influencers on X vary on different social and political issues based on their political leaning?}

We currently focus on the Indian political context, although our methods are generalizable to any geography. We first develop a unique proxy for political leaning of influencers based on how politicians from the centrally ruling and opposition parties engage with their content. Next, we develop an \textit{Aspect based Stance Classifier} based on a small language model (\textit{Mistral-7B})\cite{jiang2023clip}, which analyzes the stance of the influencer tweets corresponding to the primary aspects/subjects contained in them. We also carry out our analysis separately for the tweets written in English and Hindi, to compare our findings across the linguistic spectrum.

Our findings empirically reveal that Indian influencers exhibit bipolar polarization with respect to a wide range of social and political issues, as identified by the aspects present in their tweets. Furthermore, an influencer’s political leaning is strongly correlated with the stance they adopt when tweeting on a given aspect. These observations raise several important questions and directions for future work, including how such polarization evolves over time, the mechanisms through which influencers shape and reinforce audience beliefs, and the extent to which polarized influencer discourse affects information diffusion and opinion formation among social media users. Future research could also investigate causal relationships between influencer stances and shifts in public opinion, as well as explore intervention strategies aimed at mitigating the amplification of polarization in social media ecosystems.

\section{Related Work}
Social media has become an indispensable source for news and information consumption in the last decade \cite{pew_social_media_news_2025, toi_reuters_digital_news_india_2024}. A significant fraction of users stay updated with current affairs through social media, which leads to its impact on public opinion. A large body of work in recent years has also shown the pervasiveness of social media consumption in both elections and in the general political discourse, showing that social media content plays a pivotal role in the functioning of present-day democracy \cite{levy2021social,park2024exposure}. There are two elements of social media impact that are relevant to our work here -- first, from a sociological perspective, the ways in which polarization is impacting society, and from a technical perspective, how we approach understanding the issue of polarization -- ie through stance classification. 

\subsection{Social Media Polarization}
While originally envisioned as a tool to broaden the horizons of knowledge and ideas, social media has contributed to an increasingly polarized and fragmented global society \cite{cicchini2022news,kofi_annan_social_media_polarization,cinelli2020echo,chitra2019understanding}. Polarization leads to imbalance in opinion formation, hindering individuals from obtaining a holistic view of salient issues. Numerous examples of the detrimental social impact causes by a polarized social media have been highlighted in prior studies\cite{garimella2017long,conover2011political}. With the increasing penetration of digital technologies, policy-makers, think-tanks, and governments are increasingly planning necessary interventions to tackle the undesirable effect social media brings on the society \cite{oecd_facts_not_fakes_2024,carnegie_countering_disinformation_2024}. 

Furthermore, the impact of polarization is often accentuated by \textit{micro-influencers} -- social media creators with dedicated following focused on a specific niche. These micro-influencers (or influencers) usually have a significantly high number of regular followers forming closely knit communities. Prior work has sufficiently exemplified cases where these influencers have promoted certain political agenda through highly partisan content posted through their social media accounts \cite{reinikainen2025convergence}. Owing to their network reach and impact, such content is consumed by their follower base, strengthening the pre-existing echo chambers. Similar trends have also been observed in India \cite{dash2022divided}, which forms the third highest user base for X, capturing around 36\% of all social media interactions globally across platforms \cite{socialsamosa2025influencerreport}.  

Social media polarization has been studied using both content \cite{falkenberg2022growing,nordbrandt2023affective,SUN2023113845} and network analysis methods \cite{esteve2022homophily,candellone2025negative,peralta2024multidimensional,singh2025multi,pournaki2025influencers,zhang2025polarization} in extant literature. %Falkenberg et al. \cite{falkenberg2022growing} study the discussion around the UN Conference of the Parties on Climate Change (COP) using Twitter data, and reveal a large increase in ideological polarization during COP26, contrary to the period between COP20 and COP25.  Overgaard et al. \cite{overgaard2024manifestation} measure affective polarization by looking at the expressed negativity towards certain actors through user comments. Maria Nordbrandt \cite{nordbrandt2023affective} uses Dutch panel data to show that the level of affective polarization affects subsequent use of social media. Sun et al. \cite{SUN2023113845} perform sentiment analysis of 3600 posts on Weibo, and find that content ideology and symbolic expressions are positively related to social media opinion polarization. Munoz et al. \cite{munoz2024quantifying} present a comparative analysis of polarization measurement methods on X during Spanish election cycles, and propose a novel algorithm for capturing polarization in political content.
The connection between partisanship in social media content and polarization has been well established in prior work. Demszky et al. \cite{demszky2019analyzing} use linguistic and semantic content analysis show that polarization is affected mainly by partisan differences in content framing. Flamino et al. \cite{flamino2023political} analyze tweets to examine how partisan news content circulated by media and influencer accounts increased ideological polarization between the 2016 and 2020 US elections. Dash et al. \cite{dash2022divided} quantify how influences engage with politically charged content in a partisan manner, showing that ideological polarization among influencers is linked to increased engagement and retweeting, thus amplifying partisan dynamics.

% Pournaki et al. \cite{pournaki2025influencers} measure political polarization by using topic modeling to identify issues and network analysis (Stochastic Block Models on retweet networks) to cluster users into global left-and right-leaning opinion camps. Zhang et al. \cite{zhang2025polarization} construct user opinion networks, and quantify polarization on social media through a random walk based approach.

Motivated by prior work, we study the political discourse around Indian influencers to assess the level of partisanship and polarization in it. Specifically, we define a set of polar aspects/topics around which the stance of influencer tweets are analyzed. We additionally see how these stance signatures relate to the political leanings of these influencers. %are henceforth For this purpose, we first manually annotate a set of influencer generated (tweet,aspect) pairs to create a dataset for aspect based stance classification. It is ensured that these tweets are politically endorsed, i.e., have been retweeted at least once by a political entity. Next, we use this dataset to fine-tune a small language model (Mistral-7B) to develop an aspect based stance classifier. While existing works have focused on various stance classification methods including XXX [ref], XXX [ref], and XXX [ref], we use an SLM based approach to ensure that our framework is generalizable to low resource settings. We also evaluate partisanism in influencer generated content in both English and Hindi, to establish robustness of our findings.
\subsection{Stance Classification}
Stance classification has long been recognized as a challenging NLP task involving the identification of a speaker’s holistic subjective disposition toward a topic or aspect. Prior work has shown that stance is not reducible to sentiment \cite{anand-etal-2011-cats}, and requires leveraging of significantly more nuanced linguistic features. Stance classification methods include both unsupervised \cite{gambini2022tweets2stance}, and supervised \cite{zarrella2016mitre,mohammad2017stance,xu2018cross,kuccuk2018stance} approaches. Prior work has shown that transformer-based approaches often substantially outperform classical approaches for stance detection on social media data \cite{glandt-etal-2021-stance,karande2021stance}. Recent work has additionally examined the use of LLMs for stance classification \cite{ding2024cross}, showing that LLM performance is highly sensitive to prompt design and that parameter-efficient fine-tuning such as LoRA does not consistently outperform zero-shot prompting \cite{cruickshank2023prompting,ma2024chain}. Prior work has also shown that effective stance classification requires rich representations and contextual modeling beyond surface lexical features \cite{hasan-ng-2013-stance}. Our work operationalizes these insights by extending this line of research to large-scale political influencer discourse, integrating LLM-based stance reasoning of multilingual tweet data. Specifically, we test the performance of both basic transformer and LLM based approaches (zero-shot, few-shot, and fine-tuned) in the task of stance classification, on influencer generated political tweets.

\section{Data Collection}
We begin by creating a list of 27,916 Indian politicians’ X usernames and party affiliations from the NivaDuck database \cite{panda2020nivaduck} and a list of
11,578 influencer usernames from the DISMISS database \cite{arya2022dismiss}. We use these two lists to extract their tweets and
retweets from the Twitter U.S. and India Politicians dataset \cite{mukherjee2024political}, for the years 2020-2023 while
storing the tweet content, user information (such as profile description), dates, hashtags, and retweet counts. %We also obtain the follower counts for influencers through the DISMISS dataset. For politicians, we obtain the party affiliations from NivaDuck and follower counts from the tweet database. 
Finally, we use the influencer tweets, and
information about the politicians who retweeted them, for all years to conduct our quantitative analysis. Our final dataset contains tweets and retweets for a total of 960 influencers. We aggregated the 50 political parties obtained from the politician affiliations into two major blocs of ``ruling" and ``opposition". %as shown in Table~\ref{tab:party_blocs}.We aggregate the 50 political parties obtained from the politician affiliations into two major blocs as shown in Table~\ref{tab:party_blocs}.

Political parties were classified into these two blocs based on their alignment during the study period (2020–2023), recognizing the fluidity of party alliances in India. Party positions were determined through triangulation of multiple sources, including official government documents, party press releases, verified party and leader social media accounts, and newspaper reporting. 
% We utilize a dataset of tweets provided by Professor Joyojeet Pal, comprising retweets made by Indian politicians on X (Twitter) during the period 2020--2023. The dataset contains over 7.1 million retweet records, capturing the amplification behavior of politicians across the Indian political spectrum. Each record represents a retweet action containing the original tweet content, timestamp, retweeting politician's X handle, and the original author (influencer) whose content was retweeted.
%To associate politicians with their party affiliations, we utilize an external mapping file containing X handles and corresponding party memberships. This mapping covers politicians from over 50 political parties, which 
%\%vspace{0.5em}
% \begin{center}
% \footnotesize
% \captionof{table}{Political Party Bloc Mapping}
% \label{tab:party_blocs}
% \begin{tabular}{p{3.2cm}p{3.2cm}}
% \toprule
% \textbf{Ruling Bloc} & \textbf{Opposition Bloc} \\
% \midrule
% BJP, JDU, LJP, HAM, JSP, NPP, AIADMK, AJSU, AGP, RPI, NPF, IPFT, NDPP, RSS, NDA, MNF, VHP, YSRCP, ABVP, BJD, BSCP & INC, AITC/TMC, DMK, SP, CPIM, RJD, AAP, JMM, IUML, CPI, NCP, TRS, Shiv Sena, TDP, JDS \\
% \bottomrule
% \end{tabular}
% \end{center}
%\vspace{0.5em}
%The data processing pipeline consists of two stages. In the first stage (party labeling), we process the raw CSV in chunks of 100,000 rows and match each retweeting politician's handle to their party affiliation using lowercase normalization, producing a labeled dataset with the \texttt{retweet\_party} column. In the second stage (polarity calculation), we compute yearly polarity scores for each influencer based on who retweets their content, then average across years to obtain a stable polarity measure, identifying 960 unique influencers with calculable scores. 
%\vspace{0.5em}
% \begin{center}
% \small
% \captionof{table}{Dataset Statistics}
% \label{tab:dataset_stats}
% \begin{tabular}{lr}
% \toprule
% \textbf{Metric} & \textbf{Value} \\
% \midrule
% Total retweets & 7,115,963 \\
% English tweets & 2,939,716 (41.3\%) \\
% Hindi/Regional tweets & 4,176,247 (58.7\%) \\
% Unique influencers (with polarity) & 960 \\
% Influencer-year observations & 3,712 \\
% Temporal range & 2020--2023 \\
% \bottomrule
% \end{tabular}
% \end{center}
% %\vspace{0.5em}

{\color{blue} Our dataset comprises 7,115,963 total retweets spanning the years 2020--2023, of which 2,939,716 (41.3\%) are in English and 4,176,247 (58.7\%) are in Hindi or other regional languages. The dataset covers 960 unique influencers with calculable polarity scores, yielding 3,712 influencer-year observations across the four-year temporal range.
}

The temporal distribution of number of unique influencers tweeting per year is mostly uniform, ranging between 908-949 influencers tweeting per year. %(Table~\ref{tab:temporal_dist}) shows consistent activity across all four years, with relatively balanced influencer-year records ranging from 908 to 949 per year.
% %\vspace{0.5em}
% \begin{center}
% \small
% \captionof{table}{Temporal Distribution of Tweets}
% \label{tab:temporal_dist}
% \begin{tabular}{lrr}
% \toprule
% \textbf{Year} & \textbf{Influencer-Year Records} \\
% \midrule
% 2020 & 908 \\
% 2021 & 944 \\
% 2022 & 949 \\
% 2023 & 911 \\
% \bottomrule
% \end{tabular}
% \end{center}
% %\vspace{0.5em}
%Table~\ref{tab:columns} describes the key columns in our final processed dataset.
Our original dataset contains the following columns: \textit{Timestamp, tweet text, retweet author, tweet author, and political party of retweet author}. We also have tweets in multiple languages, including Hindi and regional Indian languages. For the English analysis pipeline, we filter tweets using an ASCII-ratio heuristic: a tweet is classified as English if at least 85\% of its alphabetic characters are ASCII letters, providing computationally efficient language detection for large-scale processing.

%\vspace{0.5em}
% \begin{center}
% \footnotesize
% \captionof{table}{Dataset Column Descriptions}
% \label{tab:columns}
% \begin{tabular}{p{2.2cm}p{4.3cm}}
% \toprule
% \textbf{Column} & \textbf{Description} \\
% \midrule
% \texttt{timestamp} & Date and time of the retweet (UTC) \\
% \texttt{tweet} & Full text content of original tweet \\
% \texttt{retweet\_author} & X handle of retweeting politician \\
% \texttt{original\_author} & X handle of influencer \\
% \texttt{retweet\_party} & Political party of retweeter \\
% \texttt{year} & Year extracted from timestamp \\
% \texttt{side} & Political bloc (ruling/opposition/other) \\
% \texttt{polarity\_avg} & Influencer's average polarity score \\
% \texttt{tweet\_label} & Inferred political leaning of tweet \\
% \bottomrule
% \end{tabular}
% \end{center}
% %\vspace{0.5em}
\section{Methodology}
\subsection{Influencer Polarity}
We calculate an influencer's political leaning (also termed as \textit{polarity}) using the amount of endorsement their tweets receive from Indian politicians with known party affiliations. Generally, the retweet behavior of politicians reveals their endorsement towards a tweet \cite{metaxas2015retweets}. %Thus, by aggregating retweet patterns across politicians with known party affiliations, we can infer the political leaning of influencers. 
This methodology provides a more reliable signal of endorsement than passive following or hashtag co-occurrence, since hashtags can be appropriated or used sarcastically, while generic sentiment analysis methods struggle with political nuance and sarcasm.

For each influencer $i$ and year $y$, we calculate a yearly political polarity score using the formula in Equation~\ref{eq:polarity} where $R_{i,y}$ represents the count of retweets the influencer received from ruling politicians in year $y$, and $O_{i,y}$ represents the count from opposition politicians.

\begin{equation}
P_{i,y} = \frac{R_{i,y} - O_{i,y}}{R_{i,y} + O_{i,y}}
\label{eq:polarity}
\end{equation}

The polarity score ranges from $-1$ (exclusively retweeted by opposition) to $+1$ (exclusively retweeted by ruling). %The computation involves grouping data by influencer, year, and political side, then pivoting to create ruling and opposition columns with missing values filled as zero.
We next compute an average polarity score across all available years using Equation~\ref{eq:polarity_avg}, where $Y_i$ is the set of years in which influencer $i$ was retweeted by at least one politician.

\begin{equation}
P_i^{avg} = E_y[P_{i,y}] = \frac{1}{|Y_i|} \sum_{y \in Y_i} P_{i,y}
\label{eq:polarity_avg}
\end{equation}

Based on the average polarity score, influencers are classified into three categories using a threshold of 0.5 (experimentally determined). Influencers with $P_i^{avg} \geq 0.5$ are classified as Pro-Ruling, those with $P_i^{avg} \leq -0.5$ as Pro-Opposition, and those within $-0.5 < P_i^{avg} < 0.5$ as Neutral. %This classification is propagated to all tweets authored by each influencer, effectively labeling tweets based on their author's political leaning.

Two independent annotators manually evaluated influencer polarities to validate the results (by checking 20 tweets per-influencer), achieving high inter-annotator agreement (95\% unanimous agreement). Disagreements were resolved using majority voting. %Table \ref{tab:inflexample} shows some examples of influencers and their polarities.
% requires: \usepackage{xcolor}
% \begin{table}[t]
% {\begin{tabular}{|l|r|l|}
% \hline
% \textbf{original\_author} & \textbf{polarity\_avg} & \textbf{label\_0\_5} \\
% \hline
% Advaidism        & -0.938282   & Pro OPP    \\
% BrutIndia        & -0.7901837  & Pro OPP    \\
% FarahKhanAli     & -0.9794872  & Pro OPP    \\
% NargisBano\_     & -0.9646005  & Pro OPP    \\
% SonuSood         & -0.521716   & Pro OPP    \\
% AnupamPKher      & 0.89765928  & Pro Ruling \\
% BabitaPhogat     & 0.8252169   & Pro Ruling \\
% BloomsburyIndia  & 0.60277778  & Pro Ruling \\
% DivyaSoti        & 0.95337017  & Pro Ruling \\
% DDNewslive       & 0.93212181  & Pro Ruling \\
% CNBCTV18News     & 0.10645253  & Neutral    \\
% Neeraj\_chopra1  & 0.09111461  & Neutral    \\
% NewsX            & 0.42678028  & Neutral    \\
% maryashakil      & 0.18952428  & Neutral    \\
% kiranshaw        & -0.1879728  & Neutral    \\
% \hline
% \end{tabular}}
% \caption{Example influencers with their polarity scores ($P_i^{avg}$ and their political leanings)}
% \label{tab:inflexample}
% \end{table}
\subsection{Aspect Identification}
To identify the topical keyphrases or \textit{aspects} discussed in influencer tweets, we employ KeyBERT\cite{grootendorst2020keybert} for automated aspect extraction from tweet text. KeyBERT leverages BERT embeddings to identify aspects and phrases that are semantically similar to the document as a whole, making it suitable for capturing the topical essence of short-form social media content.

Before aspect extraction, we preprocess the tweets to remove URLs, strip the "@" character while retaining usernames, perform hashtag segmentation using the \texttt{wordsegment} library (e.g., \texttt{\#AatmaNirbharBharat} becomes \textit{aatma nirbhar bharat}), and normalize whitespaces.

The extraction process uses the \texttt{paraphrase-} \texttt{multilingual-MiniLM-L12-v2}\cite{reimers2019sentence} sentence transformer model as the KeyBERT backend. We configure n-gram range of 1-2, and apply Maximal Marginal Relevance (MMR) with diversity 0.7 to balance semantic similarity with aspect diversity.

\textbf{Seed Polar Aspect Identification:} We detect partisanship in influencer tweets by checking if influencers with a certain political polarity preferentially tweet around certain politically polar aspects. For this purpose, we analyze aspects extracted using KeyBERT to identify polar aspects -- topics that exhibit disproportionately strong polarity in public political discourse. With the help from an expert in Political Science, we selected 15 aspects representing contentious and significantly discussed political issues, forming the seed set of polar aspects. Notably, all seed aspects fall within the top 98th percentile of the tweet frequency distribution.
\begin{table}[]
    \centering
    \scriptsize
    \caption{Complete Aspect Set for Stance Analysis}
    \label{tab:all_aspects}
    \begin{tabular}{llll}
    \midrule
    \multicolumn{4}{c}{\textbf{15 Seed Aspects (in lowercase)}} \\
    \midrule
    caa & congress & farm\_laws & farmers\_protests \\
    hindu & hindutva & kashmir & kashmiri\_pandits \\
    modi & muslim & new\_parliament & rahulgandhi \\
    ram\_mandir & shaheen\_bagh & china & \\
    \midrule
    \multicolumn{4}{c}{\textbf{20 Extended Aspects (in lowercase)}} \\
    \midrule
    aatmanirbhar & ayodhya & balochistan & bhakts \\
    democracy & demonetisation & dictatorship & gdp \\
    hathras & inflation & islamists & lynching \\
    mahotsav & minorities & msp & unemployment \\
    sangh & sharia & spyware & suicides \\
    \midrule
    \end{tabular}
\end{table}
% \begin{center}
% \scriptsize
% \captionof{table}{Complete Aspect Set for Stance Analysis}
% \label{tab:all_aspects}
% \begin{tabular}{llll}
% \toprule
% \multicolumn{4}{c}{\textbf{15 Seed Aspects (in lowercase)}} \\
% \midrule
% caa & congress & farm\_laws & farmers\_protests \\
% hindu & hindutva & kashmir & kashmiri\_pandits \\
% modi & muslim & new\_parliament & rahulgandhi \\
% ram\_mandir & shaheen\_bagh & china & \\
% \midrule
% \multicolumn{4}{c}{\textbf{20 Extended Aspects (in lowercase)}} \\
% \midrule
% aatmanirbhar & ayodhya & balochistan & bhakts \\
% democracy & demonetisation & dictatorship & gdp \\
% hathras & inflation & islamists & lynching \\
% mahotsav & minorities & msp & unemployment \\
% sangh & sharia & spyware & suicides \\
% \bottomrule
% \end{tabular}
% \end{center}
To get an initial sense of partisanship, we examined the proportion of tweets from pro-ruling and pro-opposition sources for each selected aspect (using Nivaduck), and categorized the aspect as pro-ruling/opposition based on which side tweeted more about it. Aspects like \textit{aatmanirbhar} (self-reliance campaign), \textit{ayodhya}, and \textit{mahotsav} are found to be predominantly used by Pro-Ruling sources ($>74\%$), while \textit{unemployment}, \textit{demonetisation}, \textit{bhakts}, and \textit{spyware} are predominantly discussed by Pro-Opposition sources ($>80\%$). These findings align closely with the assumptions articulated by the political science expert.
{\color{black}

\textbf{Extended Aspect Selection:} Next, to expand coverage, we systematically analyzed aspect frequencies and identified top 20 additional polar aspects exhibiting clear partisan skew. Table~\ref{tab:all_aspects} presents the complete set of 35 aspects used for stance analysis.
KeyBERT extracted a total of 96,477 unique aspects across the entire corpus. To systematically identify additional polar aspects, we ranked all aspects by their total tweet frequency and retain only those in the top 95th percentile. This threshold yielded 9,647 candidate aspects. We then filtered out aspects containing less than 300 tweets in total. %covering over 7.5 million tweets.%, ensuring sufficient data volume for downstream stance classification. 
Finally, from this filtered set, the domain expert qualitatively reviewed and selected the polar aspects based on the same criteria used to select the seed aspects. %Figure~\ref{fig:aspect_distribution} illustrates the distribution of all aspects by tweet volume, highlighting the position of the extended aspects in red.%: (A) political relevance to contemporary Indian discourse, (B) evidence of partisan polarization (clear skew toward either pro-ruling or pro-opposition sources), and (C) representation from both ideological camps to ensure balanced coverage across the political spectrum. 
%To ensure balance in the volume of tweets extracted across aspects, we extracted all tweets containing the aspect as a aspect/keyphrase, irrespective of whether the aspect was among the top three aspects listed by KeyBERT.
%This selection rationale ensures that the chosen aspects have adequate tweet volume from which KeyBERT initially labeled instances; the main extraction process subsequently retrieves \textit{all} tweets containing each aspect term, regardless of whether KeyBERT had explicitly labeled that tweet with that aspect.
 %All selected aspects fall within the top 6.1\% of aspects by volume (ranks 50 to 5,895 out of 96,477).
% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.9\linewidth]{results_eng/keyword_distribution_improved.png}
%   \caption{Aspect distribution showing tweet volume across all 96,477 aspects extracted by KeyBERT. The 20 target extended aspects (red dots) are highlighted, spanning ranks 50 to 5,895, all within the top 5\% threshold.}
%   \label{fig:aspect_distribution}
% \end{figure}
Table~\ref{tab:extended_aspects} presents the complete set of 20 extended aspects with their KeyBERT-labeled tweet counts, partisan distribution, and percentile ranking.

% \begin{center}
% \scriptsize
% \captionof{table}{Extended Aspects: KeyBERT-Labeled Tweet Distribution}
% \label{tab:extended_aspects}
% \begin{tabular}{lrrrr}
% \toprule
% \textbf{Aspect} & \textbf{Total Tweets} & \textbf{Pro-Ruling} & \textbf{Pro-Opp} & \textbf{Percentile} \\
% \midrule
% democracy & 12,763 & 4,609 & 6,772 & 99.95\% \\
% ayodhya & 5,912 & 4,146 & 932 & 99.83\% \\
% unemployment & 4,296 & 521 & 3,233 & 99.75\% \\
% inflation & 3,431 & 967 & 1,679 & 99.65\% \\
% mahotsav & 2,854 & 2,758 & 57 & 99.56\% \\
% islamists & 2,085 & 2,018 & 12 & 99.36\% \\
% lynching & 1,775 & 1,208 & 449 & 99.23\% \\
% minorities & 1,614 & 640 & 776 & 99.15\% \\
% hathras & 1,457 & 343 & 982 & 99.02\% \\
% bhakts & 1,359 & 139 & 1,197 & 98.94\% \\
% spyware & 1,314 & 98 & 1,109 & 98.89\% \\
% gdp & 994 & 244 & 678 & 98.49\% \\
% balochistan & 957 & 919 & 8 & 98.45\% \\
% aatmanirbhar & 883 & 780 & 43 & 98.32\% \\
% msp & 741 & 516 & 141 & 98.03\% \\
% dictatorship & 617 & 100 & 409 & 97.64\% \\
% sharia & 605 & 572 & 17 & 97.59\% \\
% demonetisation & 555 & 57 & 420 & 97.39\% \\
% % ratetvdebate & 509 & 0 & 509 & 97.16\% \\
% suicides & 504 & 86 & 381 & 97.14\% \\
% sangh & 300 & 219 & 78 & 95.66\% \\
% % ucc & 187 & 102 & 39 & 93.89\% \\
% \bottomrule
% \end{tabular}
% \end{center}
% }

\begin{table}[t]
\centering
\scriptsize
\caption{Extended Aspects: KeyBERT-Labeled Tweet Distribution}
\label{tab:extended_aspects}
\begin{tabular}{lrrrr}
\toprule
\textbf{Aspect} & \textbf{Total Tweets} & \textbf{Pro-Ruling} & \textbf{Pro-Opp} & \textbf{Percentile} \\
\midrule
democracy & 12,763 & 4,609 & 6,772 & 99.95\% \\
ayodhya & 5,912 & 4,146 & 932 & 99.83\% \\
unemployment & 4,296 & 521 & 3,233 & 99.75\% \\
inflation & 3,431 & 967 & 1,679 & 99.65\% \\
mahotsav & 2,854 & 2,758 & 57 & 99.56\% \\
islamists & 2,085 & 2,018 & 12 & 99.36\% \\
lynching & 1,775 & 1,208 & 449 & 99.23\% \\
minorities & 1,614 & 640 & 776 & 99.15\% \\
hathras & 1,457 & 343 & 982 & 99.02\% \\
bhakts & 1,359 & 139 & 1,197 & 98.94\% \\
spyware & 1,314 & 98 & 1,109 & 98.89\% \\
gdp & 994 & 244 & 678 & 98.49\% \\
balochistan & 957 & 919 & 8 & 98.45\% \\
aatmanirbhar & 883 & 780 & 43 & 98.32\% \\
msp & 741 & 516 & 141 & 98.03\% \\
dictatorship & 617 & 100 & 409 & 97.64\% \\
sharia & 605 & 572 & 17 & 97.59\% \\
demonetisation & 555 & 57 & 420 & 97.39\% \\
% ratetvdebate & 509 & 0 & 509 & 97.16\% \\
suicides & 504 & 86 & 381 & 97.14\% \\
sangh & 300 & 219 & 78 & 95.66\% \\
% ucc & 187 & 102 & 39 & 93.89\% \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Stance Classification}
The goal of stance classification is to determine how influencers position themselves towards specific political aspects. Given a tweet $t$ and a target aspect $e$ %(such as \textit{CAA} or \textit{Farmers Protests} that are nationally prominent policy issues)
, the system predicts one of three stance labels: \textit{Favor} (expressing support), \textit{Against} (expressing opposition), or \textit{Neutral} (no clear stance). %This three-way classification avoids the ambiguity of an ``unrelated'' category, which in practice often overlaps with neutral stances.
Motivated by prior work, we test multiple stance classification approaches on our data, ranging from transformer based models (PyABSA{\color{blue}\cite{YangZL23}}, BERT and RoBERTa, which act as our baselines) to zero-shot, few-shot, and fine-tuned versions of an SLM (Mistral-7B)\cite{jiang2023clip}. A comparative analysis of these stance classification approaches is present in the Results section.
\subsubsection{Training Data}
We created a rigorously manually annotated stance classification dataset for Indian influencer discourse on political issues. %Unlike prior work that often relied on weakly-supervised or automatically generated labels [ref], we invested significant effort in creating high-quality ground truth data to serve as a reliable benchmark. 
This dataset plays a central role in our pipeline: It provides the core supervision signal for fine-tuning our baseline discriminative models (BERT, RoBERTa, PyABSA), serves as the source of examples used in our few-shot prompts, and for fine-tuning the SLM. A section of the data is also used as the gold-standard held-out test set for rigorous performance evaluation of our models.

The training dataset creation involved a multi-stage process beginning with stratified sampling. For each of the 15 seed aspects, we sampled 150 tweets, ensuring a balanced representation of viewpoints from both Ruling and Opposition sources. This stratification technique is crucial as it prevents the model from learning specious correlations between political affiliation and stance, such as assuming all opposition tweets are inherently "Against" the aspect. Following the sampling, deduplication was applied to remove duplicate tweets. %ensuring the dataset captures diverse linguistic expressions rather than repetitive coordinated behavior. 
Finally, each tweet was manually annotated with its stance toward the specific target aspect by two independent annotators possessing domain expertise in contemporary political affairs. %Statements that were ambiguous, purely factual, or lacked a clear evaluative stance were labeled as Neutral. 
Inter-annotator reliability was high, with a pairwise agreements of 87\% and 95\%., indicating the robustness of the annotation process. Disagreements were resolved using majority voting.

% {\color{blue} Following the sampling, deduplication was applied within each aspect using exact string matching on the tweet text; tweets with identical content were removed, retaining only the first occurrence per aspect. Notably, the same tweet may appear under multiple aspects if it references multiple topics, preserving the natural multi-topic nature of political discourse. }

The final dataset comprises 1,944 labeled tweet-aspect pairs. We adopted a strict train-test split strategy where 265 samples were set aside as a held-out test set (stratified by aspect and class). From the remaining training pool, we reserved 10 examples per aspect (150 total) to serve as few-shot demonstrations, and utilized the rest (1,529 samples) for supervised training of baseline models.
\subsubsection{Models Compared} To rigorously evaluate the effectiveness of our approach, we compared multiple stance classification approaches, whose details are present in the appendix. %whose details are present in table~\ref{tab:model_comparison}.
% \begin{center}
% \footnotesize
% \captionof{table}{Models Used for Stance Classification Comparison}
% \label{tab:model_comparison}
% \begin{tabular}{p{2.8cm}p{1.8cm}p{2.2cm}}
% \toprule
% \textbf{Model} & \textbf{Type} & \textbf{Method} \\
% \midrule
% \textbf{BERT-base-uncased} (Baseline-1) & Encoder & Full Fine-tuning \\
% \textbf{RoBERTa-base} (Baseline-2) & Encoder & Full Fine-tuning \\
% \textbf{PyABSA (FAST\_LCF)} (Baseline-3) & ABSA & Aspect-focused Fine-tuning \\
% \textbf{Mistral-7B-Instruct} & Decoder & Zero-shot / Few-shot \\
% \textbf{Mistral-7B + LoRA} & Decoder & LoRA Fine-tuning \\
% \bottomrule
% \end{tabular}
% \end{center}
% %\vspace{0.5em}
The three baselines against which we compare our approach of stance classification using the SLM are BERT-base-uncased{\color{blue}\cite{devlin2019bert}}, RoBERTa-base{\color{blue}\cite{liu2019roberta}}, and PyABSA \cite{YangZL23}. %For the BERT baseline, we fine-tuned the model using an aspect-aware input format: \texttt{[CLS] Aspect [SEP] Tweet [SEP]} where CLS is the special classification token whose contextualized representation is used for downstream prediction, while SEP is the separator token. This allows the self-attention mechanism to directly model the relationship between the target topic and the tweet content. Similarly, for \texttt{roberta-base}, we employed a prompt-like input format by explicitly prepending the context: ``Topic: \{Aspect\}. Tweet: \{Content\}'', leveraging RoBERTa's strong semantic understanding. 
We formulate stance detection as a 3-way single-label classification task with label set $\{\textsc{For},\textsc{Against},\textsc{Neutral}\}$ (mapped to IDs \{0,1,2\}). For the BERT baseline, we fine-tuned \texttt{BertForSequenceClassification} using (aspect,tweet) pairs. The model uses the final hidden state of the \texttt{[CLS]} token ($\mathbf{h}_{\texttt{[CLS]}}\in\mathbb{R}^{768}$) and a linear classification head ($\mathbf{W}\in\mathbb{R}^{768\times3}$) to compute logits $\mathbf{z}\in\mathbb{R}^{3}$. We fine-tuned \texttt{RobertaForSequenceClassification} using an explicit prompt-like input string of the form \texttt{``Topic: \{aspect\}. Tweet: \{tweet\}"}. RoBERTa computes a sequence representation from the start token \texttt{<s>} (RoBERTa's \texttt{[CLS]} equivalent) and applies the standard RoBERTa classification head (dropout $\rightarrow$ dense layer $\rightarrow$ tanh $\rightarrow$ dropout $\rightarrow$ output projection) to produce logits $\mathbf{z}\in\mathbb{R}^{3}$. Finally, we utilized the specialized PyABSA framework's \texttt{FAST\_LCF\_BERT} model, which uses a Local Context Focus (LCF) mechanism to dynamically weight context words based on their semantic and syntactic distance to the aspect.

% For the BERT baseline, we fine-tuned \textit{BertForSequenceClassification} [ref] using (aspect,tweet) pairs. The tokenizer %as input via the tokenizer call \texttt{tokenizer(aspect, text, ...)},
% yields the sequence \texttt{[CLS] aspect [SEP] tweet [SEP]} with standard attention masking and padding/truncation to scale the input to a maximum length of 256 tokens. The model uses the final hidden state of the \texttt{[CLS]} token ($\mathbf{h}_{\texttt{[CLS]}}\in\mathbb{R}^{768}$) and a linear classification head ($\mathbf{W}\in\mathbb{R}^{768\times3}$) to compute logits $\mathbf{z}\in\mathbb{R}^{3}$.  During training, we minimize the cross-entropy loss and use the AdamW optimizer, %returned by \texttt{BertForSequenceClassification} (via \texttt{outputs.loss}) and optimize all parameters end-to-end using AdamW (\texttt{torch.optim.AdamW}) 
% with learning rate $2\times 10^{-5}$ and weight decay $0.01$, batch size 16, gradient clipping at max-norm 1.0, and early stopping with patience 3 based on validation macro-F1. %The learning rate is scheduled with a linear decay from the initial rate to zero across all training steps using \texttt{torch.optim.lr\_scheduler.LinearLR}. 
% We train for up to 5 epochs (based on convergence) with a stratified 90/10 train--validation split.

% We fine-tuned \textit{RobertaForSequenceClassification} [ref] using an explicit prompt-like input string of the form \texttt{``Topic: \{aspect\}. Tweet: \{tweet\}"}, tokenized with padding/truncation to a maximum length of 256 tokens. RoBERTa computes a sequence representation from the start token \texttt{<s>} (RoBERTa's \texttt{[CLS]} equivalent) and applies the standard RoBERTa classification head (dropout $\rightarrow$ dense layer $\rightarrow$ tanh $\rightarrow$ dropout $\rightarrow$ output projection) to produce logits $\mathbf{z}\in\mathbb{R}^{3}$. Training uses AdamW with learning rate $2\times 10^{-5}$ and weight decay $0.01$, batch size 16, gradient clipping at 1.0, and a linear warmup schedule for the first 10\% of training steps followed by linear decay (via \texttt{get\_linear\_schedule\_with\_warmup}). The model is trained for 3 epochs, using a 10\% validation subset sampled from the training data (fixed random seed).

% % \textit{RoBERTa (\texttt{roberta-base}):}
% % We fine-tuned HuggingFace \texttt{RobertaForSequenceClassification} using an explicit prompt-like input string of the form \texttt{``Topic: \{aspect\}. Tweet: \{tweet\}''}, tokenized with padding/truncation to a maximum length of 256 tokens. RoBERTa computes a sequence representation from the start token \texttt{<s>} (RoBERTa's \texttt{[CLS]} equivalent) and applies the standard RoBERTa classification head (dropout $\rightarrow$ dense layer $\rightarrow$ tanh $\rightarrow$ dropout $\rightarrow$ output projection) to produce logits $\mathbf{z}\in\mathbb{R}^{3}$, followed by softmax probabilities and $\arg\max$ prediction. Training uses AdamW with learning rate $2\times 10^{-5}$ and weight decay $0.01$, batch size 16, gradient clipping at 1.0, and a linear warmup schedule for the first 10\% of training steps followed by linear decay (via \texttt{get\_linear\_schedule\_with\_warmup}). The model is trained for 3 epochs, using a 10\% validation subset sampled from the training data (fixed random seed).}
% % }
% Finally, we utilized the specialized PyABSA framework's \texttt{FAST\_LCF\_BERT} model, which uses a Local Context Focus (LCF) mechanism to dynamically weight context words based on their semantic and syntactic distance to the aspect. All baseline models were trained on the exact same training split (1,529 samples) and evaluated on the same held-out test set (265 samples) to ensure a strictly fair comparison.  For each baseline, we convert the encoder output into a 3-dimensional logit vector, apply a softmax to obtain class probabilities, and predict the stance via $\arg\max$.

We also employed Mistral-7B-Instruct (with zero-shot and few-shot variations. The prompts are available in the Appendix). The base model was additionally enhanced with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning \cite{hu2022lora}. %Full fine-tuning of 7B+ parameter models is computationally prohibitive and prone to catastrophic forgetting. LoRA freezes the pre-trained weights $W$ and injects trainable rank decomposition matrices $A$ and $B$, such that the weight update is $\Delta W = BA$, where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$. We set the rank $r=16$ and alpha $\alpha=32$, targeting all linear projection layers (query, key, value, output, gate, up, down). 
The model was fine-tuned on the training split to generate structured JSON outputs containing both the stance label and a reasoning field. %The instruction-tuning approach encourages the model to perform chain-of-thought reasoning before assigning a label, improving interpretability.

% \subsubsection{Evaluation Results}
% We evaluated all models on the held-out test set of 265 samples. Table~\ref{tab:eval_overall} presents the comparative results. The LoRA-adapted Mistral model demonstrates competitive performance, effectively balancing precision and recall across classes.

% %\vspace{0.5em}
% \begin{center}
% \footnotesize
% \captionof{table}{Stance Classification Evaluation Results}
% \label{tab:eval_overall}
% \begin{tabular}{lr}
% \toprule
% \textbf{Metric} & \textbf{Value} \\
% \midrule
% Total test samples & 264 \\
% Accuracy & 78.0\% \\
% Precision (macro) & 78.0\% \\
% Recall (macro) & 75.3\% \\
% F1-Score (macro) & 76.2\% \\
% F1-Score (weighted) & 77.7\% \\
% \bottomrule
% \end{tabular}
% \end{center}
% \%vspace{0.5em}

% Table~\ref{tab:eval_perclass} shows per-class performance for the best model (fine-tuned SLM). The model performs strongest on the ``favor'' class (F1=0.82), followed by ``against'' (F1=0.79). The ``neutral'' class shows lower recall (0.60), reflecting the inherent difficulty of distinguishing neutral stances from weakly expressed opinions. We present a detailed analysis of the misclassifications in the Discussion section.

% \%vspace{0.5em}
% \begin{center}
% \footnotesize
% \captionof{table}{Per-Class Classification Performance}
% \label{tab:eval_perclass}
% \begin{tabular}{lrrrr}
% \toprule
% \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Support} \\
% \midrule
% Against & 0.78 & 0.80 & 0.79 & 95 \\
% Favor & 0.78 & 0.86 & 0.82 & 111 \\
% Neutral & 0.78 & 0.60 & 0.68 & 58 \\
% \bottomrule
% \end{tabular}
% \end{center}
% \%vspace{0.5em}

% Table~\ref{tab:eval_aspect} presents per-aspect accuracy for the seed aspects, showing that some yield clearer stance signals than others. Aspects like \textit{Modi} (name of the Prime Minister of India) achieve the highest accuracy (94.4\%), followed by \textit{CAA} (89.5\%) and \textit{Congress} (88.9\%) (the largest party in opposition).

% \%vspace{0.5em}
% \begin{center}
% \footnotesize
% \captionof{table}{Per-Aspect Classification Accuracy (Top 10)}
% \label{tab:eval_aspect}
% \begin{tabular}{lrr}
% \toprule
% \textbf{Aspect} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
% \midrule
% modi & 94.4\% & 0.91 \\
% caa & 89.5\% & 0.90 \\
% congress & 88.9\% & 0.84 \\
% hindutva & 85.7\% & 0.56 \\
% new\_parliament & 85.7\% & 0.82 \\
% rahulgandhi & 81.3\% & 0.81 \\
% farmers\_protests & 80.0\% & 0.82 \\
% shaheen\_bagh & 78.9\% & 0.71 \\
% muslim & 78.9\% & 0.79 \\
% china & 73.7\% & 0.64 \\
% \bottomrule
% \end{tabular}
% \end{center}
% \%vspace{0.5em}

%\subsubsection{Final Inference Pipeline}
To generalize our stance classification approach, we also performed an out-of-distribution testing on 20 additional aspects unseen by the model as primary aspects for tweets. For this purpose, we deploy an inference pipeline that combines the trained LoRA adapter with few-shot prompting (on the unseen aspects). %The pipeline loads the base Mistral-7B-Instruct model and applies the trained LoRA adapter using PEFT's \texttt{PeftModel}, effectively deploying our fine-tuned stance classifier without modifying the base model weights.
%{\color{blue}
%\textbf{Extended Aspect Selection and Inference Clarification:}
%The 23 extended aspects were identified through frequency analysis of the aspect distribution across political stances. For each candidate aspect, we computed its usage frequency among Pro-Ruling and Pro-Opposition sources and calculated each aspect's partisan skew percentage. Aspects exhibiting notable partisan skew and sufficient volume were manually reviewed and selected based on their political relevance to Indian discourse. These aspects were \textit{not} used as primary annotated targets during supervised training. For inference on these extended aspects, we deploy the \textit{already fine-tuned} LoRA-adapted Mistral model with few-shot prompting---no additional fine-tuning is performed at this stage. The model receives a set of global representative examples as the few-shot context, enabling generalization to these unseen target aspects while leveraging the stance classification capabilities acquired during training on the 15 seed aspects.
%}
%The inference prompt is constructed using the same 10 few-shot examples per aspect that were reserved during annotation. For each input tweet, the pipeline retrieves aspect-specific examples following the naming pattern \texttt{\{prefix\}\_\{aspect\}\_stance.json}. For the 20 extended aspects that lack dedicated {\color{blue} few-shot example} annotations, the system falls back to a global set of representative examples, enabling stance classification across all 38 aspects with consistent prompt structure.
% Inference uses deterministic settings (temperature 0, no sampling) to ensure reproducible outputs. 
% % The model returns JSON containing stance and reasoning, which is parsed by a robust extraction function that handles various output formats and applies label normalization. Batched processing with length-based bucketing enables efficient large-scale inference, while resume functionality and periodic checkpointing ensure reliability for processing the full dataset.
%{\color{blue}
Inference uses deterministic decoding (temperature 0) to ensure reproducible outputs. %The model returns structured JSON containing both the stance label and reasoning.
%}

\subsection{Hindi Tweet Analysis}
We also analyzed the tweets written in Hindi (Devanagari font) to perform a comparative analysis of the trends around stance classification for both Hindi and English tweets. For this purpose, we first selected 5000 unique Hindi tweets, by dropping retweets and duplicates. We also stratified the data by script$\times$URL$\times$hashtag, to prevent over-/under-representation exhibited in Naive Random Sampling. %This yields us a randomised 500 datapoint csv file and an ID manifest.
While there are NLP tools for aspect extraction and stance analysis for non-English/Hindi documents, in this work we use a translation pipeline to convert the Hindi tweets to their English equivalents, and then perform aspect extraction and stance classification on the translated data. A major advantage of this translation based approach is the applicability of SOTA NLP approaches (e.g., KeyBERT and SLMs) on the Hindi data, and comparability of the methods between English and Hindi. %Additionally, application of the same methods of aspect extraction and stance analysis on both Hindi and English datasets ensures that their performance on both datasets are easily comparable. This in turn leads to generalizability of our findings. 

We used the \texttt{facebook/nllb-200-distilled-600M} model \cite{costa2022no} (batch size=16, \texttt{max-new-tokens}=128) to translate the Hindi tweets to English, since the model has an advantage of efficient handling of pure Devanagari scripts, which are abundant in our data. This resulted in translation of all Hindi tweets to English. %Next, our goal was to perform quality check of the translated data. Standard qualitative analysis approaches to check translation quality often suffer from sampling issues, and the requirement of significant manual effort. To avoid this, we developed a quantitative method to assess the quality of translations, on top of a qualitative layer.

\textbf{Translation Quality Assessment:} To evaluate the quality of translations, we first represented the two datasets in the same embedding space, and then observed the drift between each Hindi and its equivalent translated English data point, to assess their semantic coherence. If the average drift was lower than a certain threshold, we determined the translation quality to be above par, and vice versa. We first embedded the original and the translated texts using the \textit{paraphrase-multilingual-MiniLM-L12-v2} model\cite{reimers2019sentence}. The model is trained on parallel and paraphrase data across 50+ languages, including Hindi, ensuring that both the Hindi and English texts are embedded into a shared embedding space. We then measured the cosine similarity of the embeddings to see how semantically similar the original and translated data points are. A set of similarity threshold scores were experimentally determined. Specifically, if the two data points exhibit: (A) A similarity score of above 0.77, they are semantically coherent, (B) A similarity score of below 0.7, they are considered to be semantically incoherent, (C) A similarity score $\epsilon [0.7,0.77]$, the data points are considered to require further manual inspection. {\color{blue}Two annotators manually inspected the tweets in the 0.7-0.77 band, and found no gross mistranslations. A unanimous agreement was reached.}

Our analysis of the 5,000-tweet set revealed a median cosine similarity of 0.7768 between original Hindi/Roman-Hindi tweets and their English translations (p10 = 0.5607, p90 = 0.9016), indicating a high percentage of semantically coherent translations, suitable for stance analysis. 
%Only a small proportion of tweets fall into low-similarity regimes, corresponding to cases of substantial semantic drift. Overall, these results indicate that the majority of translations produced by our pipeline are semantically faithful and suitable for downstream stance analysis.

% We also use \textit{Uniform Manifold Approximation and Projection} (UMAP) \cite{mcinnes2018umap} to project the original and translated embeddings into a 2D space (figure \ref{fig:umap_all}), to observe the drifts between pair of data points. %UMAP is a non-linear dimensionality reduction technique to visualize high-dimensional data in 2D or 3D. It is generally faster and better at preserving overall structure of data compared to other existing methods for dimensionality reduction. 
% We see that both the Hindi and corresponding English data points are similarly distributed in the 2D space, and only a few exhibit significant drifts, once again corroborating our finding of satisfactory translation quality. 
% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.8\linewidth]{Agriya_visualisations/full_umap_drift_with_arrows_legend_1.png}
%   \caption{UMAP projection of original (Hindi/Roman) vs translated (English) tweets with drift arrows.}
%   \label{fig:umap_all}
% \end{figure}

We also performed a manual check of the translated data (above the 0.77 threshold), to doubly ensure the quality of translations generated. Two annotators (comfortable with Hindi) independently annotated 100 randomly selected examples for their translation quality (1:satisfactory translation, 0: incorrect translation).% A label of 1 was assigned for satisfactory and semantically coherent translation, and 0 was assigned otherwise. 
This exercise revealed an inter-annotator agreement of around 98\% (disagreements resolved through discussion and consensus) and an accuracy of 93.75\%, indicating a satisfactory performance of the translation pipeline.
% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.8\linewidth]{umap_lowcos_below_0.85_xy.png}
%   \caption{Low-cosine translations ($<0.85$): UMAP coordinates for originals (blue) and translations (orange) with arrows.}
% \end{figure}
% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.8\linewidth]{umap_lowcos_below_0.90.png}
%   \caption{Low-cosine translations ($<0.90$): UMAP coordinates for originals (blue) and translations (orange) in line graph format.}
% \end{figure}
% Table~\ref{tab:eval_embedding} showcases the justification for our use of multilingual sentence embeddings, we compared \texttt{paraphrase-multilingual-MiniLM-L12-v2} with a monolingual MiniLM baseline on 5,000 Hindi–English translation pairs. The multilingual model achieved high cross-lingual retrieval accuracy (R@1 = 0.837, MRR = 0.854) and near-perfect separation between true and mismatched translation pairs ($\delta$ = 0.539, AUC = 0.960, Cohen’s d = 2.88), while the monolingual model performed substantially worse (R@1 = 0.462, AUC = 0.717).
% \begin{table}[t]
% \centering
% \caption{Embedding model comparison. Multilingual-MiniLM outperforms the monolingual model.}
% \label{tab:eval_embedding}
% \begin{tabular}{lrr}
% \toprule
% Metric & Multilingual-MiniLM & Monolingual-MiniLM \\
% \midrule
% R@1 & 0.837 & 0.4618 \\
% R@5 & 0.8726 & 0.4676 \\
% R@10 & 0.8816 & 0.4716 \\
% MRR & 0.854 & 0.465 \\
% POS\_mean & 0.831 & 0.606 \\
% POS\_median & 0.8805 & 0.5105 \\
% NEG\_mean & 0.2922 & 0.3031 \\
% NEG\_median & 0.2803 & 0.3077 \\
% Delta & 0.5394 & 0.3029 \\
% AUC & 0.9604 & 0.7168 \\
% Cohen\_d & 2.88 & 1.09 \\
% \bottomrule
% \end{tabular}
% \end{table}
% \%vspace{0.5em}
% \begin{center}
% \footnotesize
% \captionof{table}{Embedding Model Comparison: Multilingual-MiniLM clearly outperforms the monolingual model}
% \label{tab:eval_embedding}
% \begin{tabular}{lrr}
% \toprule
% \textbf{Metric} & \textbf{multilingual-MiniLM} & \textbf{monolingual-MiniLM} \\
% \midrule
% R@1 & 0.837 & 0.4618 \\
% R@5 & 0.8726 & 0.4676 \\
% R@10 & 0.8816 & 0.4716 \\
% MRR & 0.854 & 0.465 \\
% POS\_mean & 0.831 & 0.606 \\
% POS\_median & 0.8805 & 0.5105 \\
% NEG\_mean & 0.2922 & 0.3031 \\
% NEG\_median & 0.2803 & 0.3077 \\
% Delta & 0.5394 & 0.3029 \\
% AUC & 0.9604 & 0.7168 \\
% Cohen\_d & 2.88 & 1.09 \\
% \bottomrule
% \end{tabular}
% \end{center}
%\vspace{0.5em}
%This behavior is consistent with the multilingual alignment objective described by Reimers and Gurevych \cite{reimers2020making}, who show that sentence embeddings trained via knowledge distillation on parallel data form a shared semantic space across languages. %Our results empirically confirm that  the multilingual model yields a sufficiently well-aligned cross-lingual embedding space for reliable translation quality assessment and multilingual stance analysis.
After completing the translation quality calibration on the sampled Hindi tweets, we applied the same translation and embedding pipeline to the remaining unique Hindi tweets to convert them into English. We then reran the English stance analysis pipeline on the translated data.
%enabling a direct and consistent comparison between Hindi and English tweet analyses.
% \subsubsection{Multilingual Model Support}

% The Mistral-7B model supports multilingual input, enabling direct processing of Hindi tweets. The few-shot examples for Hindi tweets are constructed with Hindi text while maintaining the same JSON output format.

% \subsubsection{Fallback Mechanism}

% The pipeline implements hierarchical few-shot example retrieval:
% \begin{enumerate}
%     \item Attempt to load aspect-specific examples
%     \item If unavailable, fall back to a global fallback JSON
% \end{enumerate}

% This ensures robust inference even for aspects without dedicated few-shot examples.

% \%vspace{0.5em}
% \begin{center}
% \begin{tikzpicture}[
%     node distance=1cm,
%     auto,
%     block/.style={
%         rectangle,
%         draw,
%         fill=blue!10,
%         text width=1.8cm,
%         text centered,
%         rounded corners,
%         minimum height=0.8cm,
%         font=\scriptsize
%     },
%     cloud/.style={
%         draw,
%         ellipse,
%         fill=red!10,
%         minimum height=0.6cm,
%         font=\scriptsize
%     },
%     database/.style={
%         cylinder,
%         cylinder uses custom fill,
%         cylinder body fill=yellow!20,
%         cylinder end fill=yellow!10,
%         shape border rotate=90,
%         draw,
%         aspect=0.25,
%         minimum height=1cm,
%         text centered,
%         font=\scriptsize
%     },
%     line/.style={
%         draw,
%         -Latex,
%         thick
%     }
% ]

% % Nodes
% \node [cloud] (input) {Input Tweet};
% \node [block, below of=input] (lookup) {Few-Shot Lookup};
% \node [database, right of=lookup, node distance=2.5cm] (db) {Examples DB};
% \node [block, below of=lookup] (prompt) {Prompt Build};
% \node [block, below of=prompt] (llm) {LLM + LoRA};
% \node [cloud, below of=llm] (output) {JSON Output};

% % Edges
% \path [line] (input) -- (lookup);
% \path [line] (db) -- (lookup);
% \path [line] (lookup) -- (prompt);
% \path [line] (prompt) -- (llm);
% \path [line] (llm) -- (output);

% \end{tikzpicture}
% \captionof{figure}{Stance Detection System Architecture}
% \label{fig:architecture}
% \end{center}
% \%vspace{0.5em}

\section{Results}
%This section reports the results of our qualitative and quantitative analysis of influencer discourse. The analysis covers 680,847 tweet-aspect pairs across 38 political aspects, with influencers categorized by political polarizaton into Pro-Ruling (462 influencers) and Pro-Opposition (498 influencers) groups.%% 
% {\color{blue} This section reports the results of our qualitative and quantitative analysis of influencer discourse. The analysis covers 544,679 tweet-aspect pairs across 37 political aspects, with influencers categorized by political polarization into Pro-Ruling (405 influencers) and Pro-Opposition (346 influencers) groups. {\color{blue} This final count reflects the dataset after aspect-based filtering: from the original corpus of 7.1 million retweets (Table~\ref{tab:dataset_stats}), we extracted tweets containing the 37 polar aspects identified in the Methodology, yielding %448,686 unique tweets. Since a single tweet may reference multiple aspects, 
% the 468,914 tweet-aspect pairs (311,099 English and 157,815 Hindi tweet-aspect pairs.}}
{\color{blue} This section reports the results of our qualitative and quantitative analysis of influencer discourse. The original corpus comprised 7.1 million retweets from 960 unique influencers across the 2020--2023 period (Table~\ref{tab:dataset_stats}). After aspect-based filtering---extracting only tweets containing the 35 polar aspects identified in the Methodology---the final dataset comprises 457,509 tweet-aspect pairs (303,233 English and 154,276 Hindi). These pairs correspond to 418,233 unique tweets, as a single tweet may reference multiple aspects. The analysis covers 674 influencers, categorized by political polarization into Pro-Ruling (372 influencers) and Pro-Opposition (302 influencers) groups. Influencers labelled as ``neutral'' were excluded from this analysis, as we focus specifically on the discourse patterns of politically aligned influencers.}



\subsection{Model Performance}
We compare the performance of Mistral-7B based stance classification approaches against the baseline architectures as described in the Methodology. 
Classical transformer architectures -- BERT (55.1\%), RoBERTa (57.4\%), and PyABSA (54.0\%) -- fall substantially short of the fine-tuned Mistral model (77.7\%), with accuracy gaps exceeding 20 percentage points.
Figure~\ref{fig:model_comparison} illustrates the comparative results across key metrics. The LoRA-tuned Mistral model consistently outperforms the baseline approaches in all of the metrics (accuracy: 77.7\%, F1 score (weighted): 77.4\%) for the 265 held out test samples. %demonstrating superior capability in capturing the nuances of political stance. 
The per-class accuracy reveals that the model performs best on the \textit{Favor} class (85.6\%), followed by \textit{Against} (80.0\%), while the \textit{Neutral} class shows comparatively lower accuracy (59.3\%). We observe that instances of misclassification are primarily concentrated around the Neutral category (for the best performing as well as baseline models), reflecting the inherent ambiguity often present in less explicit partisan expressions.
%The confusion matrix presented in Figure~\ref{fig:confusion_matrix} shows a strong diagonal, indicating high precision in correctly identifying the three stance labels. We observe that the instances of misclassification are primarily concentrated around the Neutral category (for the best performing as well as baseline models), reflecting the inherent ambiguity often present in less explicit partisan expressions.  %Notably, the model effectively minimizes confusion between opposing stances (Favor vs. Against), which is critical for accurate polarization analysis. 
%We observe that the instances of misclassification are primarily concentrated around the Neutral category, reflecting the inherent ambiguity often present in less explicit partisan expressions. We present our analysis of these misclassifications in the Discussion section.%, followed by RoBERTa. For the current dataset, we find that PyABSA performs the worst and fails to capture the linguistic nuances of the tweets. 
% In the Appendix, we qualitatively compare the performance of the best and worst performing models, and discuss cases where PyABSA fails to perform for stance classification.
\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{results_eng/model_comparison.png}
  \caption{Performance comparison of the fine-tuned Mistral (LoRA) model against baseline architectures.}
  \label{fig:model_comparison}
\end{figure}
%These experiments clearly echo the superior performance of fine-tuned SLMs over the classical transformer based approaches, owing to fine-tuning of the SLM to learn context specific lingusitic attributes. The SLM further outperforms the baseline BERT-based model trained on the same dataset, due to its greater capacity and representational complexity. However, since stance classification is heavily contextual, and the results might vary depending on the data under consideration. 
We use the best performing LoRA-tuned model for stance classification hereinafter. 


% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.95\linewidth]{results_eng/mistral_finetuned_confusion_matrix.png}
%   \caption{Confusion matrix for the fine-tuned Mistral model on the held-out test set.}
%   \label{fig:confusion_matrix}
% \end{figure}


%% Detailed Evaluation Results - Moved from Methodology per reviewer comment (Anirban Sen, 6 Jan)
% \subsubsection{Detailed Evaluation Metrics}
% Table~\ref{tab:eval_overall_results} presents the detailed metrics for the best performing model (LoRA-adapted Mistral). The model demonstrates competitive performance, effectively balancing precision and recall across classes.

% \%vspace{0.5em}
% \begin{center}
% \footnotesize
% \captionof{table}{Stance Classification Evaluation Results (Mistral LoRA)}
% \label{tab:eval_overall_results}
% \begin{tabular}{lr}
% \toprule
% \textbf{Metric} & \textbf{Value} \\
% \midrule
% Total test samples & 264 \\
% Accuracy & 78.0\% \\
% Precision (macro) & 77.6\% \\
% Recall (macro) & 75.3\% \\
% F1-Score (macro) & 76.1\% \\
% F1-Score (weighted) & 77.7\% \\
% \bottomrule
% \end{tabular}
% \end{center}
% \%vspace{0.5em}
% Table~\ref{tab:eval_perclass_results} shows per-class performance for the fine-tuned SLM. The model performs strongest on the ``favor'' class (F1=0.82), followed by ``against'' (F1=0.79). The ``neutral'' class shows lower recall (0.60), reflecting the inherent difficulty of distinguishing neutral stances from weakly expressed opinions. We present a detailed analysis of the misclassifications in the Discussion section.
% \begin{center}
% \footnotesize
% \captionof{table}{Per-Class Classification Performance (Mistral LoRA)}
% \label{tab:eval_perclass_results}
% \begin{tabular}{lrrrr}
% \toprule
% \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Support} \\
% \midrule
% Against & 0.78 & 0.80 & 0.79 & 95 \\
% Favor & 0.79 & 0.86 & 0.82 & 111 \\
% Neutral & 0.76 & 0.60 & 0.67 & 58 \\
% \bottomrule
% \end{tabular}
% \end{center}
% \%vspace{0.5em}
Table~\ref{tab:eval_aspect_results} presents the 
%top 10 
per-aspect accuracy for the seed aspects, showing that the highly polar aspects yield clearer stance signals. Polar aspects like \textit{Modi} (name of the Prime Minister of India) achieve the highest accuracy (94.4\%), followed by \textit{CAA} (89.5\%) and \textit{Congress} (88.9\%) (the largest party in opposition), indicating their usage in tweets with clear stance signatures.% (when compared to less polar aspects like \textit{China} and \textit{Shaheen\_bagh}).
%\vspace{0.5em}
\begin{table}[]
    \centering
    \caption{Per-aspect Classification Accuracy (Seed Aspects)}
    \label{tab:eval_aspect_results}
    \begin{tabular}{lrr}
    \hline
    \textbf{Aspect} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
    \hline
    modi & 94.4\% & 0.91 \\
    caa & 89.5\% & 0.90 \\
    congress & 88.9\% & 0.84 \\
    hindutva & 85.7\% & 0.56 \\
    new\_parliament & 85.7\% & 0.82 \\
    rahulgandhi & 81.2\% & 0.82 \\
    farmers\_protests & 80.0\% & 0.82 \\
    shaheen\_bagh & 78.9\% & 0.71 \\
    muslim & 75.0\% & 0.73 \\
    china & 73.7\% & 0.64 \\
    farm\_laws & 72.7\% & 0.74 \\
    ram\_mandir & 71.4\% & 0.70 \\
    hindu & 66.7\% & 0.59 \\
    kashmiri\_pandits & 62.5\% & 0.43 \\
    kashmir & 58.8\% & 0.38 \\ 
    \hline
    \end{tabular}
\end{table}
% \begin{center}
% \footnotesize
% \captionof{table}{Per-aspect Classification Accuracy (All 15 Seed Aspects)}
% \label{tab:eval_aspect_results}
% \begin{tabular}{lrr}
% \toprule
% \textbf{Aspect} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
% \midrule
% modi & 94.4\% & 0.91 \\
% caa & 89.5\% & 0.90 \\
% congress & 88.9\% & 0.84 \\
% hindutva & 85.7\% & 0.56 \\
% new\_parliament & 85.7\% & 0.82 \\
% rahulgandhi & 81.2\% & 0.82 \\
% farmers\_protests & 80.0\% & 0.82 \\
% shaheen\_bagh & 78.9\% & 0.71 \\
% muslim & 75.0\% & 0.73 \\
% china & 73.7\% & 0.64 \\
% farm\_laws & 72.7\% & 0.74 \\
% ram\_mandir & 71.4\% & 0.70 \\
% hindu & 66.7\% & 0.59 \\
% kashmiri\_pandits & 62.5\% & 0.43 \\
% kashmir & 58.8\% & 0.38 \\
% \bottomrule
% \end{tabular}
% \end{center}
%\vspace{0.5em}
For the OOD testing, we performed stance classification on the 20 unseen aspects. Two independent annotators again annotated around 20 (tweet,aspect) pairs for each of these aspects, reaching a 88\% agreement (disagreements resolved through discussion and consensus).
%To evaluate the generalization capability of the fine-tuned model beyond the seed aspects used during training, we conducted a qualitative accuracy check on 20 additional political aspects (out-of-distribution testing) that were not part of the fine-tuning dataset. For each aspect, we randomly sampled 20 tweets and obtained human annotations (by two independent annotators) for stance classification. The model predictions were then compared against these human annotations to assess performance on unseen political topics.

Across 390 annotated tweet-aspect pairs spanning 20 aspects, the model achieved an overall accuracy of 73.1\%, with a macro F1-score of 65.3\% and weighted F1-score of 70.9\%. Table~\ref{tab:post_analysis_overall} summarizes the overall performance metrics.
% \begin{center}
% \footnotesize
% \captionof{table}{Post-Analysis Accuracy: Overall Metrics}
% \label{tab:post_analysis_overall}
% \begin{tabular}{lr}
% \toprule
% \textbf{Metric} & \textbf{Value} \\
% \midrule
% Total annotated samples & 390 \\
% Accuracy & 73.1\% \\
% Precision (macro) & 68.9\% \\
% Recall (macro) & 66.2\% \\
% F1-Score (macro) & 65.3\% \\
% F1-Score (weighted) & 70.9\% \\
% \bottomrule
% \end{tabular}
% \end{center}
The per-class analysis (table \ref{tab:post_analysis_aspects}) reveals that the model performs best on the ``against'' stance (F1=0.83), followed by ``favor'' (F1=0.74), while ``neutral'' classification remains challenging (F1=0.40) due to lower recall (29.4\%), suggesting that neutral stance detection remains inherently difficult across both seen and unseen aspects.
% \begin{center}
% \footnotesize
% \captionof{table}{Post-Analysis Accuracy: Per-Class Performance}
% \label{tab:post_analysis_perclass}
% \begin{tabular}{lrrrr}
% \toprule
% \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Support} \\
% \midrule
% Favor & 0.67 & 0.82 & 0.74 & 102 \\
% Against & 0.79 & 0.87 & 0.83 & 203 \\
% Neutral & 0.61 & 0.29 & 0.40 & 85 \\
% \bottomrule
% \end{tabular}
% \end{center}
%Table~\ref{tab:post_analysis_aspects} presents the per-aspect accuracy for the 20 evaluated aspects. 
Notable high-accuracy aspects include polar aspects like \textit{Bhakts}, \textit{Islamists}, \textit{Democracy}, \textit{Demonetisation}, and \textit{Suicides}. %These aspects tend to have clearer partisan framing patterns that the model successfully captures. 
Conversely, aspects like \textit{MSP}, \textit{Minorities}, and \textit{Lynching} show lower accuracy, likely due to more nuanced or contested discourse patterns. %that differ from the training distribution.
% \begin{center}
% \scriptsize
% \captionof{table}{Post-Analysis Accuracy: Per-aspect Performance}
% \label{tab:post_analysis_aspects}
% \begin{tabular}{lrrr}
% \toprule
% \textbf{aspect} & \textbf{Samples} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
% \midrule
% Bhakts & 20 & 95.0\% & 0.65 \\
% Islamists & 20 & 95.0\% & 0.33 \\
% Democracy & 20 & 90.0\% & 0.61 \\
% Demonetisation & 20 & 90.0\% & 0.62 \\
% Suicides & 20 & 90.0\% & 0.80 \\
% Aatmanirbhar Bharat & 21 & 85.7\% & 0.84 \\
% GDP & 20 & 80.0\% & 0.72 \\
% Dictatorship & 19 & 78.9\% & 0.43 \\
% Inflation & 20 & 75.0\% & 0.66 \\
% Ayodhya & 20 & 70.0\% & 0.69 \\
% Mahotsav & 20 & 70.0\% & 0.69 \\
% Sangh & 20 & 70.0\% & 0.59 \\
% Sharia & 20 & 70.0\% & 0.43 \\
% Spyware & 20 & 70.0\% & 0.62 \\
% Unemployment & 20 & 70.0\% & 0.57 \\
% Hathras & 20 & 65.0\% & 0.41 \\
% Lynching & 20 & 55.0\% & 0.36 \\
% Balochistan & 10 & 50.0\% & 0.24 \\
% Minorities & 20 & 50.0\% & 0.46 \\
% MSP & 20 & 30.0\% & 0.31 \\
% \bottomrule
% \end{tabular}
% \end{center}
The results demonstrate that while the fine-tuned model generalizes reasonably well to unseen political aspects (73.1\% overall accuracy), performance varies significantly across aspects. Aspects with clearer ideological positioning and less ambiguous framing tend to yield higher accuracy, while aspects involving contested narratives or requiring deeper contextual understanding show reduced performance. This suggests potential avenues for further model improvement through targeted data augmentation.
\subsection{Partisanship in Influencer Discourse}
Figure~\ref{fig:multilingual_stack} presents stacked stance distribution plots for English and Hindi tweets, for the different aspects considered. We discuss in this section the results for a few example aspects across the polarity spectrum, and present the plots for the rest of the 29 aspects in the appendix.%-- while some of these aspects are highly polar (e.g., Hindutva), some others are relatively less polar (e.g., Democracy), when it comes to influencer discourse as can be seen from table XXX.
The visualization shows three distinct bands/rows of aspects. The top row highlights aspects that the ruling dispensation disproportionately favors in public discourse, while the opposition generally criticizes  (\textit{Hindutva, Modi, etc.}). The middle row represents a "mixed" zone where both the establishment and the opposition show relatively similar stance distributions, indicating areas of shared narrative or lower polarization. The bottom row serves as the counterpart to the first, showcasing aspects that the opposition disproportionately favors and the ruling party generally criticizes (e.g., Rahul Gandhi, Farmer's Protests, etc.). 

A few striking trends emerge. We see that the influencer tweets are significantly partisan and highly correlated to influencer polarity, when it comes to discussion on aspects leaning towards a certain party. As can be seen from the plot, aspects like \textit{Hindutva, Modi,} and \textit{Ram Mandir} are reported with an overwhelmingly positive stance by pro-ruling influencers (and negatively by pro-opposition influencers). Notably, these aspects are also prominently and favorably framed by the ruling establishment across mainstream media and broader public discourse \cite{HansIndia_RamMandir_Opposition_2024}, while being criticized by the opposition.
On the other hand, aspects like \textit{Rahul Gandhi, Muslims, and Farmers' Protests} are reported with a disproportionately negative stance by pro-ruling influencers (and positively by pro-opposition). Once again, the trends closely follow the public discourse around these topics by the political parties the influencers favor \cite{ET_Congress_FarmersSupport_2026}. Additionally, the stance distribution patterns are remarkably similar across both languages as can be seen from the plot. This observation suggests that the polarization dynamics and narrative structures deployed by political influencers are consistent, irrespective of the linguistic medium. We further corroborate this finding by cross-referencing it with a negativity scatter analysis (Figure~\ref{fig:negativity_scatter}).
\\\\
\textbf{Significance Testing:} To rigorously test whether influencer political alignment significantly predicts the likelihood of expressing a favorable stance toward each aspect, we employ a binomial Generalized Linear Model (GLM) with a logit link function. For each aspect $k$, we model the probability $p_i$ that influencer $i$ tweets favorably as:
$\text{logit}(p_i) = \beta_0 + \beta_1 \cdot \text{Alignment}_i$
%\end{equation}
where $\text{Alignment}_i = 0$ for pro-ruling influencers and $\text{Alignment}_i = 1$ for pro-opposition influencers. The response variable is the binomial count $[n_{\text{favor}}, n_{\text{total}} - n_{\text{favor}}]$ per influencer, representing the number of favor tweets out of total tweets for that aspect. A significant $\beta_1$ coefficient ($p < 0.05$) indicates that influencer alignment is a significant predictor of favorable stance expression. Across 35 aspects tested, 30 (85.7\%) yield statistically significant results ($p < 0.05$), confirming that influencer political alignment is a robust predictor of stance across most political topics. %The odds ratios, calculated as $e^{|\beta_1|}$, quantify the magnitude of this effect. Aspects exhibiting the strongest pro-ruling bias include \textit{New Parliament} (OR = 40.76, $p < 10^{-41}$), \textit{Demonetisation} (OR = 34.62, $p < 10^{-36}$), \textit{Aatmanirbhar} (OR = 28.50, $p < 10^{-37}$), and \textit{Ram Mandir} (OR = 25.88, $p < 10^{-84}$). Conversely, aspects with the strongest pro-opposition bias include \textit{Farmers Protests} (OR = 15.78, $p < 10^{-51}$), \textit{Muslim} (OR = 12.07, $p \approx 0$), \textit{Rahul Gandhi} (OR = 10.79, $p \approx 0$), and \textit{Shaheen Bagh} (OR = 10.55, $p < 10^{-70}$). 
Only five aspects—Suicides, Lynching, Dictatorship, Sharia, and Islamists, show no significant alignment effect, likely due to insufficient sample sizes or low variation in stance across political groups (detailed plot in appendix).%  Figure~\ref{fig:forest_plot} presents a forest plot visualization of these results.
% The x-axis displays odds ratios on a logarithmic scale, with the vertical line at OR = 1 representing no effect. Points to the left indicate aspects where pro-ruling influencers are more likely to favor (red), while points to the right indicate pro-opposition favor (blue). Horizontal lines represent 95\% confidence intervals. 
%The plot reveals a clear bimodal distribution: aspects cluster either strongly to the left (pro-ruling favor) or strongly to the right (pro-opposition favor), with very few aspects near the center. This visual pattern reinforces the quantitative finding that Indian political influencer discourse is deeply polarized.%, with stance expression on most political topics being predictable from influencer alignment alone.
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.95\linewidth]{results_eng/1_forest_plot_odds_ratio.png}
%   \caption{Forest plot showing odds ratios with 95\% confidence intervals for the effect of influencer alignment on favorable stance expression. Red points indicate aspects where pro-ruling influencers are significantly more likely to favor; blue points indicate pro-opposition favor; gray points are non-significant.}
%   \label{fig:forest_plot}
% \end{figure}
%}
%, which similarly indicated high congruence in the sentiment and negativity levels associated with specific aspects across both languages.
\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{results_eng/combined_hindi_english_stance_stacked.png}
  \caption{Stacked stance distribution comparison between English and Hindi tweets.}
  \label{fig:multilingual_stack}
\end{figure}

%Given this high degree of congruence between English and Hindi discourse, we appended the two datasets for the remainder of our analysis to provide a more comprehensive view of the political landscape.
\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{results_eng/6_negativity_scatter.png}
  \caption{Comparison of negativity scores for aspects in English vs. Hindi tweets.}
  \label{fig:negativity_scatter}
\end{figure}
The scatter plot in Figure~\ref{fig:negativity_scatter} plots the average percentage of \textit{against} stance (termed as the \textit{negativity score}) for each aspect in English (X-axis) against its score in Hindi (Y-axis). We observe a strong positive correlation, with most aspects clustering along the diagonal ($y=x$ line). To understand the significance of these results, we calculate the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). For Pro-Ruling aspects ($n=32$ aspects), we obtain: RMSE = 5.11, and MAE = 3.76 percentage points are obtained. For Pro-Opposition aspects ($n=33$ aspects): RMSE = 7.37, and MAE = 4.89 percentage points are obtained. These significantly low error values confirm that the cross-lingual negativity patterns are highly consistent, further supporting the aggregation of English and Hindi datasets for subsequent analysis.

% {\color{blue}
% To quantify the cross-lingual consistency, we measure the perpendicular distance of each data point from the diagonal ($y=x$) line. For a point $(x_i, y_i)$ representing the negativity scores in English and Hindi respectively, the perpendicular distance to the diagonal is given by:
% \[
% d_i = \frac{y_i - x_i}{\sqrt{2}}
% \]
% We then compute the following metrics over all $n$ aspects: (1) \textbf{Root Mean Squared Error (RMSE)}: $\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}d_i^2}$, which captures the average magnitude of deviation, penalizing larger deviations more heavily; (2) \textbf{Mean Absolute Error (MAE)}: $\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|d_i|$, which provides the average absolute deviation from the diagonal; (3) \textbf{Median Absolute Distance}: the median of $|d_i|$, a robust measure less sensitive to outliers; and (4) \textbf{Mean Signed Distance}: $\frac{1}{n}\sum_{i=1}^{n}d_i$, which indicates systematic bias (positive values suggest higher negativity in Hindi, negative values suggest higher negativity in English).

% For Pro-Ruling aspects ($n=32$ aspects), we obtain: RMSE = 5.11 percentage points, MAE = 3.76 pp, Median Absolute Distance = 2.39 pp, and Mean Signed Distance = +1.01 pp (slight Hindi bias). For Pro-Opposition aspects ($n=33$ aspects): RMSE = 7.37 pp, MAE = 4.89 pp, Median Absolute Distance = 2.74 pp, and Mean Signed Distance = $-$1.75 pp (slight English bias). These low error values -- with median deviations under 3 percentage points in both camps -- confirm that the cross-lingual negativity patterns are highly consistent, further supporting the aggregation of English and Hindi datasets for subsequent analysis.
% }

The findings indicate that topics evoking negative sentiment in English discourse tend to trigger similar levels of negativity in Hindi, reinforcing the hypothesis of a unified partisan narrative that transcends language barriers. 
{\color{blue}Similar cross-lingual consistency is observed for favorable (RMSE: 5.68–6.09 pp) and neutral (RMSE: 4.33–5.90 pp) stances, confirming that stance distribution patterns are robust across both languages for all stance categories}
The visual evidence suggests that the emotional valence of political topics is consistent across linguistic communities, further justifying the aggregation of datasets for subsequent analysis.

% Using the combined English and Hindi dataset, we next re-examine the percentages of \textit{favor} stance (normalized favor rates) for each of the 38 aspects. Figure~\ref{fig:butterfly_favor} displays these percentages, with Pro-Ruling favor percentages on the right and Pro-Opposition favor percentages on the left.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.95\linewidth]{results_eng/2_butterfly_favor_normalized.png}
%   \caption{Butterfly chart showing normalized favor rates by aspect (combined English \& Hindi data).}
%   \label{fig:butterfly_favor}
% \end{figure}

% The combined analysis reinforces the bimodal nature of the discourse. We observe a set of aspects where the Pro-Ruling side exhibits near-complete dominance in favorable expression, contrasting sharply with aspects where the Pro-Opposition side dominates. Topics located near the center of the chart continue to exhibit more balanced favor distributions, but the overall trend confirms that most political topics in the Indian context are heavily skewed toward one side's narrative. This polarization is even more pronounced when leveraging the larger, combined dataset, validating the robustness of our initial findings.
\subsection{Aspect-Level Polarization}
Figure~\ref{fig:divergence_scatter} presents the aspect level stance divergence for the combined corpus, for all aspects. This scatter plot maps each aspect based on the difference between percentage of favor tweets (x-axis) and against tweets (y-axis). The size of each bubble represents the number of tweets for a certain aspect. The color gradient represents the ``favor divergence score" -- the difference between percentages of favor and against tweets (green representing high \textit{favor} percentage, and red representing high \textit{against} percentage).
\begin{figure}[t]
  \centering
\includegraphics[width=0.95\linewidth]{results_eng/divergence_scatter_english_hindi_combined.png}
  \caption{aspect-level stance divergence scatter plot for the combined English and Hindi dataset.}
  \label{fig:divergence_scatter}
\end{figure}
The plot clearly highlights the two distinct poles occupied by the different aspects. We find two clusters of highly polar aspects in the first and third quadrants of the plot. These are aspects where one side of influencers (pro-ruling/pro-opposition) is disproportionately in favor. %Outliers in this plot represent topics that generate unique polarization patterns -- for instance, topics where one side is highly favorable while the other is not necessarily hostile but silent, or where both are hostile (bottom-left). 
We also see relatively \textit{neutral} (yellow) aspects in the middle. However, the bubble sizes for these neutral aspects are significantly smaller, indicating their scarcity in influencer discourse. Thus, we see a highly polar aspect level discourse -- specific high-volume topics drive the bulk of the polarization and are discussed in a highly partisan manner, while a "long tail" of insignificantly discussed issues remain relatively contested or neutral. %\footnote{We also present a plot of the variation in favor divergence with aspect size (number of tweets) in the Appendix, which exhibits a similar finding where neutral aspects are seen to clutter near the origin, indicative of their relative insignificance in the influencer discourse.}

%% ============================================================
%% SECTION 5.5: COMPLETE STANCE DISTRIBUTION
%% ============================================================
% \subsection{Complete Stance Distribution by aspect}
% Figure~\ref{fig:stance_by_aspect} presents the full stance distribution for each aspect, disaggregated by political affiliation. Each aspect displays proportions of favor, neutral, and against stances for both Pro-Ruling and Pro-Opposition influencers.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.95\linewidth]{results_eng/4_stance_distribution_by_aspect.png}
%   \caption{Stance distribution by aspect and political affiliation, showing proportions of favor, neutral, and against stances.}
%   \label{fig:stance_by_aspect}
% \end{figure}
% To organize aspects into meaningful thematic categories, we employed Google's Gemini model. The model was provided only with the list of 38 aspects and prompted to create thematic buckets based on its knowledge of Indian political discourse. Table~\ref{tab:stance_composition} presents the resulting aspect categories.
% \begin{center}
% \footnotesize
% \captionof{table}{aspects Grouped by Thematic Category (Generated using Gemini)}
% \label{tab:stance_composition}
% \begin{tabular}{p{2.4cm}p{4.1cm}}
% \toprule
% \textbf{Category} & \textbf{aspects} \\
% \midrule
% Favor-dominant (both) & ayodhya, mahotsav, ucc \\
% Against-dominant (both) & inflation, unemployment, suicides \\
% Split (PR favor, PO against) & modi, ram\_mandir, aatmanirbhar, hindutva, farm\_laws, caa \\
% Split (PO favor, PR against) & rahulgandhi, congress, farmers\_protests, shaheen\_bagh, muslim \\
% Mixed neutral ($>$20\%) & china, gdp, democracy, minorities \\
% \bottomrule
% \end{tabular}
% \end{center}
% {\footnotesize \textit{Note: PR = Pro-Ruling, PO = Pro-Opposition}}

We also performed a thematic stance analysis where each (tweet,aspect) pair was bucketed into a thematic topic using an LLM (GPT-4.5){\color{blue}\cite{achiam2023gpt}}. The analysis revealed existence of polar themes (e.g., \textit{Majoritarian Ideology and Hindu Nationalist Mobilization}) that exhibit significant partisanship in influencer discourse. The prompts and detailed results for this exercise are present in the Appendix.

% {\color{blue}
% \subsubsection{Thematic Stance Analysis}
% To perform a thematic analysis of stance, we employed ChatGPT (GPT-4.5\footnote{We also tried Gemini for this exercise, but a manual analysis revealed a slightly better performance of GPT, with clearer aspect definitions.}) with a detailed prompt (appendix) through few-shot learning -- the model was provided with the list of aspects and a sample of tweets for each aspect, and instructed to: (A) read all tweets to understand framing patterns, tone, and narrative structures; (B) create high-level thematic buckets that capture major stance/framing themes relevant to pro-ruling vs. pro-opposition influencer discourse; (C) assign each aspect to one primary bucket with reasoning grounded in observed tweet patterns.

% This process yielded nine thematic buckets: (1) \textit{Leader \& Party Contestation} (modi, rahulgandhi, congress); (2) \textit{Institutions, Democracy \& State Accountability} (democracy, dictatorship, spyware, new parliament); (3) \textit{Economy, Development \& Macro-Stewardship} (aatmanirbhar, demonetisation, gdp, inflation, unemployment, suicides); (4) \textit{Agrarian Reform \& Farmer Movement} (farm laws, farmers protests, msp); (5) \textit{Citizenship, Belonging \& Mass Protest Politics} (caa, shaheen bagh); (6) \textit{Majoritarian Ideology \& Hindu Nationalist Mobilization} (hindutva, sangh, bhakts, hindu); (7) \textit{Communal Relations, Minority Rights \& Collective Violence} (minorities, muslim, lynching, sharia, islamists, hathras); (8) \textit{Symbolic Nationhood \& Cultural-Religious Projects} (ayodhya, ram mandir, mahotsav); and (9) \textit{Security, Territory \& Geopolitics} (china, kashmir, balochistan, kashmiri pandits).

% Figure~\ref{fig:party_focus_stance} presents the stance analysis for each thematic bucket, for pro-ruling and pro-opposition influencers. Each horizontal bar represents the percentage of a side's total tweets devoted to a given bucket.
% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.95\linewidth]{results_eng/party_focus_stance_breakdown.png}
%   \caption{Thematic stance analysis: Bar length indicates percentage of total tweets}
%   \label{fig:party_focus_stance}
% \end{figure}
% Several differences emerge between the two camps, in terms of the partisanship exhibited at a theme level. \textit{Leader \& Party Contestation} dominates both groups' discourse, but occupies a larger share for Pro-Opposition (59.3\%) than Pro-Ruling (49.7\%). Pro-Ruling influencers allocate a higher proportion to \textit{Majoritarian Ideology \& Hindu Nationalist Mobilization} (16.8\%) compared to Pro-Opposition (9.3\%). \textit{Symbolic Nationhood \& Cultural-Religious Projects} accounts for 4.5\% of Pro-Ruling discourse but only 0.8\% of Pro-Opposition discourse. Conversely, \textit{Institutions, Democracy \& State Accountability} receives greater attention from Pro-Opposition (4.1\%) than Pro-Ruling (1.9\%). The stance composition within buckets also differs: Pro-Ruling tweets in the \textit{Majoritarian Ideology} and \textit{Leader and Party Contestation} buckets are predominantly favorable (green), while Pro-Opposition tweets in the same buckets are predominantly against (red). A detailed table showing individual thematic bucket definitions, aspect contributions to each bucket, with stance breakdowns and tweet counts, is provided in the Appendix.}
{\color{blue}
\subsection{Misclassification}
Upon analyzing the misclassifications around stance analysis, we found that a common source of error involved implicit evaluation, especially in tweets discussing economic aspects such as GDP (Gross Domestic Product). While such tweets are factual and are tagged as \textit{neutral} by the model, human annotators interpreted them as favourable because such statistics are commonly used to signal government performance or national strength. Another frequent issue relates to sarcasm and mockery, especially when expressed through quotation marks, rhetorical framing, or code-mixed language (e.g., tweets invoking ironic references to ``democratic'' governance). The model fails to recognize the intent often around such messages, highlighting the difficulty of capturing pragmatic cues that rely on shared political and cultural context. An example of this is Ram Mandir, which goes misclassified as the model does not capture the broader political context behind it.%Misclassification also occurred due to unclear targets of critique. Some tweets mention politically charged events but direct criticism toward secondary actors such as opposition leaders, institutions, or public figures. The model tends to rely on aspect associations and struggles to distinguish between references to an issue and evaluation of a specific actor. 
Furthermore, aspects such as “Hindu” and “Kashmiri Pandits” are frequently misclassified because the model collapses group mentions with stance toward the group itself. Intra-group criticism (e.g., Hindus criticising an individual for violating religious norms) and third-party or policy-focused critique are incorrectly read as group-level evaluation, producing systematic target confusion. Finally, some errors involved descriptive statements with implicit moral judgement, particularly in factual tweets reporting violence or governance failures. While human readers interpret these descriptions as critical, the model assigns neutral stance to these due to the absence of explicit sentiment markers (e.g., the Hathras aspect \cite{LiveMint_HathrasTimeline_2023} where misclassifications occurred because the model anchored on the event keyword (Hathras) and failed to distinguish between references to the incident and criticism directed at secondary political actors). Detailed examples are shown in the appendix. %Table~\ref{tab:misclassification_examples}.
% \begin{center}
% \tiny
% \captionof{table}{Representative Examples of Model Misclassification}
% \label{tab:misclassification_examples}
% \begin{tabular}{p{5.5cm}p{1.3cm}ccp{3.5cm}}
% \toprule
% \textbf{Tweet} & \textbf{Aspect} & \textbf{Model} & \textbf{Human} & \textbf{Reasoning} \\
% \midrule
% India Exports to Bahrain \$450M... Total exports just 1.5B, India GDP \$2.7T, so not even 0.1\%... The more they overdo the bigger the hit back on local converts. & GDP & Neutral & Favor & Model treats comparative economic statistics as descriptive data, missing implied national strength endorsement. \\
% \midrule
% 6th March is a Sunday please fill your vehicles full tank wait to see the fuel price on the 8th of March \& thank Modiji to have made you aatmanirbhar & aatmanirbhar & Favor & Against & Sarcasm framed as informational statement. \\
% \midrule
% Vultures of Congress crossing all limit. Now CM @VNarayanasami is giving tribute to Hathras victim using picture of a girl who died 2 years back. & hathras & Against & Neutral & Model anchors on ``Hathras'' aspect even though the stance is on Congress. \\
% \midrule
% Both police \& relatives of Israr killed by brutal beating by a crowd are avoiding to call it `mob lynching' for reasons best known to them... & lynching & Neutral & Against & Model treats descriptive crime reporting as neutral and misses implicit condemnation. \\
% \bottomrule
% \end{tabular}
% \end{center}

% \begin{table*}[t]
% \centering
% \tiny
% \caption{Representative Examples of Model Misclassification}
% \label{tab:misclassification_examples}
% \begin{tabular}{p{8.5cm}p{1.8cm}ccp{5.5cm}}
% \toprule
% \textbf{Tweet} & \textbf{Aspect} & \textbf{Model} & \textbf{Human} & \textbf{Reasoning} \\
% \midrule
% India Exports to Bahrain \$450M... Total exports just 1.5B, India GDP \$2.7T, so not even 0.1\%... The more they overdo the bigger the hit back on local converts. &
% GDP & Neutral & Favor &
% Model treats comparative economic statistics as descriptive data, missing implied national strength endorsement. \\
% \midrule
% 6th March is a Sunday please fill your vehicles full tank wait to see the fuel price on the 8th of March \& thank Modiji to have made you aatmanirbhar &
% aatmanirbhar & Favor & Against &
% Sarcasm framed as informational statement. \\
% \midrule
% Vultures of Congress crossing all limit. Now CM @VNarayanasami is giving tribute to Hathras victim using picture of a girl who died 2 years back. &
% hathras & Against & Neutral &
% Model anchors on ``Hathras'' aspect even though the stance is on Congress. \\
% \midrule
% Both police \& relatives of Israr killed by brutal beating by a crowd are avoiding to call it `mob lynching' for reasons best known to them... &
% lynching & Neutral & Against &
% Model treats descriptive crime reporting as neutral and misses implicit condemnation. \\
% \bottomrule
% \end{tabular}
% \end{table*}

}

\section{Discussion}
Our study of social media discourse around digital influencers shows very strong evidence that Indian influencers are significantly partisan on issues of political polarization. 

Using a retweet-based polarity score to classify 960 influencers, the study found that 85.7% of the political topics analyzed showed a statistically significant correlation between an influencer's leaning and their expressed stance. This polarization is most acute regarding high-volume, contentious issues like "Modi" (94.4% classification accuracy) and "CAA" (89.5% per cent, where influencers strictly mirror official party narratives to maximize engagement within their respective echo chambers.



%Our metric of measuring the political leaning of influencers through the volume of political actors retweeting them reveals two distinct communities/poles of influencers, one in favor of the ruling dispensation and the other against it.
At the aspect level, we identify three distinct categories based on the proportion of influencers with a given political polarity engaging with them: (A) aspects predominantly favored by pro-ruling influencers and opposed by pro-opposition influencers; (B) aspects predominantly favored by pro-opposition influencers and opposed by pro-ruling influencers; and (C) relatively neutral aspects. Notably, neutral aspects receive substantially less influencer attention in terms of tweet volume, whereas aspects leaning toward either political side attract significantly greater engagement. This pattern reflects \textit{attention economy dynamics}, a broader tendency among influencers to focus on contentious issues in order to maximize visibility and sustain relevance.

A deeper look at the aspects through stance analysis reveals a highly partisan and dichotomous nature of influencer discourse. For political entities such as \textit{Modi} (the Prime Minister) or \textit{Rahul Gandhi} (the leader of the largest opposition party INC), influencers closely adhere to the official party stance of their preferred political parties, an expected pattern given partisan alignment. However, we also find that ostensibly generic topics of discussion exhibit pronounced partisan alignment. For instance, the pro-ruling influencers disproportionately amplify issues such as \textit{Hindutva}, and \textit{Ram Mandir}, while consistently opposing aspects like \textit{Farmers’ Protests} and {\textit{Muslims}. Notably, these positions closely mirror the official stances of the political parties they most frequently endorse -- the ruling party has, in recent years, gained substantial political traction by foregrounding religious and cultural issues as central themes of its mobilization and electoral strategy, while also publicly criticizing Farmers’ Protests. Conversely, pro-opposition influencers exhibit the opposite pattern across the same aspects, once again closely mirroring the official positions of the political parties they align with. Relatively neutral aspects, like \textit{Democracy, Minorities}, and \textit{Kashmir} exhibit limited divergence in the stances expressed by influencers across both political sides. These are topics where the discourse tends to be mixed and largely diplomatic, rather than overtly polarized. However, even for these seemingly neutral aspects, the influencers are seen to kowtow party lines. 

We also see that these trends are uniformly observed for both English and Hindi tweets, indicative of the fact that the observed polarization holds across languages. In a country where a significant fraction of the social media population consumes tweets in Hindi \cite{ETTweetingInHindi2018}, this is a sign of a polarized political discourse cutting across linguistic borders. With the recent trend of increasing polarization in India \cite{dash2022divided}, this especially does not augur well. Influencers are highly popular figures, many of whom are revered and closely followed on social media by numerous users. A bipolar and partisan nature of the influencer discourse amplifies the extant biases in the society, thereby leading to a fragmented democracy. 
%The topical analysis of tweets reveals ...

This work comes as a timely intervention around influencer polarization in India, provided that several recent studies have targeted similar research questions. Our work acts as a formative step towards large-scale analysis of influencer discourse on social media (from 2020-2023). A major contribution of this study is the development of a generalizable research framework for nuanced stance analysis on social media. While we study influencer discourse in India corresponding to two languages, the framework is applicable to any geography and can be extended to work on other languages as well. The use of an SLM to perform stance analysis also highlights the need to incorporate low-infrastructure methods to perform large-scale data analysis.

As part of future work, we intend to extend the study to other regional languages in India, to see if similar trends are observed. Furthermore, while the current work focuses on an overall analysis of influencer discourse (since we observed imbalance in tweet counts across years), a temporal study capturing the evolution of influencer leaning and discourse partisanism can be undertaken. Additionally, while our best method for stance analysis (LoRA fine-tuned Mistral-7B) significantly outperforms the baselines, there is significant scope for improvement in the exercise. In this direction, enhancing the training dataset and experimentation with ensemble based stance classification methods might be necessary. Finally, we acknowledge the inherent subjectivity in our manual annotation procedures and qualitative analyses. Further verification and validation of these methods through larger annotation efforts, multiple independent annotators, and alternative operationalizations constitute important directions for future work.

\section{Conclusion}
We study political discourse of Indian influencers on X to understand if they exhibit partisanship with respect to various social and political issues. For this purpose, we propose a framework for influencer discourse analysis, relying on a mixed-methods approach. With the help of a political science expert, we first identify a set of polarizing aspects that structure bipolar political discourse. We then propose a proxy for influencer polarity, based on endorsement signals from national politicians as reflected in retweets. Finally, we develop an SLM-based stance classification pipeline to measure influencers’ stances toward the identified aspects.

Our analysis provides strong empirical evidence of two distinct communities of influencers -- pro-ruling and pro-opposition. We also see that a majority of these influencers exhibit pronounced partisanship in their engagement with these issues, leading to a bipolar nature of influencer discourse. These findings highlight the influential role of content creators in shaping and reinforcing polarized political discourse on social media platforms. %Furthermore, polarization at the aggregate level is distinctly dichotomous, revealing the presence of two well-defined communities of influencers—one broadly aligned with the ruling dispensation and the other positioned in opposition. These findings highlight the influential role of content creators in shaping and reinforcing polarized political discourse on social media platforms.
%In this paper, we examined the political discourse of Indian influencers to assess the extent and nature of polarization across a range of social and political issues. To this end, we first proposed a proxy to infer influencers’ political leanings based on the political retweets received in response to their content. We then analyzed the stances expressed in their tweets with respect to different issue-specific aspects, leveraging a small language model fine-tuned on influencer-generated data. Our analysis provides strong empirical evidence that a majority of Indian influencers exhibit pronounced partisanship in their engagement with these issues. Furthermore, polarization at the aggregate level is distinctly dichotomous, revealing the presence of two well-defined communities of influencers—one broadly aligned with the ruling dispensation and the other positioned in opposition. These findings highlight the influential role of content creators in shaping and reinforcing polarized political discourse on social media platforms.


\bibliography{bibliography}
\input{ICWSM_checklist}
\include{appendix}
\end{document}
