\section{Results}
%This section reports the results of our qualitative and quantitative analysis of influencer discourse. The analysis covers 680,847 tweet-aspect pairs across 38 political aspects, with influencers categorized by political polarizaton into Pro-Ruling (462 influencers) and Pro-Opposition (498 influencers) groups.%% 
{\color{blue} This section reports the results of our qualitative and quantitative analysis of influencer discourse. The original corpus comprised 7.1 million retweets from 960 unique influencers across the 2020--2023 period (Table~\ref{tab:dataset_stats}). After aspect-based filtering---extracting only tweets containing the 35 polar aspects identified in the Methodology---the final dataset comprises 457,509 tweet-aspect pairs (303,233 English and 154,276 Hindi). These pairs correspond to 418,233 unique tweets, as a single tweet may reference multiple aspects. The analysis covers 674 influencers, categorized by political polarization into Pro-Ruling (372 influencers) and Pro-Opposition (302 influencers) groups. Influencers labelled as ``neutral'' were excluded from this analysis, as we focus specifically on the discourse patterns of politically aligned influencers.}
\subsection{Model Performance}
We compare the performance of our Mistral-7B based stance classification approaches against the baseline architectures as described in the Methodology. Figure~\ref{fig:model_comparison} illustrates the comparative results across key metrics. The LoRA-tuned Mistral model consistently outperforms the baseline approaches in all of the metrics (accuracy: 78\%, F1 score (weighted): 77.1\%) for the 264 held out test samples, demonstrating superior capability in capturing the nuances of political stance. The confusion matrix presented in Figure~\ref{fig:confusion_matrix} shows a strong diagonal, indicating high precision in correctly identifying the three stance labels. %Notably, the model effectively minimizes confusion between opposing stances (Favor vs. Against), which is critical for accurate polarization analysis. 
We observe that the instances of misclassification are primarily concentrated around the Neutral category, reflecting the inherent ambiguity often present in less explicit partisan expressions. We present our analysis of these misclassifications in the Discussion section.%, followed by RoBERTa. For the current dataset, we find that PyABSA performs the worst and fails to capture the linguistic nuances of the tweets. 
% In the Appendix, we qualitatively compare the performance of the best and worst performing models, and discuss cases where PyABSA fails to perform for stance classification.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{results_eng/model_comparison.png}
  \caption{Performance comparison of the fine-tuned Mistral (LoRA) model against baseline architectures.}
  \label{fig:model_comparison}
\end{figure}
These experiments clearly echo the superior performance of fine-tuned SLMs over the classical transformer based approaches, owing to fine-tuning of the SLM to learn context specific lingusitic attributes. The SLM further outperforms the baseline BERT-based model trained on the same dataset, due to its greater capacity and representational complexity. However, since stance classification is heavily contextual, and the results might vary depending on the data under consideration. We use the best performing LoRA-tuned model for stance classification hereinafter. 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{results_eng/mistral_finetuned_confusion_matrix.png}
  \caption{Confusion matrix for the fine-tuned Mistral model on the held-out test set.}
  \label{fig:confusion_matrix}
\end{figure}
%% Detailed Evaluation Results - Moved from Methodology per reviewer comment (Anirban Sen, 6 Jan)
{\color{blue}
% \subsubsection{Detailed Evaluation Metrics}
% Table~\ref{tab:eval_overall_results} presents the detailed metrics for the best performing model (LoRA-adapted Mistral). The model demonstrates competitive performance, effectively balancing precision and recall across classes.

% \vspace{0.5em}
% \begin{center}
% \footnotesize
% \captionof{table}{Stance Classification Evaluation Results (Mistral LoRA)}
% \label{tab:eval_overall_results}
% \begin{tabular}{lr}
% \toprule
% \textbf{Metric} & \textbf{Value} \\
% \midrule
% Total test samples & 264 \\
% Accuracy & 78.0\% \\
% Precision (macro) & 77.6\% \\
% Recall (macro) & 75.3\% \\
% F1-Score (macro) & 76.1\% \\
% F1-Score (weighted) & 77.7\% \\
% \bottomrule
% \end{tabular}
% \end{center}
% \vspace{0.5em}
% Table~\ref{tab:eval_perclass_results} shows per-class performance for the fine-tuned SLM. The model performs strongest on the ``favor'' class (F1=0.82), followed by ``against'' (F1=0.79). The ``neutral'' class shows lower recall (0.60), reflecting the inherent difficulty of distinguishing neutral stances from weakly expressed opinions. We present a detailed analysis of the misclassifications in the Discussion section.
% \begin{center}
% \footnotesize
% \captionof{table}{Per-Class Classification Performance (Mistral LoRA)}
% \label{tab:eval_perclass_results}
% \begin{tabular}{lrrrr}
% \toprule
% \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Support} \\
% \midrule
% Against & 0.78 & 0.80 & 0.79 & 95 \\
% Favor & 0.79 & 0.86 & 0.82 & 111 \\
% Neutral & 0.76 & 0.60 & 0.67 & 58 \\
% \bottomrule
% \end{tabular}
% \end{center}
% \vspace{0.5em}
Table~\ref{tab:eval_aspect_results} presents the top 10 per-aspect accuracy for the seed aspects, showing that some yield clearer stance signals than others. aspects like \textit{Modi} (name of the Prime Minister of India) achieve the highest accuracy (94.4\%), followed by \textit{CAA} (89.5\%) and \textit{Congress} (88.9\%) (the largest party in opposition), indicating their usage in tweets with clear stance signatures.

\vspace{0.5em}
\begin{center}
\footnotesize
\captionof{table}{Per-aspect Classification Accuracy (Top 10)}
\label{tab:eval_aspect_results}
\begin{tabular}{lrr}
\toprule
\textbf{aspect} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
\midrule
modi & 94.4\% & 0.91 \\
caa & 89.5\% & 0.90 \\
congress & 88.9\% & 0.84 \\
hindutva & 85.7\% & 0.84 \\
new\_parliament & 85.7\% & 0.82 \\
rahulgandhi & 81.2\% & 0.82 \\
farmers\_protests & 80.0\% & 0.82 \\
muslim & 78.9\% & 0.79 \\
shaheen\_bagh & 78.9\% & 0.71 \\
china & 73.7\% & 0.64 \\
\bottomrule
\end{tabular}
\end{center}
\vspace{0.5em}
}

%% ============================================================
%% SECTION 5.2: MULTILINGUAL STANCE TRENDS
%% ============================================================
{\color{blue}
To evaluate the generalization capability of the fine-tuned model beyond the seed aspects used during training, we conducted a qualitative accuracy check on 20 additional political aspects (out-of-distribution testing) that were not part of the fine-tuning dataset. For each aspect, we randomly sampled 20 tweets and obtained human annotations (by three independent annotators) for stance classification. The model predictions were then compared against these human annotations to assess performance on unseen political topics.

% Across 390 annotated tweet-aspect pairs spanning 20 aspects, the model achieved an overall accuracy of 73.1\%, with a macro F1-score of 65.3\% and weighted F1-score of 70.9\%. Table~\ref{tab:post_analysis_overall} summarizes the overall performance metrics.
% \begin{center}
% \footnotesize
% \captionof{table}{Post-Analysis Accuracy: Overall Metrics}
% \label{tab:post_analysis_overall}
% \begin{tabular}{lr}
% \toprule
% \textbf{Metric} & \textbf{Value} \\
% \midrule
% Total annotated samples & 390 \\
% Accuracy & 73.1\% \\
% Precision (macro) & 68.9\% \\
% Recall (macro) & 66.2\% \\
% F1-Score (macro) & 65.3\% \\
% F1-Score (weighted) & 70.9\% \\
% \bottomrule
% \end{tabular}
% \end{center}
The per-class analysis reveals that the model performs best on the ``against'' stance (F1=0.83), followed by ``favor'' (F1=0.74), while ``neutral'' classification remains challenging (F1=0.40) due to lower recall (29.4\%), suggesting that neutral stance detection remains inherently difficult across both seen and unseen aspects.
% \begin{center}
% \footnotesize
% \captionof{table}{Post-Analysis Accuracy: Per-Class Performance}
% \label{tab:post_analysis_perclass}
% \begin{tabular}{lrrrr}
% \toprule
% \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Support} \\
% \midrule
% Favor & 0.67 & 0.82 & 0.74 & 102 \\
% Against & 0.79 & 0.87 & 0.83 & 203 \\
% Neutral & 0.61 & 0.29 & 0.40 & 85 \\
% \bottomrule
% \end{tabular}
% \end{center}
Table~\ref{tab:post_analysis_aspects} presents the per-aspect accuracy for the 20 evaluated aspects. Notable high-accuracy aspects include \textit{Bhakts} (95\%), \textit{Islamists} (95\%), \textit{Democracy} (90\%), \textit{Demonetisation} (90\%), and \textit{Suicides} (90\%). These aspects tend to have clearer partisan framing patterns that the model successfully captures. Conversely, aspects like \textit{MSP} (30\%), \textit{Minorities} (50\%), and \textit{Lynching} (55\%) show lower accuracy, likely due to more nuanced or contested discourse patterns that differ from the training distribution.
\begin{center}
\scriptsize
\captionof{table}{Post-Analysis Accuracy: Per-aspect Performance}
\label{tab:post_analysis_aspects}
\begin{tabular}{lrrr}
\toprule
\textbf{aspect} & \textbf{Samples} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
\midrule
Bhakts & 20 & 95.0\% & 0.65 \\
Islamists & 20 & 95.0\% & 0.33 \\
Democracy & 20 & 90.0\% & 0.61 \\
Demonetisation & 20 & 90.0\% & 0.62 \\
Suicides & 20 & 90.0\% & 0.80 \\
Aatmanirbhar Bharat & 21 & 85.7\% & 0.84 \\
GDP & 20 & 80.0\% & 0.72 \\
Dictatorship & 19 & 78.9\% & 0.43 \\
Inflation & 20 & 75.0\% & 0.66 \\
Ayodhya & 20 & 70.0\% & 0.69 \\
Mahotsav & 20 & 70.0\% & 0.69 \\
Sangh & 20 & 70.0\% & 0.59 \\
Sharia & 20 & 70.0\% & 0.43 \\
Spyware & 20 & 70.0\% & 0.62 \\
Unemployment & 20 & 70.0\% & 0.57 \\
Hathras & 20 & 65.0\% & 0.41 \\
Lynching & 20 & 55.0\% & 0.36 \\
Balochistan & 10 & 50.0\% & 0.24 \\
Minorities & 20 & 50.0\% & 0.46 \\
MSP & 20 & 30.0\% & 0.31 \\
\bottomrule
\end{tabular}
\end{center}
The results demonstrate that while the fine-tuned model generalizes reasonably well to unseen political aspects (73.1\% overall accuracy), performance varies significantly across aspects. Aspects with clearer ideological positioning and less ambiguous framing tend to yield higher accuracy, while aspects involving contested narratives or requiring deeper contextual understanding show reduced performance. This suggests potential avenues for further model improvement through targeted data augmentation.}
\subsection{Partisanship in Influencer Discourse}
Figure~\ref{fig:multilingual_stack} presents stacked stance distribution plots for English and Hindi tweets, for the different aspects considered. We discuss in this section the results for a few example aspects across the polarity spectrum, and present the plots for the rest of the 29 aspects in the appendix.%-- while some of these aspects are highly polar (e.g., Hindutva), some others are relatively less polar (e.g., Democracy), when it comes to influencer discourse as can be seen from table XXX.
The visualization shows three distinct bands/rows of aspects. The top row highlights aspects that the ruling dispensation disproportionately favors in public discourse, while the opposition generally criticizes  (\textit{Hindutva, Modi, etc.}). The middle row represents a "mixed" zone where both the establishment and the opposition show relatively similar stance distributions, indicating areas of shared narrative or lower polarization. The bottom row serves as the counterpart to the first, showcasing aspects that the opposition disproportionately favors and the ruling party generally criticizes (e.g., Rahul Gandhi, Farmer's Protests, etc.). 

A few striking trends emerge. We see that the influencer tweets are significantly partisan and highly correlated to influencer polarity, when it comes to discussion on aspects leaning towards a certain party. As can be seen from the plot, aspects like \textit{Hindutva, Modi,} and \textit{Ram Mandir} are reported with an overwhelmingly positive stance by pro-ruling influencers (and negatively by pro-opposition influencers). Notably, these aspects are also prominently and favorably framed by the ruling establishment across mainstream media and broader public discourse [ref], while being criticized by the opposition.

On the other hand, aspects like \textit{Rahul Gandhi, Muslims, and Farmers' Protests} are reported with a disproportionately negative stance by pro-ruling influencers (and positively by pro-opposition). Once again, the trends closely follow the public discourse around these topics by the political parties the influencers favor [ref].

We assess the statistical significance of the differences between the proportions of tweets expressing favorable and opposing stances for each aspect. An XXX test yields XXX values in the range [XXX–XXX], indicating a high level of statistical significance. The differences between favoring and opposing stances for pro-ruling and pro-opposition groups are also statistically significant, with values in the range [XXX–XXX].


{\color{blue}
\subsubsection{Significance Testing: Binomial GLM Analysis}
To rigorously test whether influencer political alignment significantly predicts the likelihood of expressing a favorable stance toward each aspect, we employ a binomial Generalized Linear Model (GLM) with a logit link function. For each aspect $k$, we model the probability $p_i$ that influencer $i$ tweets favorably as:
\begin{equation}
\text{logit}(p_i) = \beta_0 + \beta_1 \cdot \text{Alignment}_i
\end{equation}
where $\text{Alignment}_i = 0$ for pro-ruling influencers and $\text{Alignment}_i = 1$ for pro-opposition influencers. The response variable is the binomial count $[n_{\text{favor}}, n_{\text{total}} - n_{\text{favor}}]$ per influencer, representing the number of favor tweets out of total tweets for that aspect. A significant $\beta_1$ coefficient ($p < 0.05$) indicates that influencer alignment is a significant predictor of favorable stance expression.

Across 36 aspects tested, 31 (86.1\%) yield statistically significant results ($p < 0.05$), confirming that influencer political alignment is a robust predictor of stance across most political topics. The odds ratios, calculated as $e^{|\beta_1|}$, quantify the magnitude of this effect. Aspects exhibiting the strongest pro-ruling bias include \textit{New Parliament} (OR = 40.76, $p < 10^{-41}$), \textit{Demonetisation} (OR = 34.62, $p < 10^{-36}$), \textit{Aatmanirbhar} (OR = 28.50, $p < 10^{-37}$), and \textit{Ram Mandir} (OR = 25.88, $p < 10^{-84}$). Conversely, aspects with the strongest pro-opposition bias include \textit{Farmers Protests} (OR = 15.78, $p < 10^{-51}$), \textit{Muslim} (OR = 12.07, $p \approx 0$), \textit{Rahul Gandhi} (OR = 10.79, $p \approx 0$), and \textit{Shaheen Bagh} (OR = 10.55, $p < 10^{-70}$). Only five aspects—Suicides, Lynching, Dictatorship, Sharia, and Islamists—show no significant alignment effect, likely due to insufficient sample sizes or low variation in stance across political groups.

Figure~\ref{fig:forest_plot} presents a forest plot visualization of these results. The x-axis displays odds ratios on a logarithmic scale, with the vertical line at OR = 1 representing no effect. Points to the left indicate aspects where pro-ruling influencers are more likely to favor (red), while points to the right indicate pro-opposition favor (blue). Horizontal lines represent 95\% confidence intervals. The plot reveals a clear bimodal distribution: aspects cluster either strongly to the left (pro-ruling favor) or strongly to the right (pro-opposition favor), with very few aspects near the center. This visual pattern reinforces the quantitative finding that Indian political influencer discourse is deeply polarized, with stance expression on most political topics being predictable from influencer alignment alone.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{results_eng/1_forest_plot_odds_ratio.png}
  \caption{Forest plot showing odds ratios with 95\% confidence intervals for the effect of influencer alignment on favorable stance expression. Red points indicate aspects where pro-ruling influencers are significantly more likely to favor; blue points indicate pro-opposition favor; gray points are non-significant.}
  \label{fig:forest_plot}
\end{figure}
}

Additionally, the stance distribution patterns are remarkably similar across both languages as can be seen from the plot. A statistical significance test yields values in the range [XXX–XXX] for the differences in tweet proportions between English and Hindi across each stance category.This observation suggests that the polarization dynamics and narrative structures deployed by political influencers are consistent, irrespective of the linguistic medium. We further corroborate this finding by cross-referencing it with a negativity scatter analysis (Figure~\ref{fig:negativity_scatter}).%, which similarly indicated high congruence in the sentiment and negativity levels associated with specific aspects across both languages.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{results_eng/combined_hindi_english_stance_stacked.png}
  \caption{Stacked stance distribution comparison between English and Hindi tweets.}
  \label{fig:multilingual_stack}
\end{figure}

%Given this high degree of congruence between English and Hindi discourse, we appended the two datasets for the remainder of our analysis to provide a more comprehensive view of the political landscape.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{results_eng/6_negativity_scatter.png}
  \caption{Comparison of negativity scores for aspects in English vs. Hindi tweets.}
  \label{fig:negativity_scatter}
\end{figure}
The scatter plot in Figure~\ref{fig:negativity_scatter} plots the average percentage of \textit{against} stance (termed as the \textit{negativity score}) for each aspect in English (X-axis) against its score in Hindi (Y-axis). We observe a strong positive correlation, with most aspects clustering along the diagonal ($y=x$ line). To understand the significance of these results, we calculate the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). For Pro-Ruling aspects ($n=32$ aspects), we obtain: RMSE = 5.11, and MAE = 3.76 percentage points are obtained. For Pro-Opposition aspects ($n=33$ aspects): RMSE = 7.37, and MAE = 4.89 percentage points are obtained. These significantly low error values confirm that the cross-lingual negativity patterns are highly consistent, further supporting the aggregation of English and Hindi datasets for subsequent analysis.

% {\color{blue}
% To quantify the cross-lingual consistency, we measure the perpendicular distance of each data point from the diagonal ($y=x$) line. For a point $(x_i, y_i)$ representing the negativity scores in English and Hindi respectively, the perpendicular distance to the diagonal is given by:
% \[
% d_i = \frac{y_i - x_i}{\sqrt{2}}
% \]
% We then compute the following metrics over all $n$ aspects: (1) \textbf{Root Mean Squared Error (RMSE)}: $\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}d_i^2}$, which captures the average magnitude of deviation, penalizing larger deviations more heavily; (2) \textbf{Mean Absolute Error (MAE)}: $\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|d_i|$, which provides the average absolute deviation from the diagonal; (3) \textbf{Median Absolute Distance}: the median of $|d_i|$, a robust measure less sensitive to outliers; and (4) \textbf{Mean Signed Distance}: $\frac{1}{n}\sum_{i=1}^{n}d_i$, which indicates systematic bias (positive values suggest higher negativity in Hindi, negative values suggest higher negativity in English).

% For Pro-Ruling aspects ($n=32$ aspects), we obtain: RMSE = 5.11 percentage points, MAE = 3.76 pp, Median Absolute Distance = 2.39 pp, and Mean Signed Distance = +1.01 pp (slight Hindi bias). For Pro-Opposition aspects ($n=33$ aspects): RMSE = 7.37 pp, MAE = 4.89 pp, Median Absolute Distance = 2.74 pp, and Mean Signed Distance = $-$1.75 pp (slight English bias). These low error values -- with median deviations under 3 percentage points in both camps -- confirm that the cross-lingual negativity patterns are highly consistent, further supporting the aggregation of English and Hindi datasets for subsequent analysis.
% }

The findings indicate that topics evoking negative sentiment in English discourse tend to trigger similar levels of negativity in Hindi, reinforcing the hypothesis of a unified partisan narrative that transcends language barriers. 
{\color{blue}Similar cross-lingual consistency is observed for favorable (RMSE: 5.68–6.09 pp) and neutral (RMSE: 4.33–5.90 pp) stances, confirming that stance distribution patterns are robust across both languages for all stance categories}
The visual evidence suggests that the emotional valence of political topics is consistent across linguistic communities, further justifying the aggregation of datasets for subsequent analysis.

% Using the combined English and Hindi dataset, we next re-examine the percentages of \textit{favor} stance (normalized favor rates) for each of the 38 aspects. Figure~\ref{fig:butterfly_favor} displays these percentages, with Pro-Ruling favor percentages on the right and Pro-Opposition favor percentages on the left.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.95\linewidth]{results_eng/2_butterfly_favor_normalized.png}
%   \caption{Butterfly chart showing normalized favor rates by aspect (combined English \& Hindi data).}
%   \label{fig:butterfly_favor}
% \end{figure}

% The combined analysis reinforces the bimodal nature of the discourse. We observe a set of aspects where the Pro-Ruling side exhibits near-complete dominance in favorable expression, contrasting sharply with aspects where the Pro-Opposition side dominates. Topics located near the center of the chart continue to exhibit more balanced favor distributions, but the overall trend confirms that most political topics in the Indian context are heavily skewed toward one side's narrative. This polarization is even more pronounced when leveraging the larger, combined dataset, validating the robustness of our initial findings.
\subsection{Aspect-Level Polarization}
Figure~\ref{fig:divergence_scatter} presents the aspect level stance divergence for the combined corpus, for all aspects. This scatter plot maps each aspect based on the difference between percentage of favor tweets (x-axis) and against tweets (y-axis). The size of each bubble represents the number of tweets for a certain aspect. The color gradient represents the ``favor divergence score" -- the difference between percentages of favor and against tweets (green representing high \textit{favor} percentage, and red representing high \textit{against} percentage).
\begin{figure}[H]
  \centering
\includegraphics[width=0.95\linewidth]{results_eng/divergence_scatter_english_hindi_combined.png}
  \caption{aspect-level stance divergence scatter plot for the combined English and Hindi dataset.}
  \label{fig:divergence_scatter}
\end{figure}
The plot clearly highlights the two distinct poles occupied by the different aspects. We find two clusters of highly polar aspects in the first and third quadrants of the plot. These are aspects where one side of influencers (pro-ruling/pro-opposition) is disproportionately in favor. %Outliers in this plot represent topics that generate unique polarization patterns -- for instance, topics where one side is highly favorable while the other is not necessarily hostile but silent, or where both are hostile (bottom-left). 
We also see relatively \textit{neutral} (yellow) aspects in the middle. However, the bubble sizes for these neutral aspects are significantly smaller, indicating their scarcity in influencer discourse. Thus, we see a highly polar aspect level discourse -- specific high-volume topics drive the bulk of the polarization and are discussed in a highly partisan manner, while a "long tail" of insignificantly discussed issues remain relatively contested or neutral. %\footnote{We also present a plot of the variation in favor divergence with aspect size (number of tweets) in the Appendix, which exhibits a similar finding where neutral aspects are seen to clutter near the origin, indicative of their relative insignificance in the influencer discourse.}

%% ============================================================
%% SECTION 5.5: COMPLETE STANCE DISTRIBUTION
%% ============================================================
% \subsection{Complete Stance Distribution by aspect}
% Figure~\ref{fig:stance_by_aspect} presents the full stance distribution for each aspect, disaggregated by political affiliation. Each aspect displays proportions of favor, neutral, and against stances for both Pro-Ruling and Pro-Opposition influencers.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.95\linewidth]{results_eng/4_stance_distribution_by_aspect.png}
%   \caption{Stance distribution by aspect and political affiliation, showing proportions of favor, neutral, and against stances.}
%   \label{fig:stance_by_aspect}
% \end{figure}
% To organize aspects into meaningful thematic categories, we employed Google's Gemini model. The model was provided only with the list of 38 aspects and prompted to create thematic buckets based on its knowledge of Indian political discourse. Table~\ref{tab:stance_composition} presents the resulting aspect categories.
% \begin{center}
% \footnotesize
% \captionof{table}{aspects Grouped by Thematic Category (Generated using Gemini)}
% \label{tab:stance_composition}
% \begin{tabular}{p{2.4cm}p{4.1cm}}
% \toprule
% \textbf{Category} & \textbf{aspects} \\
% \midrule
% Favor-dominant (both) & ayodhya, mahotsav, ucc \\
% Against-dominant (both) & inflation, unemployment, suicides \\
% Split (PR favor, PO against) & modi, ram\_mandir, aatmanirbhar, hindutva, farm\_laws, caa \\
% Split (PO favor, PR against) & rahulgandhi, congress, farmers\_protests, shaheen\_bagh, muslim \\
% Mixed neutral ($>$20\%) & china, gdp, democracy, minorities \\
% \bottomrule
% \end{tabular}
% \end{center}
% {\footnotesize \textit{Note: PR = Pro-Ruling, PO = Pro-Opposition}}
{\color{blue}
\subsubsection{Thematic Stance Analysis}
To perform a thematic analysis of stance, we employed ChatGPT (GPT-4.5\footnote{We also tried Gemini for this exercise, but a manual analysis revealed a slightly better performance of GPT, with clearer aspect definitions.}) with a detailed prompt (appendix) through few-shot learning -- the model was provided with the list of aspects and a sample of tweets for each aspect, and instructed to: (A) read all tweets to understand framing patterns, tone, and narrative structures; (B) create high-level thematic buckets that capture major stance/framing themes relevant to pro-ruling vs. pro-opposition influencer discourse; (C) assign each aspect to one primary bucket with reasoning grounded in observed tweet patterns.

This process yielded nine thematic buckets: (1) \textit{Leader \& Party Contestation} (modi, rahulgandhi, congress); (2) \textit{Institutions, Democracy \& State Accountability} (democracy, dictatorship, spyware, new parliament); (3) \textit{Economy, Development \& Macro-Stewardship} (aatmanirbhar, demonetisation, gdp, inflation, unemployment, suicides); (4) \textit{Agrarian Reform \& Farmer Movement} (farm laws, farmers protests, msp); (5) \textit{Citizenship, Belonging \& Mass Protest Politics} (caa, shaheen bagh); (6) \textit{Majoritarian Ideology \& Hindu Nationalist Mobilization} (hindutva, sangh, bhakts, hindu); (7) \textit{Communal Relations, Minority Rights \& Collective Violence} (minorities, muslim, lynching, sharia, islamists, hathras); (8) \textit{Symbolic Nationhood \& Cultural-Religious Projects} (ayodhya, ram mandir, mahotsav); and (9) \textit{Security, Territory \& Geopolitics} (china, kashmir, balochistan, kashmiri pandits).

Figure~\ref{fig:party_focus_stance} presents the stance analysis for each thematic bucket, for pro-ruling and pro-opposition influencers. Each horizontal bar represents the percentage of a side's total tweets devoted to a given bucket.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{results_eng/party_focus_stance_breakdown.png}
  \caption{Thematic stance analysis: Bar length indicates percentage of total tweets}
  \label{fig:party_focus_stance}
\end{figure}
Several differences emerge between the two camps, in terms of the partisanship exhibited at a theme level. \textit{Leader \& Party Contestation} dominates both groups' discourse, but occupies a larger share for Pro-Opposition (59.3\%) than Pro-Ruling (49.7\%). Pro-Ruling influencers allocate a higher proportion to \textit{Majoritarian Ideology \& Hindu Nationalist Mobilization} (16.8\%) compared to Pro-Opposition (9.3\%). \textit{Symbolic Nationhood \& Cultural-Religious Projects} accounts for 4.5\% of Pro-Ruling discourse but only 0.8\% of Pro-Opposition discourse. Conversely, \textit{Institutions, Democracy \& State Accountability} receives greater attention from Pro-Opposition (4.1\%) than Pro-Ruling (1.9\%). The stance composition within buckets also differs: Pro-Ruling tweets in the \textit{Majoritarian Ideology} and \textit{Leader and Party Contestation} buckets are predominantly favorable (green), while Pro-Opposition tweets in the same buckets are predominantly against (red). A detailed table showing individual thematic bucket definitions, aspect contributions to each bucket, with stance breakdowns and tweet counts, is provided in the Appendix.}
{\color{blue}
\subsection{Misclassification}
We discuss the common sources of misclassification for our best performing model in this section. We found that a common source of error involves implicit evaluation, especially in tweets discussing economic indicators such as GDP, exports, or inflation. Although these tweets appear factual, human annotators interpreted them as favourable because such statistics are commonly used to signal government performance or national strength. The model, however, often treats numeric information as neutral and fails to recognise the intent behind these references. Another frequent issue relates to sarcasm and mockery, especially when expressed through quotation marks, rhetorical framing, or code-mixed language. Tweets invoking ironic references to ``democratic'' governance, written with a mix of Hindi and English or employing historical analogy are frequently misclassified. In these cases, surface-level language appeared neutral or even positive, while the intended stance was oppositional. This highlights the difficulty of capturing pragmatic cues that rely on shared political and cultural context. Misclassification also occurs due to unclear targets of critique. Some tweets mention politically charged events but direct criticism toward secondary actors such as opposition leaders, institutions, or public figures. The model tends to rely on aspect associations and struggles to distinguish between references to an issue and evaluation of a specific actor. Finally, some errors involve descriptive statements with implicit moral judgement, particularly in tweets reporting violence or governance failures. While the language used is factual, human readers interpret these descriptions as critical. The model, by contrast, often classifies them as neutral due to the absence of explicit sentiment markers.
\begin{center}
\tiny
\captionof{table}{Representative Examples of Model Misclassification}
\label{tab:misclassification_examples}
\begin{tabular}{p{5.5cm}p{1.3cm}ccp{3.5cm}}
\toprule
\textbf{Tweet} & \textbf{Aspect} & \textbf{Model} & \textbf{Human} & \textbf{Reasoning} \\
\midrule
India Exports to Bahrain \$450M... Total exports just 1.5B, India GDP \$2.7T, so not even 0.1\%... The more they overdo the bigger the hit back on local converts. & GDP & Neutral & Favor & Model treats comparative economic statistics as descriptive data, missing implied national strength endorsement. \\
\midrule
6th March is a Sunday please fill your vehicles full tank wait to see the fuel price on the 8th of March \& thank Modiji to have made you aatmanirbhar & aatmanirbhar & Favor & Against & Sarcasm framed as informational statement. \\
\midrule
Vultures of Congress crossing all limit. Now CM @VNarayanasami is giving tribute to Hathras victim using picture of a girl who died 2 years back. & hathras & Against & Neutral & Model anchors on ``Hathras'' aspect even though the stance is on Congress. \\
\midrule
Both police \& relatives of Israr killed by brutal beating by a crowd are avoiding to call it `mob lynching' for reasons best known to them... & lynching & Neutral & Against & Model treats descriptive crime reporting as neutral and misses implicit condemnation. \\
\bottomrule
\end{tabular}
\end{center}
}