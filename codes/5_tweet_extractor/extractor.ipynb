{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Tweet Extractor by Keywords\n",
                "\n",
                "This notebook extracts tweets from the main dataset for:\n",
                "1. Keywords from finetuning JSONs (auto-extracted)\n",
                "2. Custom keywords you specify manually\n",
                "\n",
                "Features:\n",
                "- **Primary search**: exact match on `keyword` column\n",
                "- **Fallback search**: case-insensitive text search in tweet content (triggered if primary yields < 300 tweets)\n",
                "- **Extract ALL tweets** for each keyword (no sampling limit)\n",
                "- No duplicate tweets across all extractions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CSV Path: /scratch/ziv_baretto/Research_X/Partisan-Discourse-on-X-English-/final_data/tweets_exploded_by_keyword.csv\n",
                        "JSON Dir: /scratch/ziv_baretto/Research_X/Partisan-Discourse-on-X-English-/codes/4_finetuning/4_a_DataProcessing/data_formatting/jsons\n",
                        "Output Dir: extracted_by_keyword\n",
                        "Min tweets threshold (for fallback): 300\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import re\n",
                "import os\n",
                "from pathlib import Path\n",
                "from functools import reduce\n",
                "import operator\n",
                "\n",
                "# ============================================================================\n",
                "# CONFIGURATION - Edit these values as needed\n",
                "# ============================================================================\n",
                "\n",
                "# Data paths - Updated for server\n",
                "CSV_PATH = \"/scratch/ziv_baretto/Research_X/Partisan-Discourse-on-X-English-/final_data/tweets_exploded_by_keyword.csv\"\n",
                "JSON_DIR = \"/scratch/ziv_baretto/Research_X/Partisan-Discourse-on-X-English-/codes/4_finetuning/4_a_DataProcessing/data_formatting/jsons\"\n",
                "OUT_DIR  = Path(\"extracted_by_keyword\")\n",
                "\n",
                "# Minimum tweets threshold for primary search before using fallback\n",
                "# If primary search yields fewer than this many tweets, fallback search is triggered\n",
                "MIN_TWEETS_THRESHOLD = 300\n",
                "\n",
                "SEED = 42\n",
                "\n",
                "# ============================================================================\n",
                "# CUSTOM KEYWORDS - Add your own keywords here!\n",
                "# Format: [\"keyword1\", \"keyword2\", ...]\n",
                "# Examples:\n",
                "#   CUSTOM_KEYWORDS = [\"demonetization\", \"article 370\", \"nrc\"]\n",
                "# Set to empty [] to skip custom keywords\n",
                "# ============================================================================\n",
                "CUSTOM_KEYWORDS = [\n",
                "    # Add your custom keywords here:\n",
                "    # \"your keyword\",\n",
                "    # \"another keyword\",\n",
                "]\n",
                "\n",
                "# Set to True to include keywords from JSON files, False to skip them\n",
                "USE_JSON_KEYWORDS = True\n",
                "\n",
                "# ============================================================================\n",
                "# Column configuration (usually don't need to change)\n",
                "# ============================================================================\n",
                "POSSIBLE_TWEET_COLS = (\"tweet\", \"text\", \"full_text\", \"content\", \"body\")\n",
                "KEYWORD_COL = \"keyword\"\n",
                "LABEL_COL = \"tweet_label\"\n",
                "TARGETS = [\"pro ruling\", \"pro opposition\"]\n",
                "\n",
                "print(f\"CSV Path: {CSV_PATH}\")\n",
                "print(f\"JSON Dir: {JSON_DIR}\")\n",
                "print(f\"Output Dir: {OUT_DIR}\")\n",
                "print(f\"Min tweets threshold (for fallback): {MIN_TWEETS_THRESHOLD}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "build_keyword_list",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 15 JSON files:\n",
                        "  [JSON] 'caa'\n",
                        "  [JSON] 'china'\n",
                        "  [JSON] 'congress'\n",
                        "  [JSON] 'farm laws'\n",
                        "  [JSON] 'farmers protests'\n",
                        "  [JSON] 'hindu'\n",
                        "  [JSON] 'hindutva'\n",
                        "  [JSON] 'kashmir'\n",
                        "  [JSON] 'kashmiri pandits'\n",
                        "  [JSON] 'modi'\n",
                        "  [JSON] 'muslim'\n",
                        "  [JSON] 'new parliament'\n",
                        "  [JSON] 'rahulgandhi'\n",
                        "  [JSON] 'ram mandir'\n",
                        "  [JSON] 'shaheen bagh'\n",
                        "\n",
                        "============================================================\n",
                        "TOTAL KEYWORDS TO EXTRACT: 15\n",
                        "============================================================\n",
                        "  - 'caa'\n",
                        "  - 'china'\n",
                        "  - 'congress'\n",
                        "  - 'farm laws'\n",
                        "  - 'farmers protests'\n",
                        "  - 'hindu'\n",
                        "  - 'hindutva'\n",
                        "  - 'kashmir'\n",
                        "  - 'kashmiri pandits'\n",
                        "  - 'modi'\n",
                        "  - 'muslim'\n",
                        "  - 'new parliament'\n",
                        "  - 'rahulgandhi'\n",
                        "  - 'ram mandir'\n",
                        "  - 'shaheen bagh'\n"
                    ]
                }
            ],
            "source": [
                "# Build the final keyword list\n",
                "\n",
                "KEYWORDS = []\n",
                "\n",
                "# 1. Add keywords from JSON files if enabled\n",
                "if USE_JSON_KEYWORDS and os.path.exists(JSON_DIR):\n",
                "    json_files = [f for f in os.listdir(JSON_DIR) if f.endswith('.json')]\n",
                "    print(f\"Found {len(json_files)} JSON files:\")\n",
                "    \n",
                "    for jf in sorted(json_files):\n",
                "        match = re.match(r'kyra_(.+)_stance\\.json', jf)\n",
                "        if match:\n",
                "            kw = match.group(1).replace('_', ' ')\n",
                "            KEYWORDS.append(kw)\n",
                "            print(f\"  [JSON] '{kw}'\")\n",
                "else:\n",
                "    print(\"Skipping JSON keywords (USE_JSON_KEYWORDS=False or dir not found)\")\n",
                "\n",
                "# 2. Add custom keywords\n",
                "if CUSTOM_KEYWORDS:\n",
                "    print(f\"\\nAdding {len(CUSTOM_KEYWORDS)} custom keywords:\")\n",
                "    for kw in CUSTOM_KEYWORDS:\n",
                "        KEYWORDS.append(kw.lower().strip())\n",
                "        print(f\"  [CUSTOM] '{kw}'\")\n",
                "\n",
                "# Remove duplicate keywords (keep first occurrence)\n",
                "seen = set()\n",
                "unique_keywords = []\n",
                "for kw in KEYWORDS:\n",
                "    kw_lower = kw.lower().strip()\n",
                "    if kw_lower not in seen:\n",
                "        seen.add(kw_lower)\n",
                "        unique_keywords.append(kw_lower)\n",
                "\n",
                "KEYWORDS = unique_keywords\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"TOTAL KEYWORDS TO EXTRACT: {len(KEYWORDS)}\")\n",
                "print(f\"{'='*60}\")\n",
                "for kw in KEYWORDS:\n",
                "    print(f\"  - '{kw}'\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ---------- Helper functions ----------\n",
                "def _norm_nospace(x):\n",
                "    \"\"\"Lowercase + drop all non-alphanumerics (incl. spaces). Case-insensitive.\"\"\"\n",
                "    if isinstance(x, pd.Series):\n",
                "        return (\n",
                "            x.fillna(\"\")\n",
                "             .astype(str)\n",
                "             .str.lower()  # Case-insensitive: 'RAM' and 'ram' become 'ram'\n",
                "             .str.replace(r\"[^a-z0-9]+\", \"\", regex=True)\n",
                "        )\n",
                "    return re.sub(r\"[^a-z0-9]+\", \"\", str(x).lower())\n",
                "\n",
                "def _phrase_variants(s: str) -> list:\n",
                "    \"\"\"\n",
                "    Support ' or ' and '|' as OR separators inside a keyword/phrase.\n",
                "    Returns the ORIGINAL (lowercased/trimmed) variants.\n",
                "    \"\"\"\n",
                "    raw = str(s).strip()\n",
                "    parts = re.split(r\"\\s+or\\s+|\\|\", raw, flags=re.IGNORECASE)\n",
                "    parts = [p.strip().lower() for p in parts if p.strip()]\n",
                "    return parts if parts else [raw.lower().strip()]\n",
                "\n",
                "def _any_contains_norm(tw_norm_series: pd.Series, raw_phrase: str) -> pd.Series:\n",
                "    \"\"\"\n",
                "    Build a boolean mask: tweet contains ANY normalized variant of raw_phrase.\n",
                "    This is the FALLBACK search - case-insensitive text search.\n",
                "    \n",
                "    Since tw_norm_series is already normalized (lowercased, non-alphanum removed),\n",
                "    both 'RAM' and 'ram' will match 'ram' in the search.\n",
                "    \"\"\"\n",
                "    variants = _phrase_variants(raw_phrase)\n",
                "    variants_norm = [_norm_nospace(v) for v in variants]\n",
                "    masks = [tw_norm_series.str.contains(re.escape(vn), regex=True) for vn in variants_norm]\n",
                "    return reduce(operator.or_, masks) if masks else pd.Series(False, index=tw_norm_series.index)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "load_data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading CSV... (this may take a while for large files)\n",
                        "Loaded 8,346,024 rows\n",
                        "Columns: ['timestamp', 'tweet', 'retweet_author', 'original_author', 'retweet_lc', 'original_lc', 'retweet_party', 'year', 'side', 'polarity_avg', 'label_0_5', 'tweet_label', 'subjects_scored', 'keyword']\n",
                        "Tweet column: tweet\n",
                        "After dedup: 1,079,099 rows (removed 7,266,925 duplicates)\n"
                    ]
                }
            ],
            "source": [
                "# ---------- Load & prep ----------\n",
                "print(\"Loading CSV... (this may take a while for large files)\")\n",
                "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
                "print(f\"Loaded {len(df):,} rows\")\n",
                "print(f\"Columns: {df.columns.tolist()}\")\n",
                "\n",
                "# choose tweet column\n",
                "tweet_col = next((c for c in POSSIBLE_TWEET_COLS if c in df.columns), None)\n",
                "if tweet_col is None:\n",
                "    raise ValueError(f\"Couldn't find a tweet/text column. Tried: {POSSIBLE_TWEET_COLS}.\")\n",
                "print(f\"Tweet column: {tweet_col}\")\n",
                "\n",
                "# stable id\n",
                "id_col = \"source_row\" if \"source_row\" in df.columns else None\n",
                "if id_col is None:\n",
                "    df[\"source_row\"] = df.index\n",
                "    id_col = \"source_row\"\n",
                "\n",
                "# de-dup by tweet text\n",
                "before_dedup = len(df)\n",
                "df = df.drop_duplicates(subset=[tweet_col]).copy()\n",
                "print(f\"After dedup: {len(df):,} rows (removed {before_dedup - len(df):,} duplicates)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "normalize_labels",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Label distribution (before filtering):\n",
                        "_label_norm\n",
                        "pro ruling        540330\n",
                        "pro opposition    335961\n",
                        "other             202808\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "After filtering to TARGETS: 876,291 rows\n",
                        "Normalizing tweet text for fallback search (case-insensitive)...\n",
                        "Done.\n"
                    ]
                }
            ],
            "source": [
                "# normalize labels to TARGETS\n",
                "def normalize_label(x: str) -> str:\n",
                "    if not isinstance(x, str): return \"other\"\n",
                "    s = x.strip().lower()\n",
                "    if re.search(r\"\\bpro[-_\\s]*rul(?:ing)?\\b\", s): return \"pro ruling\"\n",
                "    if re.search(r\"\\bpro[-_\\s]*(opp|opposition)\\b\", s): return \"pro opposition\"\n",
                "    return \"other\"\n",
                "\n",
                "df[\"_label_norm\"] = df[LABEL_COL].apply(normalize_label)\n",
                "print(f\"Label distribution (before filtering):\")\n",
                "print(df[\"_label_norm\"].value_counts())\n",
                "\n",
                "df = df[df[\"_label_norm\"].isin(TARGETS)].copy()\n",
                "print(f\"\\nAfter filtering to TARGETS: {len(df):,} rows\")\n",
                "\n",
                "# lowercase keyword col for primary match\n",
                "if KEYWORD_COL not in df.columns:\n",
                "    raise ValueError(f\"Column '{KEYWORD_COL}' not found. Available: {list(df.columns)[:25]}\")\n",
                "\n",
                "df[\"_kw_lc\"] = df[KEYWORD_COL].astype(str).str.strip().str.lower()\n",
                "\n",
                "# normalized tweet text for fallback search (case-insensitive)\n",
                "print(\"Normalizing tweet text for fallback search (case-insensitive)...\")\n",
                "tw_norm = _norm_nospace(df[tweet_col])\n",
                "print(\"Done.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "extract_function",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Global set to track all used tweet IDs across keywords (no duplicates)\n",
                "GLOBAL_USED_IDS = set()\n",
                "\n",
                "def extract_all_for_keyword(kw_raw: str) -> tuple:\n",
                "    \"\"\"\n",
                "    Extract ALL tweets for a keyword.\n",
                "    \n",
                "    Search strategy:\n",
                "    1. PRIMARY: Exact match on keyword column\n",
                "    2. FALLBACK: Case-insensitive text search in tweet content\n",
                "       (triggered if primary yields < MIN_TWEETS_THRESHOLD tweets)\n",
                "    \n",
                "    Case-insensitivity: 'RAM' and 'ram' are treated as the same.\n",
                "    \n",
                "    Ensures no duplicate tweets across all extractions via GLOBAL_USED_IDS.\n",
                "    \n",
                "    Returns:\n",
                "        (DataFrame of extracted tweets, stats dict)\n",
                "    \"\"\"\n",
                "    global GLOBAL_USED_IDS\n",
                "    \n",
                "    # variants for this bucket (handles 'or' and '|' separators)\n",
                "    kw_variants = _phrase_variants(kw_raw)\n",
                "\n",
                "    # Exclude already-used tweets globally\n",
                "    available_mask = ~df[id_col].isin(GLOBAL_USED_IDS)\n",
                "    available_df = df[available_mask]\n",
                "    available_tw_norm = tw_norm[available_mask]\n",
                "\n",
                "    # PRIMARY pool = keyword column equals any variant (case-insensitive)\n",
                "    pool_primary = available_df[available_df[\"_kw_lc\"].isin(kw_variants)].copy()\n",
                "    primary_count = len(pool_primary)\n",
                "    \n",
                "    # Check if we need fallback search\n",
                "    use_fallback = primary_count < MIN_TWEETS_THRESHOLD\n",
                "    \n",
                "    if use_fallback:\n",
                "        # FALLBACK pool = tweet text contains ANY normalized variant (case-insensitive)\n",
                "        # This is case-insensitive because tw_norm is already lowercased\n",
                "        contains_any = _any_contains_norm(available_tw_norm, kw_raw)\n",
                "        pool_fallback = available_df[contains_any].copy()\n",
                "        \n",
                "        # Combine: primary + fallback (avoiding duplicates)\n",
                "        primary_ids = set(pool_primary[id_col])\n",
                "        pool_fallback_new = pool_fallback[~pool_fallback[id_col].isin(primary_ids)]\n",
                "        \n",
                "        out_kw = pd.concat([pool_primary, pool_fallback_new], axis=0)\n",
                "        fallback_count = len(pool_fallback_new)\n",
                "    else:\n",
                "        out_kw = pool_primary\n",
                "        fallback_count = 0\n",
                "\n",
                "    # Remove duplicates within extraction (just to be safe)\n",
                "    out_kw = out_kw.drop_duplicates(subset=[id_col]).copy()\n",
                "    \n",
                "    # Shuffle for variety\n",
                "    if not out_kw.empty:\n",
                "        out_kw = out_kw.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
                "\n",
                "    # Integrity checks\n",
                "    assert out_kw[id_col].nunique() == len(out_kw), f\"[{kw_raw}] duplicate IDs\"\n",
                "    assert out_kw[tweet_col].nunique() == len(out_kw), f\"[{kw_raw}] duplicate tweets\"\n",
                "\n",
                "    # Add to global used set\n",
                "    GLOBAL_USED_IDS |= set(out_kw[id_col])\n",
                "\n",
                "    # Overwrite keyword column with canonical keyword\n",
                "    canonical = kw_variants[0] if kw_variants else str(kw_raw).strip().lower()\n",
                "    out_kw[KEYWORD_COL] = canonical\n",
                "\n",
                "    # Compute stats per label\n",
                "    stats = {\n",
                "        \"total_extracted\": len(out_kw),\n",
                "        \"from_primary\": primary_count,\n",
                "        \"from_fallback\": fallback_count,\n",
                "        \"used_fallback\": use_fallback,\n",
                "        \"by_label\": {}\n",
                "    }\n",
                "    for label in TARGETS:\n",
                "        label_count = len(out_kw[out_kw[\"_label_norm\"] == label])\n",
                "        stats[\"by_label\"][label] = label_count\n",
                "\n",
                "    return out_kw, stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "run_extraction",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "EXTRACTING ALL TWEETS BY KEYWORD\n",
                        "Minimum threshold for fallback: 300 tweets\n",
                        "================================================================================\n",
                        "[OK] 'caa' [FALLBACK USED]\n",
                        "     -> total: 4297, primary: 184, fallback: 4113\n",
                        "     -> by label: {'pro ruling': 2528, 'pro opposition': 1769}\n",
                        "[OK] 'china' [PRIMARY ONLY]\n",
                        "     -> total: 5479, primary: 5479, fallback: 0\n",
                        "     -> by label: {'pro ruling': 2972, 'pro opposition': 2507}\n",
                        "[OK] 'congress' [PRIMARY ONLY]\n",
                        "     -> total: 8416, primary: 8416, fallback: 0\n",
                        "     -> by label: {'pro ruling': 3790, 'pro opposition': 4626}\n",
                        "[OK] 'farm laws' [FALLBACK USED]\n",
                        "     -> total: 3399, primary: 0, fallback: 3399\n",
                        "     -> by label: {'pro ruling': 1046, 'pro opposition': 2353}\n",
                        "[OK] 'farmers protests' [FALLBACK USED]\n",
                        "     -> total: 912, primary: 0, fallback: 912\n",
                        "     -> by label: {'pro ruling': 251, 'pro opposition': 661}\n",
                        "[OK] 'hindu' [PRIMARY ONLY]\n",
                        "     -> total: 6565, primary: 6565, fallback: 0\n",
                        "     -> by label: {'pro ruling': 5213, 'pro opposition': 1352}\n",
                        "[OK] 'hindutva' [PRIMARY ONLY]\n",
                        "     -> total: 1388, primary: 1388, fallback: 0\n",
                        "     -> by label: {'pro ruling': 666, 'pro opposition': 722}\n",
                        "[OK] 'kashmir' [PRIMARY ONLY]\n",
                        "     -> total: 4478, primary: 4478, fallback: 0\n",
                        "     -> by label: {'pro ruling': 3474, 'pro opposition': 1004}\n",
                        "[OK] 'kashmiri pandits' [FALLBACK USED]\n",
                        "     -> total: 672, primary: 0, fallback: 672\n",
                        "     -> by label: {'pro ruling': 436, 'pro opposition': 236}\n",
                        "[OK] 'modi' [FALLBACK USED]\n",
                        "     -> total: 94593, primary: 52, fallback: 94541\n",
                        "     -> by label: {'pro ruling': 62894, 'pro opposition': 31699}\n",
                        "[OK] 'muslim' [PRIMARY ONLY]\n",
                        "     -> total: 3858, primary: 3858, fallback: 0\n",
                        "     -> by label: {'pro ruling': 2160, 'pro opposition': 1698}\n",
                        "[OK] 'new parliament' [FALLBACK USED]\n",
                        "     -> total: 488, primary: 0, fallback: 488\n",
                        "     -> by label: {'pro ruling': 332, 'pro opposition': 156}\n",
                        "[OK] 'rahulgandhi' [PRIMARY ONLY]\n",
                        "     -> total: 4448, primary: 4448, fallback: 0\n",
                        "     -> by label: {'pro ruling': 936, 'pro opposition': 3512}\n",
                        "[OK] 'ram mandir' [FALLBACK USED]\n",
                        "     -> total: 1235, primary: 0, fallback: 1235\n",
                        "     -> by label: {'pro ruling': 1019, 'pro opposition': 216}\n",
                        "[OK] 'shaheen bagh' [FALLBACK USED]\n",
                        "     -> total: 1083, primary: 0, fallback: 1083\n",
                        "     -> by label: {'pro ruling': 686, 'pro opposition': 397}\n",
                        "\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "# --- Run extraction for all keywords ---\n",
                "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Reset global tracking\n",
                "GLOBAL_USED_IDS = set()\n",
                "\n",
                "combined = []\n",
                "reports = {}\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"EXTRACTING ALL TWEETS BY KEYWORD\")\n",
                "print(f\"Minimum threshold for fallback: {MIN_TWEETS_THRESHOLD} tweets\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for kw in KEYWORDS:\n",
                "    out_kw, stat_kw = extract_all_for_keyword(kw)\n",
                "\n",
                "    combined.append(out_kw)\n",
                "    reports[kw] = stat_kw\n",
                "\n",
                "    # write per-keyword files\n",
                "    cols_out = [id_col, tweet_col, LABEL_COL, \"_label_norm\", KEYWORD_COL, \"subjects_scored\"]\n",
                "    cols_out = [c for c in cols_out if c in out_kw.columns]\n",
                "    canonical_name = _phrase_variants(kw)[0].replace(\" \", \"_\")\n",
                "    out_csv = OUT_DIR / f\"extracted_{canonical_name}.csv\"\n",
                "    out_ids = OUT_DIR / f\"extracted_{canonical_name}_ids.txt\"\n",
                "\n",
                "    out_kw[cols_out].to_csv(out_csv, index=False)\n",
                "    with open(out_ids, \"w\", encoding=\"utf-8\") as f:\n",
                "        for v in out_kw[id_col].tolist():\n",
                "            f.write(f\"{v}\\n\")\n",
                "\n",
                "    # Status with breakdown\n",
                "    fallback_status = \"[FALLBACK USED]\" if stat_kw[\"used_fallback\"] else \"[PRIMARY ONLY]\"\n",
                "    print(f\"[OK] '{kw}' {fallback_status}\")\n",
                "    print(f\"     -> total: {stat_kw['total_extracted']}, primary: {stat_kw['from_primary']}, fallback: {stat_kw['from_fallback']}\")\n",
                "    print(f\"     -> by label: {stat_kw['by_label']}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "save_combined",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[OK] Combined: extracted_by_keyword/extracted_ALL_15keywords_141311rows.csv (rows=141311)\n"
                    ]
                }
            ],
            "source": [
                "# Combined outputs\n",
                "all_out = pd.concat(combined, axis=0).reset_index(drop=True) if combined else pd.DataFrame()\n",
                "cols_out_all = [id_col, tweet_col, LABEL_COL, \"_label_norm\", KEYWORD_COL, \"subjects_scored\"]\n",
                "cols_out_all = [c for c in cols_out_all if c in all_out.columns]\n",
                "\n",
                "total_rows = len(all_out)\n",
                "all_csv = OUT_DIR / f\"extracted_ALL_{len(KEYWORDS)}keywords_{total_rows}rows.csv\"\n",
                "all_ids = OUT_DIR / f\"extracted_ALL_{len(KEYWORDS)}keywords_ids.txt\"\n",
                "\n",
                "all_out[cols_out_all].to_csv(all_csv, index=False)\n",
                "with open(all_ids, \"w\", encoding=\"utf-8\") as f:\n",
                "    for v in all_out[id_col].tolist():\n",
                "        f.write(f\"{v}\\n\")\n",
                "\n",
                "print(f\"[OK] Combined: {all_csv} (rows={total_rows})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "summary",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "================================================================================\n",
                        "SUMMARY\n",
                        "================================================================================\n",
                        "\n",
                        "Min tweets threshold for fallback: 300\n",
                        "(If primary search yields fewer tweets, fallback is triggered)\n",
                        "\n",
                        "  'caa':\n",
                        "      Total extracted: 4297\n",
                        "      From primary (keyword col): 184\n",
                        "      From fallback (text search, case-insensitive): 4113\n",
                        "      Fallback used: YES\n",
                        "      By label: {'pro ruling': 2528, 'pro opposition': 1769}\n",
                        "\n",
                        "  'china':\n",
                        "      Total extracted: 5479\n",
                        "      From primary (keyword col): 5479\n",
                        "      From fallback (text search, case-insensitive): 0\n",
                        "      Fallback used: NO\n",
                        "      By label: {'pro ruling': 2972, 'pro opposition': 2507}\n",
                        "\n",
                        "  'congress':\n",
                        "      Total extracted: 8416\n",
                        "      From primary (keyword col): 8416\n",
                        "      From fallback (text search, case-insensitive): 0\n",
                        "      Fallback used: NO\n",
                        "      By label: {'pro ruling': 3790, 'pro opposition': 4626}\n",
                        "\n",
                        "  'farm laws':\n",
                        "      Total extracted: 3399\n",
                        "      From primary (keyword col): 0\n",
                        "      From fallback (text search, case-insensitive): 3399\n",
                        "      Fallback used: YES\n",
                        "      By label: {'pro ruling': 1046, 'pro opposition': 2353}\n",
                        "\n",
                        "  'farmers protests':\n",
                        "      Total extracted: 912\n",
                        "      From primary (keyword col): 0\n",
                        "      From fallback (text search, case-insensitive): 912\n",
                        "      Fallback used: YES\n",
                        "      By label: {'pro ruling': 251, 'pro opposition': 661}\n",
                        "\n",
                        "  'hindu':\n",
                        "      Total extracted: 6565\n",
                        "      From primary (keyword col): 6565\n",
                        "      From fallback (text search, case-insensitive): 0\n",
                        "      Fallback used: NO\n",
                        "      By label: {'pro ruling': 5213, 'pro opposition': 1352}\n",
                        "\n",
                        "  'hindutva':\n",
                        "      Total extracted: 1388\n",
                        "      From primary (keyword col): 1388\n",
                        "      From fallback (text search, case-insensitive): 0\n",
                        "      Fallback used: NO\n",
                        "      By label: {'pro ruling': 666, 'pro opposition': 722}\n",
                        "\n",
                        "  'kashmir':\n",
                        "      Total extracted: 4478\n",
                        "      From primary (keyword col): 4478\n",
                        "      From fallback (text search, case-insensitive): 0\n",
                        "      Fallback used: NO\n",
                        "      By label: {'pro ruling': 3474, 'pro opposition': 1004}\n",
                        "\n",
                        "  'kashmiri pandits':\n",
                        "      Total extracted: 672\n",
                        "      From primary (keyword col): 0\n",
                        "      From fallback (text search, case-insensitive): 672\n",
                        "      Fallback used: YES\n",
                        "      By label: {'pro ruling': 436, 'pro opposition': 236}\n",
                        "\n",
                        "  'modi':\n",
                        "      Total extracted: 94593\n",
                        "      From primary (keyword col): 52\n",
                        "      From fallback (text search, case-insensitive): 94541\n",
                        "      Fallback used: YES\n",
                        "      By label: {'pro ruling': 62894, 'pro opposition': 31699}\n",
                        "\n",
                        "  'muslim':\n",
                        "      Total extracted: 3858\n",
                        "      From primary (keyword col): 3858\n",
                        "      From fallback (text search, case-insensitive): 0\n",
                        "      Fallback used: NO\n",
                        "      By label: {'pro ruling': 2160, 'pro opposition': 1698}\n",
                        "\n",
                        "  'new parliament':\n",
                        "      Total extracted: 488\n",
                        "      From primary (keyword col): 0\n",
                        "      From fallback (text search, case-insensitive): 488\n",
                        "      Fallback used: YES\n",
                        "      By label: {'pro ruling': 332, 'pro opposition': 156}\n",
                        "\n",
                        "  'rahulgandhi':\n",
                        "      Total extracted: 4448\n",
                        "      From primary (keyword col): 4448\n",
                        "      From fallback (text search, case-insensitive): 0\n",
                        "      Fallback used: NO\n",
                        "      By label: {'pro ruling': 936, 'pro opposition': 3512}\n",
                        "\n",
                        "  'ram mandir':\n",
                        "      Total extracted: 1235\n",
                        "      From primary (keyword col): 0\n",
                        "      From fallback (text search, case-insensitive): 1235\n",
                        "      Fallback used: YES\n",
                        "      By label: {'pro ruling': 1019, 'pro opposition': 216}\n",
                        "\n",
                        "  'shaheen bagh':\n",
                        "      Total extracted: 1083\n",
                        "      From primary (keyword col): 0\n",
                        "      From fallback (text search, case-insensitive): 1083\n",
                        "      Fallback used: YES\n",
                        "      By label: {'pro ruling': 686, 'pro opposition': 397}\n",
                        "\n",
                        "\n",
                        "✅ All files saved to: extracted_by_keyword/\n",
                        "   Total unique tweets extracted: 141,311\n",
                        "   Keywords processed: 15\n"
                    ]
                }
            ],
            "source": [
                "# Summary report\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nMin tweets threshold for fallback: {MIN_TWEETS_THRESHOLD}\")\n",
                "print(\"(If primary search yields fewer tweets, fallback is triggered)\\n\")\n",
                "\n",
                "for kw, stat in reports.items():\n",
                "    fallback_status = \"YES\" if stat[\"used_fallback\"] else \"NO\"\n",
                "    print(f\"  '{kw}':\")\n",
                "    print(f\"      Total extracted: {stat['total_extracted']}\")\n",
                "    print(f\"      From primary (keyword col): {stat['from_primary']}\")\n",
                "    print(f\"      From fallback (text search, case-insensitive): {stat['from_fallback']}\")\n",
                "    print(f\"      Fallback used: {fallback_status}\")\n",
                "    print(f\"      By label: {stat['by_label']}\")\n",
                "    print()\n",
                "\n",
                "print(f\"\\n✅ All files saved to: {OUT_DIR}/\")\n",
                "print(f\"   Total unique tweets extracted: {total_rows:,}\")\n",
                "print(f\"   Keywords processed: {len(KEYWORDS)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
