{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Tweet Extractor by Keywords\n",
                "\n",
                "This notebook extracts tweets from the main dataset for:\n",
                "1. Keywords from finetuning JSONs (auto-extracted)\n",
                "2. Custom keywords you specify manually\n",
                "\n",
                "Features:\n",
                "- Primary search: exact match on `keyword` column\n",
                "- Fallback search: text search in tweet content\n",
                "- No duplicate tweets across all extractions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import re\n",
                "import os\n",
                "from pathlib import Path\n",
                "from functools import reduce\n",
                "import operator\n",
                "\n",
                "# ============================================================================\n",
                "# CONFIGURATION - Edit these values as needed\n",
                "# ============================================================================\n",
                "\n",
                "# Data paths\n",
                "CSV_PATH = \"/Users/ziv/Desktop/Partisan Discourse Documentation/final_data/tweets_exploded_by_keyword.csv\"\n",
                "JSON_DIR = \"/Users/ziv/Desktop/Partisan Discourse Documentation/codes/4_finetuning/data_formatting/jsons\"\n",
                "OUT_DIR  = Path(\"extracted_by_keyword\")\n",
                "\n",
                "# Default N per class (used for JSON keywords)\n",
                "DEFAULT_N_PER_CLASS = 600\n",
                "SEED = 42\n",
                "\n",
                "# ============================================================================\n",
                "# CUSTOM KEYWORDS - Add your own keywords here!\n",
                "# Format: {\"keyword\": n_per_class} or just \"keyword\" (uses DEFAULT_N_PER_CLASS)\n",
                "# Examples:\n",
                "#   CUSTOM_KEYWORDS = {\"demonetization\": 500, \"article 370\": 400, \"nrc\": 300}\n",
                "#   CUSTOM_KEYWORDS = [\"demonetization\", \"article 370\"]  # uses default count\n",
                "# Set to empty {} or [] to skip custom keywords\n",
                "# ============================================================================\n",
                "CUSTOM_KEYWORDS = {\n",
                "    # Add your custom keywords here:\n",
                "    # \"your keyword\": 500,\n",
                "    # \"another keyword\": 300,\n",
                "}\n",
                "\n",
                "# Set to True to include keywords from JSON files, False to skip them\n",
                "USE_JSON_KEYWORDS = True\n",
                "\n",
                "# ============================================================================\n",
                "# Column configuration (usually don't need to change)\n",
                "# ============================================================================\n",
                "POSSIBLE_TWEET_COLS = (\"tweet\", \"text\", \"full_text\", \"content\", \"body\")\n",
                "KEYWORD_COL = \"keyword\"\n",
                "LABEL_COL = \"tweet_label\"\n",
                "TARGETS = [\"pro ruling\", \"pro opposition\"]\n",
                "\n",
                "print(f\"CSV Path: {CSV_PATH}\")\n",
                "print(f\"JSON Dir: {JSON_DIR}\")\n",
                "print(f\"Output Dir: {OUT_DIR}\")\n",
                "print(f\"Default N per class: {DEFAULT_N_PER_CLASS}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_keyword_list",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build the final keyword list with counts\n",
                "# Format: [(keyword, n_per_class), ...]\n",
                "\n",
                "KEYWORDS_WITH_COUNTS = []\n",
                "\n",
                "# 1. Add keywords from JSON files if enabled\n",
                "if USE_JSON_KEYWORDS and os.path.exists(JSON_DIR):\n",
                "    json_files = [f for f in os.listdir(JSON_DIR) if f.endswith('.json')]\n",
                "    print(f\"Found {len(json_files)} JSON files:\")\n",
                "    \n",
                "    for jf in sorted(json_files):\n",
                "        match = re.match(r'kyra_(.+)_stance\\.json', jf)\n",
                "        if match:\n",
                "            kw = match.group(1).replace('_', ' ')\n",
                "            KEYWORDS_WITH_COUNTS.append((kw, DEFAULT_N_PER_CLASS))\n",
                "            print(f\"  [JSON] '{kw}' -> {DEFAULT_N_PER_CLASS} per class\")\n",
                "else:\n",
                "    print(\"Skipping JSON keywords (USE_JSON_KEYWORDS=False or dir not found)\")\n",
                "\n",
                "# 2. Add custom keywords\n",
                "if CUSTOM_KEYWORDS:\n",
                "    print(f\"\\nAdding {len(CUSTOM_KEYWORDS)} custom keywords:\")\n",
                "    if isinstance(CUSTOM_KEYWORDS, dict):\n",
                "        for kw, n in CUSTOM_KEYWORDS.items():\n",
                "            KEYWORDS_WITH_COUNTS.append((kw.lower().strip(), n))\n",
                "            print(f\"  [CUSTOM] '{kw}' -> {n} per class\")\n",
                "    elif isinstance(CUSTOM_KEYWORDS, list):\n",
                "        for kw in CUSTOM_KEYWORDS:\n",
                "            KEYWORDS_WITH_COUNTS.append((kw.lower().strip(), DEFAULT_N_PER_CLASS))\n",
                "            print(f\"  [CUSTOM] '{kw}' -> {DEFAULT_N_PER_CLASS} per class (default)\")\n",
                "\n",
                "# Remove duplicate keywords (keep first occurrence with its count)\n",
                "seen = set()\n",
                "unique_keywords = []\n",
                "for kw, n in KEYWORDS_WITH_COUNTS:\n",
                "    if kw not in seen:\n",
                "        seen.add(kw)\n",
                "        unique_keywords.append((kw, n))\n",
                "\n",
                "KEYWORDS_WITH_COUNTS = unique_keywords\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"TOTAL KEYWORDS TO EXTRACT: {len(KEYWORDS_WITH_COUNTS)}\")\n",
                "print(f\"{'='*60}\")\n",
                "for kw, n in KEYWORDS_WITH_COUNTS:\n",
                "    print(f\"  - '{kw}': {n} per class\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ---------- Helper functions ----------\n",
                "def _norm_nospace(x):\n",
                "    \"\"\"Lowercase + drop all non-alphanumerics (incl. spaces).\"\"\"\n",
                "    if isinstance(x, pd.Series):\n",
                "        return (\n",
                "            x.fillna(\"\")\n",
                "             .astype(str)\n",
                "             .str.lower()\n",
                "             .str.replace(r\"[^a-z0-9]+\", \"\", regex=True)\n",
                "        )\n",
                "    return re.sub(r\"[^a-z0-9]+\", \"\", str(x).lower())\n",
                "\n",
                "def _phrase_variants(s: str) -> list:\n",
                "    \"\"\"\n",
                "    Support ' or ' and '|' as OR separators inside a keyword/phrase.\n",
                "    Returns the ORIGINAL (lowercased/trimmed) variants.\n",
                "    \"\"\"\n",
                "    raw = str(s).strip()\n",
                "    parts = re.split(r\"\\s+or\\s+|\\|\", raw, flags=re.IGNORECASE)\n",
                "    parts = [p.strip().lower() for p in parts if p.strip()]\n",
                "    return parts if parts else [raw.lower().strip()]\n",
                "\n",
                "def _any_contains_norm(tw_norm_series: pd.Series, raw_phrase: str) -> pd.Series:\n",
                "    \"\"\"\n",
                "    Build a boolean mask: tweet contains ANY normalized variant of raw_phrase.\n",
                "    This is the FALLBACK search - searches in tweet text.\n",
                "    \"\"\"\n",
                "    variants = _phrase_variants(raw_phrase)\n",
                "    variants_norm = [_norm_nospace(v) for v in variants]\n",
                "    masks = [tw_norm_series.str.contains(re.escape(vn), regex=True) for vn in variants_norm]\n",
                "    return reduce(operator.or_, masks) if masks else pd.Series(False, index=tw_norm_series.index)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ---------- Load & prep ----------\n",
                "print(\"Loading CSV... (this may take a while for large files)\")\n",
                "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
                "print(f\"Loaded {len(df):,} rows\")\n",
                "print(f\"Columns: {df.columns.tolist()}\")\n",
                "\n",
                "# choose tweet column\n",
                "tweet_col = next((c for c in POSSIBLE_TWEET_COLS if c in df.columns), None)\n",
                "if tweet_col is None:\n",
                "    raise ValueError(f\"Couldn't find a tweet/text column. Tried: {POSSIBLE_TWEET_COLS}.\")\n",
                "print(f\"Tweet column: {tweet_col}\")\n",
                "\n",
                "# stable id\n",
                "id_col = \"source_row\" if \"source_row\" in df.columns else None\n",
                "if id_col is None:\n",
                "    df[\"source_row\"] = df.index\n",
                "    id_col = \"source_row\"\n",
                "\n",
                "# de-dup by tweet text\n",
                "before_dedup = len(df)\n",
                "df = df.drop_duplicates(subset=[tweet_col]).copy()\n",
                "print(f\"After dedup: {len(df):,} rows (removed {before_dedup - len(df):,} duplicates)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "normalize_labels",
            "metadata": {},
            "outputs": [],
            "source": [
                "# normalize labels to TARGETS\n",
                "def normalize_label(x: str) -> str:\n",
                "    if not isinstance(x, str): return \"other\"\n",
                "    s = x.strip().lower()\n",
                "    if re.search(r\"\\bpro[-_\\s]*rul(?:ing)?\\b\", s): return \"pro ruling\"\n",
                "    if re.search(r\"\\bpro[-_\\s]*(opp|opposition)\\b\", s): return \"pro opposition\"\n",
                "    return \"other\"\n",
                "\n",
                "df[\"_label_norm\"] = df[LABEL_COL].apply(normalize_label)\n",
                "print(f\"Label distribution (before filtering):\")\n",
                "print(df[\"_label_norm\"].value_counts())\n",
                "\n",
                "df = df[df[\"_label_norm\"].isin(TARGETS)].copy()\n",
                "print(f\"\\nAfter filtering to TARGETS: {len(df):,} rows\")\n",
                "\n",
                "# lowercase keyword col for primary match\n",
                "if KEYWORD_COL not in df.columns:\n",
                "    raise ValueError(f\"Column '{KEYWORD_COL}' not found. Available: {list(df.columns)[:25]}\")\n",
                "\n",
                "df[\"_kw_lc\"] = df[KEYWORD_COL].astype(str).str.strip().str.lower()\n",
                "\n",
                "# normalized tweet text for fallback search\n",
                "print(\"Normalizing tweet text for fallback search...\")\n",
                "tw_norm = _norm_nospace(df[tweet_col])\n",
                "print(\"Done.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "sample_function",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Global set to track all used tweet IDs across keywords (no duplicates)\n",
                "GLOBAL_USED_IDS = set()\n",
                "\n",
                "def sample_for_keyword(kw_raw: str, n_per_class: int, seed_base: int = 0) -> tuple:\n",
                "    \"\"\"\n",
                "    Extract tweets for a keyword.\n",
                "    \n",
                "    Search strategy:\n",
                "    1. PRIMARY: Exact match on keyword column\n",
                "    2. FALLBACK: Text search in tweet content\n",
                "    \n",
                "    Ensures no duplicate tweets across all extractions via GLOBAL_USED_IDS.\n",
                "    \"\"\"\n",
                "    global GLOBAL_USED_IDS\n",
                "    \n",
                "    # variants for this bucket\n",
                "    kw_variants = _phrase_variants(kw_raw)\n",
                "\n",
                "    # Exclude already-used tweets globally\n",
                "    available_df = df[~df[id_col].isin(GLOBAL_USED_IDS)]\n",
                "    available_tw_norm = tw_norm[~df[id_col].isin(GLOBAL_USED_IDS)]\n",
                "\n",
                "    # PRIMARY pool = keyword column equals any variant\n",
                "    pool_kw = available_df[available_df[\"_kw_lc\"].isin(kw_variants)].copy()\n",
                "\n",
                "    # FALLBACK pool = tweet text contains ANY normalized variant\n",
                "    contains_any = _any_contains_norm(available_tw_norm, kw_raw)\n",
                "    pool_fb = available_df[contains_any].copy()\n",
                "\n",
                "    taken_ids = set()\n",
                "    parts, stats = [], {}\n",
                "\n",
                "    for i, label in enumerate(TARGETS):\n",
                "        # exclude what's already taken in this keyword\n",
                "        kw_cls = pool_kw[(pool_kw[\"_label_norm\"] == label) & (~pool_kw[id_col].isin(taken_ids))]\n",
                "        fb_cls = pool_fb[(pool_fb[\"_label_norm\"] == label) & (~pool_fb[id_col].isin(taken_ids | set(kw_cls[id_col])))]\n",
                "\n",
                "        need = n_per_class\n",
                "        seed_i = SEED + seed_base + i * 13\n",
                "\n",
                "        # PRIMARY: take from keyword column match\n",
                "        got_kw = min(need, len(kw_cls))\n",
                "        part_kw = kw_cls.sample(n=got_kw, random_state=seed_i, replace=False) if got_kw > 0 else kw_cls.head(0)\n",
                "\n",
                "        # FALLBACK: take remaining from text search\n",
                "        need_more = need - got_kw\n",
                "        got_fb = min(need_more, len(fb_cls))\n",
                "        part_fb = fb_cls.sample(n=got_fb, random_state=seed_i + 1, replace=False) if got_fb > 0 else fb_cls.head(0)\n",
                "\n",
                "        pick = pd.concat([part_kw, part_fb], axis=0)\n",
                "\n",
                "        parts.append(pick)\n",
                "        taken_ids |= set(pick[id_col])\n",
                "\n",
                "        stats[label] = {\n",
                "            \"requested\": need,\n",
                "            \"picked_total\": int(len(pick)),\n",
                "            \"from_keyword_col\": int(len(part_kw)),\n",
                "            \"from_fallback_text\": int(len(part_fb)),\n",
                "            \"short_by\": int(max(0, need - len(pick))),\n",
                "        }\n",
                "\n",
                "    out_kw = pd.concat(parts, axis=0)\n",
                "    if not out_kw.empty:\n",
                "        out_kw = out_kw.sample(frac=1.0, random_state=SEED + seed_base).reset_index(drop=True)\n",
                "\n",
                "    # integrity checks\n",
                "    assert out_kw[id_col].nunique() == len(out_kw), f\"[{kw_raw}] duplicate IDs\"\n",
                "    assert out_kw[tweet_col].nunique() == len(out_kw), f\"[{kw_raw}] duplicate tweets\"\n",
                "\n",
                "    # Add to global used set\n",
                "    GLOBAL_USED_IDS |= set(out_kw[id_col])\n",
                "\n",
                "    # Overwrite keyword column with canonical keyword\n",
                "    canonical = kw_variants[0] if kw_variants else str(kw_raw).strip().lower()\n",
                "    out_kw[KEYWORD_COL] = canonical\n",
                "\n",
                "    return out_kw, stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_extraction",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Run extraction for all keywords ---\n",
                "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Reset global tracking\n",
                "GLOBAL_USED_IDS = set()\n",
                "\n",
                "combined = []\n",
                "reports = {}\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"EXTRACTING TWEETS BY KEYWORD\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for idx, (kw, n_per_class) in enumerate(KEYWORDS_WITH_COUNTS):\n",
                "    out_kw, stat_kw = sample_for_keyword(kw, n_per_class, seed_base=idx * 101)\n",
                "\n",
                "    combined.append(out_kw)\n",
                "    reports[kw] = stat_kw\n",
                "\n",
                "    # write per-keyword files\n",
                "    cols_out = [id_col, tweet_col, LABEL_COL, \"_label_norm\", KEYWORD_COL, \"subjects_scored\"]\n",
                "    cols_out = [c for c in cols_out if c in out_kw.columns]\n",
                "    canonical_name = _phrase_variants(kw)[0].replace(\" \", \"_\")\n",
                "    out_csv = OUT_DIR / f\"extracted_{canonical_name}.csv\"\n",
                "    out_ids = OUT_DIR / f\"extracted_{canonical_name}_ids.txt\"\n",
                "\n",
                "    out_kw[cols_out].to_csv(out_csv, index=False)\n",
                "    with open(out_ids, \"w\", encoding=\"utf-8\") as f:\n",
                "        for v in out_kw[id_col].tolist():\n",
                "            f.write(f\"{v}\\n\")\n",
                "\n",
                "    # Status with breakdown\n",
                "    picked = {lbl: stat_kw[lbl][\"picked_total\"] for lbl in TARGETS}\n",
                "    from_kw = {lbl: stat_kw[lbl][\"from_keyword_col\"] for lbl in TARGETS}\n",
                "    from_fb = {lbl: stat_kw[lbl][\"from_fallback_text\"] for lbl in TARGETS}\n",
                "    print(f\"[OK] '{kw}' (req: {n_per_class}/class)\")\n",
                "    print(f\"     -> picked: {picked}, from_keyword_col: {from_kw}, from_fallback: {from_fb}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_combined",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combined outputs\n",
                "all_out = pd.concat(combined, axis=0).reset_index(drop=True) if combined else pd.DataFrame()\n",
                "cols_out_all = [id_col, tweet_col, LABEL_COL, \"_label_norm\", KEYWORD_COL, \"subjects_scored\"]\n",
                "cols_out_all = [c for c in cols_out_all if c in all_out.columns]\n",
                "\n",
                "total_rows = len(all_out)\n",
                "all_csv = OUT_DIR / f\"extracted_ALL_{len(KEYWORDS_WITH_COUNTS)}keywords_{total_rows}rows.csv\"\n",
                "all_ids = OUT_DIR / f\"extracted_ALL_{len(KEYWORDS_WITH_COUNTS)}keywords_ids.txt\"\n",
                "\n",
                "all_out[cols_out_all].to_csv(all_csv, index=False)\n",
                "with open(all_ids, \"w\", encoding=\"utf-8\") as f:\n",
                "    for v in all_out[id_col].tolist():\n",
                "        f.write(f\"{v}\\n\")\n",
                "\n",
                "print(f\"[OK] Combined: {all_csv} (rows={total_rows})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "summary",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary report\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for kw, stat in reports.items():\n",
                "    short = {lbl: s[\"short_by\"] for lbl, s in stat.items()}\n",
                "    picked = {lbl: s[\"picked_total\"] for lbl, s in stat.items()}\n",
                "    total_picked = sum(picked.values())\n",
                "    from_kw_total = sum(s[\"from_keyword_col\"] for s in stat.values())\n",
                "    from_fb_total = sum(s[\"from_fallback_text\"] for s in stat.values())\n",
                "    print(f\"  '{kw}': total={total_picked} (keyword_col={from_kw_total}, fallback={from_fb_total}), short_by={short}\")\n",
                "\n",
                "print(f\"\\nâœ… All files saved to: {OUT_DIR}/\")\n",
                "print(f\"   Total unique tweets extracted: {total_rows:,}\")\n",
                "print(f\"   Keywords processed: {len(KEYWORDS_WITH_COUNTS)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "partenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}