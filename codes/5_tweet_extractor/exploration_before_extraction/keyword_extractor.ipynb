{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Keyword Tweet Extractor (Custom List)\n",
                "\n",
                "This notebook extracts ALL tweets for a specific keyword list.\n",
                "\n",
                "Features:\n",
                "- **Primary search**: exact match on `keyword` column (case-insensitive)\n",
                "- **Text search**: case-insensitive text search in tweet content\n",
                "- **ALWAYS uses BOTH searches** to extract maximum tweets\n",
                "- No duplicate tweets across all extractions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CSV Path: /scratch/ziv_baretto/Research_X/Partisan-Discourse-on-X-English-/final_data/tweets_exploded_by_keyword.csv\n",
                        "Output Dir: extracted_by_keyword\n",
                        "Total keywords: 23\n",
                        "Mode: EXTRACT ALL (primary + text search combined)\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import re\n",
                "import os\n",
                "from pathlib import Path\n",
                "from functools import reduce\n",
                "import operator\n",
                "\n",
                "# ============================================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================================\n",
                "\n",
                "# Data paths\n",
                "CSV_PATH = \"/scratch/ziv_baretto/Research_X/Partisan-Discourse-on-X-English-/final_data/tweets_exploded_by_keyword.csv\"\n",
                "OUT_DIR  = Path(\"extracted_by_keyword\")\n",
                "\n",
                "SEED = 42\n",
                "\n",
                "# ============================================================================\n",
                "# KEYWORD LIST - Your specified keywords\n",
                "# ============================================================================\n",
                "KEYWORDS = [\n",
                "    \"ayodhya\",\n",
                "    \"islamists\",\n",
                "    \"balochistan\",\n",
                "    \"sharia\",\n",
                "    \"sangh\",\n",
                "    \"ucc\",\n",
                "    \"mahotsav\",\n",
                "    \"caa\",\n",
                "    \"aatmanirbhar\",\n",
                "    \"unemployment\",\n",
                "    \"inflation\",\n",
                "    \"minorities\",\n",
                "    \"hathras\",\n",
                "    \"gdp\",\n",
                "    \"msp\",\n",
                "    \"suicides\",\n",
                "    \"lynching\",\n",
                "    \"spyware\",\n",
                "    \"demonetisation\",\n",
                "    \"democracy\",\n",
                "    \"bhakts\",\n",
                "    \"dictatorship\",\n",
                "    \"ratetvdebate\"\n",
                "]\n",
                "\n",
                "# Column configuration\n",
                "POSSIBLE_TWEET_COLS = (\"tweet\", \"text\", \"full_text\", \"content\", \"body\")\n",
                "KEYWORD_COL = \"keyword\"\n",
                "LABEL_COL = \"tweet_label\"\n",
                "TARGETS = [\"pro ruling\", \"pro opposition\"]\n",
                "\n",
                "print(f\"CSV Path: {CSV_PATH}\")\n",
                "print(f\"Output Dir: {OUT_DIR}\")\n",
                "print(f\"Total keywords: {len(KEYWORDS)}\")\n",
                "print(f\"Mode: EXTRACT ALL (primary + text search combined)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ---------- Helper functions ----------\n",
                "def _norm_nospace(x):\n",
                "    \"\"\"Lowercase + drop all non-alphanumerics (incl. spaces). Case-insensitive.\"\"\"\n",
                "    if isinstance(x, pd.Series):\n",
                "        return (\n",
                "            x.fillna(\"\")\n",
                "             .astype(str)\n",
                "             .str.lower()  # Case-insensitive: 'RAM' and 'ram' become 'ram'\n",
                "             .str.replace(r\"[^a-z0-9]+\", \"\", regex=True)\n",
                "        )\n",
                "    return re.sub(r\"[^a-z0-9]+\", \"\", str(x).lower())\n",
                "\n",
                "def _phrase_variants(s: str) -> list:\n",
                "    \"\"\"\n",
                "    Support ' or ' and '|' as OR separators inside a keyword/phrase.\n",
                "    Returns the ORIGINAL (lowercased/trimmed) variants.\n",
                "    \"\"\"\n",
                "    raw = str(s).strip()\n",
                "    parts = re.split(r\"\\s+or\\s+|\\|\", raw, flags=re.IGNORECASE)\n",
                "    parts = [p.strip().lower() for p in parts if p.strip()]\n",
                "    return parts if parts else [raw.lower().strip()]\n",
                "\n",
                "def _any_contains_norm(tw_norm_series: pd.Series, raw_phrase: str) -> pd.Series:\n",
                "    \"\"\"\n",
                "    Build a boolean mask: tweet contains ANY normalized variant of raw_phrase.\n",
                "    Case-insensitive text search.\n",
                "    \n",
                "    Since tw_norm_series is already normalized (lowercased, non-alphanum removed),\n",
                "    both 'RAM' and 'ram' will match 'ram' in the search.\n",
                "    \"\"\"\n",
                "    variants = _phrase_variants(raw_phrase)\n",
                "    variants_norm = [_norm_nospace(v) for v in variants]\n",
                "    masks = [tw_norm_series.str.contains(re.escape(vn), regex=True) for vn in variants_norm]\n",
                "    return reduce(operator.or_, masks) if masks else pd.Series(False, index=tw_norm_series.index)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "load_data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading CSV... (this may take a while for large files)\n",
                        "Loaded 8,346,024 rows\n",
                        "Columns: ['timestamp', 'tweet', 'retweet_author', 'original_author', 'retweet_lc', 'original_lc', 'retweet_party', 'year', 'side', 'polarity_avg', 'label_0_5', 'tweet_label', 'subjects_scored', 'keyword']\n",
                        "Tweet column: tweet\n",
                        "After dedup: 1,079,099 rows (removed 7,266,925 duplicates)\n"
                    ]
                }
            ],
            "source": [
                "# ---------- Load & prep ----------\n",
                "print(\"Loading CSV... (this may take a while for large files)\")\n",
                "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
                "print(f\"Loaded {len(df):,} rows\")\n",
                "print(f\"Columns: {df.columns.tolist()}\")\n",
                "\n",
                "# choose tweet column\n",
                "tweet_col = next((c for c in POSSIBLE_TWEET_COLS if c in df.columns), None)\n",
                "if tweet_col is None:\n",
                "    raise ValueError(f\"Couldn't find a tweet/text column. Tried: {POSSIBLE_TWEET_COLS}.\")\n",
                "print(f\"Tweet column: {tweet_col}\")\n",
                "\n",
                "# stable id\n",
                "id_col = \"source_row\" if \"source_row\" in df.columns else None\n",
                "if id_col is None:\n",
                "    df[\"source_row\"] = df.index\n",
                "    id_col = \"source_row\"\n",
                "\n",
                "# de-dup by tweet text\n",
                "before_dedup = len(df)\n",
                "df = df.drop_duplicates(subset=[tweet_col]).copy()\n",
                "print(f\"After dedup: {len(df):,} rows (removed {before_dedup - len(df):,} duplicates)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "normalize_labels",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Label distribution (before filtering):\n",
                        "_label_norm\n",
                        "pro ruling        540330\n",
                        "pro opposition    335961\n",
                        "other             202808\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "After filtering to TARGETS: 876,291 rows\n",
                        "Normalizing tweet text for text search (case-insensitive)...\n",
                        "Done.\n"
                    ]
                }
            ],
            "source": [
                "# normalize labels to TARGETS\n",
                "def normalize_label(x: str) -> str:\n",
                "    if not isinstance(x, str): return \"other\"\n",
                "    s = x.strip().lower()\n",
                "    if re.search(r\"\\bpro[-_\\s]*rul(?:ing)?\\b\", s): return \"pro ruling\"\n",
                "    if re.search(r\"\\bpro[-_\\s]*(opp|opposition)\\b\", s): return \"pro opposition\"\n",
                "    return \"other\"\n",
                "\n",
                "df[\"_label_norm\"] = df[LABEL_COL].apply(normalize_label)\n",
                "print(f\"Label distribution (before filtering):\")\n",
                "print(df[\"_label_norm\"].value_counts())\n",
                "\n",
                "df = df[df[\"_label_norm\"].isin(TARGETS)].copy()\n",
                "print(f\"\\nAfter filtering to TARGETS: {len(df):,} rows\")\n",
                "\n",
                "# lowercase keyword col for primary match\n",
                "if KEYWORD_COL not in df.columns:\n",
                "    raise ValueError(f\"Column '{KEYWORD_COL}' not found. Available: {list(df.columns)[:25]}\")\n",
                "\n",
                "df[\"_kw_lc\"] = df[KEYWORD_COL].astype(str).str.strip().str.lower()\n",
                "\n",
                "# normalized tweet text for text search (case-insensitive)\n",
                "print(\"Normalizing tweet text for text search (case-insensitive)...\")\n",
                "tw_norm = _norm_nospace(df[tweet_col])\n",
                "print(\"Done.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "extract_function",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Global set to track all used tweet IDs across keywords (no duplicates)\n",
                "GLOBAL_USED_IDS = set()\n",
                "\n",
                "def extract_all_for_keyword(kw_raw: str) -> tuple:\n",
                "    \"\"\"\n",
                "    Extract ALL tweets for a keyword using BOTH search methods:\n",
                "    1. PRIMARY: Exact match on keyword column (case-insensitive)\n",
                "    2. TEXT SEARCH: Case-insensitive text search in tweet content\n",
                "    \n",
                "    ALWAYS combines both methods to get maximum tweets.\n",
                "    \n",
                "    Ensures no duplicate tweets across all extractions via GLOBAL_USED_IDS.\n",
                "    \n",
                "    Returns:\n",
                "        (DataFrame of extracted tweets, stats dict)\n",
                "    \"\"\"\n",
                "    global GLOBAL_USED_IDS\n",
                "    \n",
                "    # variants for this bucket (handles 'or' and '|' separators)\n",
                "    kw_variants = _phrase_variants(kw_raw)\n",
                "\n",
                "    # Exclude already-used tweets globally\n",
                "    available_mask = ~df[id_col].isin(GLOBAL_USED_IDS)\n",
                "    available_df = df[available_mask]\n",
                "    available_tw_norm = tw_norm[available_mask]\n",
                "\n",
                "    # PRIMARY pool = keyword column equals any variant (case-insensitive)\n",
                "    pool_primary = available_df[available_df[\"_kw_lc\"].isin(kw_variants)].copy()\n",
                "    primary_count = len(pool_primary)\n",
                "    \n",
                "    # ALWAYS use text search to get ALL tweets containing the keyword\n",
                "    # TEXT SEARCH pool = tweet text contains ANY normalized variant (case-insensitive)\n",
                "    contains_any = _any_contains_norm(available_tw_norm, kw_raw)\n",
                "    pool_text_search = available_df[contains_any].copy()\n",
                "    \n",
                "    # Combine: primary + text search (avoiding duplicates within this keyword)\n",
                "    primary_ids = set(pool_primary[id_col])\n",
                "    pool_text_new = pool_text_search[~pool_text_search[id_col].isin(primary_ids)]\n",
                "    \n",
                "    out_kw = pd.concat([pool_primary, pool_text_new], axis=0)\n",
                "    text_search_count = len(pool_text_new)\n",
                "\n",
                "    # Remove duplicates within extraction (just to be safe)\n",
                "    out_kw = out_kw.drop_duplicates(subset=[id_col]).copy()\n",
                "    \n",
                "    # Shuffle for variety\n",
                "    if not out_kw.empty:\n",
                "        out_kw = out_kw.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
                "\n",
                "    # Integrity checks\n",
                "    assert out_kw[id_col].nunique() == len(out_kw), f\"[{kw_raw}] duplicate IDs\"\n",
                "    assert out_kw[tweet_col].nunique() == len(out_kw), f\"[{kw_raw}] duplicate tweets\"\n",
                "\n",
                "    # Add to global used set\n",
                "    GLOBAL_USED_IDS |= set(out_kw[id_col])\n",
                "\n",
                "    # Overwrite keyword column with canonical keyword\n",
                "    canonical = kw_variants[0] if kw_variants else str(kw_raw).strip().lower()\n",
                "    out_kw[KEYWORD_COL] = canonical\n",
                "\n",
                "    # Compute stats per label\n",
                "    stats = {\n",
                "        \"total_extracted\": len(out_kw),\n",
                "        \"from_primary\": primary_count,\n",
                "        \"from_text_search\": text_search_count,\n",
                "        \"by_label\": {}\n",
                "    }\n",
                "    for label in TARGETS:\n",
                "        label_count = len(out_kw[out_kw[\"_label_norm\"] == label])\n",
                "        stats[\"by_label\"][label] = label_count\n",
                "\n",
                "    return out_kw, stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "run_extraction",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "EXTRACTING ALL TWEETS BY KEYWORD (PRIMARY + TEXT SEARCH)\n",
                        "Keywords: ['ayodhya', 'islamists', 'balochistan', 'sharia', 'sangh', 'ucc', 'mahotsav', 'caa', 'aatmanirbhar', 'unemployment', 'inflation', 'minorities', 'hathras', 'gdp', 'msp', 'suicides', 'lynching', 'spyware', 'demonetisation', 'democracy', 'bhakts', 'dictatorship', 'ratetvdebate']\n",
                        "================================================================================\n",
                        "[OK] 'ayodhya'\n",
                        "     -> total: 2447, primary: 1181, text_search: 1266\n",
                        "     -> by label: {'pro ruling': 1919, 'pro opposition': 528}\n",
                        "[OK] 'islamists'\n",
                        "     -> total: 1480, primary: 817, text_search: 663\n",
                        "     -> by label: {'pro ruling': 1462, 'pro opposition': 18}\n",
                        "[OK] 'balochistan'\n",
                        "     -> total: 650, primary: 431, text_search: 219\n",
                        "     -> by label: {'pro ruling': 638, 'pro opposition': 12}\n",
                        "[OK] 'sharia'\n",
                        "     -> total: 572, primary: 197, text_search: 375\n",
                        "     -> by label: {'pro ruling': 549, 'pro opposition': 23}\n",
                        "[OK] 'sangh'\n",
                        "     -> total: 4469, primary: 40, text_search: 4429\n",
                        "     -> by label: {'pro ruling': 1740, 'pro opposition': 2729}\n",
                        "[OK] 'ucc'\n",
                        "     -> total: 7656, primary: 56, text_search: 7600\n",
                        "     -> by label: {'pro ruling': 5632, 'pro opposition': 2024}\n",
                        "[OK] 'mahotsav'\n",
                        "     -> total: 5356, primary: 298, text_search: 5058\n",
                        "     -> by label: {'pro ruling': 5264, 'pro opposition': 92}\n",
                        "[OK] 'caa'\n",
                        "     -> total: 4139, primary: 178, text_search: 3961\n",
                        "     -> by label: {'pro ruling': 2399, 'pro opposition': 1740}\n",
                        "[OK] 'aatmanirbhar'\n",
                        "     -> total: 5132, primary: 64, text_search: 5068\n",
                        "     -> by label: {'pro ruling': 5037, 'pro opposition': 95}\n",
                        "[OK] 'unemployment'\n",
                        "     -> total: 1609, primary: 684, text_search: 925\n",
                        "     -> by label: {'pro ruling': 278, 'pro opposition': 1331}\n",
                        "[OK] 'inflation'\n",
                        "     -> total: 1200, primary: 654, text_search: 546\n",
                        "     -> by label: {'pro ruling': 589, 'pro opposition': 611}\n",
                        "[OK] 'minorities'\n",
                        "     -> total: 1451, primary: 183, text_search: 1268\n",
                        "     -> by label: {'pro ruling': 754, 'pro opposition': 697}\n",
                        "[OK] 'hathras'\n",
                        "     -> total: 2070, primary: 215, text_search: 1855\n",
                        "     -> by label: {'pro ruling': 684, 'pro opposition': 1386}\n",
                        "[OK] 'gdp'\n",
                        "     -> total: 1940, primary: 12, text_search: 1928\n",
                        "     -> by label: {'pro ruling': 1002, 'pro opposition': 938}\n",
                        "[OK] 'msp'\n",
                        "     -> total: 5173, primary: 108, text_search: 5065\n",
                        "     -> by label: {'pro ruling': 2950, 'pro opposition': 2223}\n",
                        "[OK] 'suicides'\n",
                        "     -> total: 265, primary: 72, text_search: 193\n",
                        "     -> by label: {'pro ruling': 80, 'pro opposition': 185}\n",
                        "[OK] 'lynching'\n",
                        "     -> total: 1020, primary: 293, text_search: 727\n",
                        "     -> by label: {'pro ruling': 693, 'pro opposition': 327}\n",
                        "[OK] 'spyware'\n",
                        "     -> total: 401, primary: 237, text_search: 164\n",
                        "     -> by label: {'pro ruling': 31, 'pro opposition': 370}\n",
                        "[OK] 'demonetisation'\n",
                        "     -> total: 491, primary: 94, text_search: 397\n",
                        "     -> by label: {'pro ruling': 99, 'pro opposition': 392}\n",
                        "[OK] 'democracy'\n",
                        "     -> total: 7189, primary: 2039, text_search: 5150\n",
                        "     -> by label: {'pro ruling': 2814, 'pro opposition': 4375}\n",
                        "[OK] 'bhakts'\n",
                        "     -> total: 1717, primary: 129, text_search: 1588\n",
                        "     -> by label: {'pro ruling': 315, 'pro opposition': 1402}\n",
                        "[OK] 'dictatorship'\n",
                        "     -> total: 199, primary: 76, text_search: 123\n",
                        "     -> by label: {'pro ruling': 82, 'pro opposition': 117}\n",
                        "[OK] 'ratetvdebate'\n",
                        "     -> total: 115, primary: 40, text_search: 75\n",
                        "     -> by label: {'pro ruling': 0, 'pro opposition': 115}\n",
                        "\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "# --- Run extraction for all keywords ---\n",
                "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Reset global tracking\n",
                "GLOBAL_USED_IDS = set()\n",
                "\n",
                "combined = []\n",
                "reports = {}\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"EXTRACTING ALL TWEETS BY KEYWORD (PRIMARY + TEXT SEARCH)\")\n",
                "print(f\"Keywords: {KEYWORDS}\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for kw in KEYWORDS:\n",
                "    out_kw, stat_kw = extract_all_for_keyword(kw)\n",
                "\n",
                "    combined.append(out_kw)\n",
                "    reports[kw] = stat_kw\n",
                "\n",
                "    # write per-keyword files\n",
                "    cols_out = [id_col, tweet_col, LABEL_COL, \"_label_norm\", KEYWORD_COL, \"subjects_scored\"]\n",
                "    cols_out = [c for c in cols_out if c in out_kw.columns]\n",
                "    canonical_name = _phrase_variants(kw)[0].replace(\" \", \"_\")\n",
                "    out_csv = OUT_DIR / f\"extracted_{canonical_name}.csv\"\n",
                "    out_ids = OUT_DIR / f\"extracted_{canonical_name}_ids.txt\"\n",
                "\n",
                "    out_kw[cols_out].to_csv(out_csv, index=False)\n",
                "    with open(out_ids, \"w\", encoding=\"utf-8\") as f:\n",
                "        for v in out_kw[id_col].tolist():\n",
                "            f.write(f\"{v}\\n\")\n",
                "\n",
                "    # Status with breakdown\n",
                "    print(f\"[OK] '{kw}'\")\n",
                "    print(f\"     -> total: {stat_kw['total_extracted']}, primary: {stat_kw['from_primary']}, text_search: {stat_kw['from_text_search']}\")\n",
                "    print(f\"     -> by label: {stat_kw['by_label']}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "save_combined",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[OK] Combined: extracted_by_keyword/extracted_ALL_23keywords_56741rows.csv (rows=56741)\n"
                    ]
                }
            ],
            "source": [
                "# Combined outputs\n",
                "all_out = pd.concat(combined, axis=0).reset_index(drop=True) if combined else pd.DataFrame()\n",
                "cols_out_all = [id_col, tweet_col, LABEL_COL, \"_label_norm\", KEYWORD_COL, \"subjects_scored\"]\n",
                "cols_out_all = [c for c in cols_out_all if c in all_out.columns]\n",
                "\n",
                "total_rows = len(all_out)\n",
                "all_csv = OUT_DIR / f\"extracted_ALL_{len(KEYWORDS)}keywords_{total_rows}rows.csv\"\n",
                "all_ids = OUT_DIR / f\"extracted_ALL_{len(KEYWORDS)}keywords_ids.txt\"\n",
                "\n",
                "all_out[cols_out_all].to_csv(all_csv, index=False)\n",
                "with open(all_ids, \"w\", encoding=\"utf-8\") as f:\n",
                "    for v in all_out[id_col].tolist():\n",
                "        f.write(f\"{v}\\n\")\n",
                "\n",
                "print(f\"[OK] Combined: {all_csv} (rows={total_rows})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "summary",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "================================================================================\n",
                        "SUMMARY\n",
                        "================================================================================\n",
                        "\n",
                        "Mode: EXTRACT ALL (primary + text search combined)\n",
                        "\n",
                        "       keyword  total  primary  text_search  pro_ruling  pro_opposition\n",
                        "       ayodhya   2447     1181         1266        1919             528\n",
                        "     islamists   1480      817          663        1462              18\n",
                        "   balochistan    650      431          219         638              12\n",
                        "        sharia    572      197          375         549              23\n",
                        "         sangh   4469       40         4429        1740            2729\n",
                        "           ucc   7656       56         7600        5632            2024\n",
                        "      mahotsav   5356      298         5058        5264              92\n",
                        "           caa   4139      178         3961        2399            1740\n",
                        "  aatmanirbhar   5132       64         5068        5037              95\n",
                        "  unemployment   1609      684          925         278            1331\n",
                        "     inflation   1200      654          546         589             611\n",
                        "    minorities   1451      183         1268         754             697\n",
                        "       hathras   2070      215         1855         684            1386\n",
                        "           gdp   1940       12         1928        1002             938\n",
                        "           msp   5173      108         5065        2950            2223\n",
                        "      suicides    265       72          193          80             185\n",
                        "      lynching   1020      293          727         693             327\n",
                        "       spyware    401      237          164          31             370\n",
                        "demonetisation    491       94          397          99             392\n",
                        "     democracy   7189     2039         5150        2814            4375\n",
                        "        bhakts   1717      129         1588         315            1402\n",
                        "  dictatorship    199       76          123          82             117\n",
                        "  ratetvdebate    115       40           75           0             115\n",
                        "\n",
                        "✅ All files saved to: extracted_by_keyword/\n",
                        "   Total unique tweets extracted: 56,741\n",
                        "   Keywords processed: 23\n",
                        "   Summary saved to: extracted_by_keyword/extraction_summary.csv\n"
                    ]
                }
            ],
            "source": [
                "# Summary report\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nMode: EXTRACT ALL (primary + text search combined)\\n\")\n",
                "\n",
                "# Create summary table\n",
                "summary_data = []\n",
                "for kw, stat in reports.items():\n",
                "    summary_data.append({\n",
                "        \"keyword\": kw,\n",
                "        \"total\": stat['total_extracted'],\n",
                "        \"primary\": stat['from_primary'],\n",
                "        \"text_search\": stat['from_text_search'],\n",
                "        \"pro_ruling\": stat['by_label'].get('pro ruling', 0),\n",
                "        \"pro_opposition\": stat['by_label'].get('pro opposition', 0)\n",
                "    })\n",
                "\n",
                "summary_df = pd.DataFrame(summary_data)\n",
                "print(summary_df.to_string(index=False))\n",
                "\n",
                "print(f\"\\n✅ All files saved to: {OUT_DIR}/\")\n",
                "print(f\"   Total unique tweets extracted: {total_rows:,}\")\n",
                "print(f\"   Keywords processed: {len(KEYWORDS)}\")\n",
                "\n",
                "# Save summary to CSV\n",
                "summary_csv = OUT_DIR / \"extraction_summary.csv\"\n",
                "summary_df.to_csv(summary_csv, index=False)\n",
                "print(f\"   Summary saved to: {summary_csv}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
