{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Deduplicate and Replace Repeated Tweets\n",
                "\n",
                "This notebook:\n",
                "1. For each annotation CSV, keeps only 1 instance of repeated tweets\n",
                "2. Replaces removed duplicates with new tweets from the main dataset that match the same keyword\n",
                "3. Saves deduplicated sheets to `deduplicated_sheets/` folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input folder: csv_sheets\n",
                        "Output folder: deduplicated_sheets\n",
                        "Main dataset: /Users/ziv/Desktop/Partisan Discourse Documentation/final_data/tweets_exploded_by_keyword.csv\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "from pathlib import Path\n",
                "import random\n",
                "\n",
                "# Paths\n",
                "CSV_SHEETS_DIR = 'csv_sheets'\n",
                "OUTPUT_DIR = 'deduplicated_sheets'\n",
                "MAIN_DATASET_PATH = '/Users/ziv/Desktop/Partisan Discourse Documentation/final_data/tweets_exploded_by_keyword.csv'\n",
                "\n",
                "# Create output directory if it doesn't exist\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Input folder: {CSV_SHEETS_DIR}\")\n",
                "print(f\"Output folder: {OUTPUT_DIR}\")\n",
                "print(f\"Main dataset: {MAIN_DATASET_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "load_main_dataset",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading main dataset...\n",
                        "Main dataset loaded: 8,346,024 rows\n",
                        "Columns: ['timestamp', 'tweet', 'retweet_author', 'original_author', 'retweet_lc', 'original_lc', 'retweet_party', 'year', 'side', 'polarity_avg', 'label_0_5', 'tweet_label', 'subjects_scored', 'keyword']\n",
                        "\n",
                        "Unique keywords in main dataset: 96,528\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>timestamp</th>\n",
                            "      <th>tweet</th>\n",
                            "      <th>retweet_author</th>\n",
                            "      <th>original_author</th>\n",
                            "      <th>retweet_lc</th>\n",
                            "      <th>original_lc</th>\n",
                            "      <th>retweet_party</th>\n",
                            "      <th>year</th>\n",
                            "      <th>side</th>\n",
                            "      <th>polarity_avg</th>\n",
                            "      <th>label_0_5</th>\n",
                            "      <th>tweet_label</th>\n",
                            "      <th>subjects_scored</th>\n",
                            "      <th>keyword</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>2020-06-18 11:59:57+00:00</td>\n",
                            "      <td>PM ‚Å¶@narendramodi‚Å© to launch Garib Kalyan Rojg...</td>\n",
                            "      <td>BJP4AnN</td>\n",
                            "      <td>PMOIndia</td>\n",
                            "      <td>bjp4ann</td>\n",
                            "      <td>pmoindia</td>\n",
                            "      <td>BJP</td>\n",
                            "      <td>2020.0</td>\n",
                            "      <td>ruling</td>\n",
                            "      <td>0.967184</td>\n",
                            "      <td>Pro Ruling</td>\n",
                            "      <td>Pro Ruling</td>\n",
                            "      <td>[{'text': 'narendramodi', 'score': 0.4989}, {'...</td>\n",
                            "      <td>narendramodi</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>2020-06-18 11:59:57+00:00</td>\n",
                            "      <td>PM ‚Å¶@narendramodi‚Å© to launch Garib Kalyan Rojg...</td>\n",
                            "      <td>BJP4AnN</td>\n",
                            "      <td>PMOIndia</td>\n",
                            "      <td>bjp4ann</td>\n",
                            "      <td>pmoindia</td>\n",
                            "      <td>BJP</td>\n",
                            "      <td>2020.0</td>\n",
                            "      <td>ruling</td>\n",
                            "      <td>0.967184</td>\n",
                            "      <td>Pro Ruling</td>\n",
                            "      <td>Pro Ruling</td>\n",
                            "      <td>[{'text': 'narendramodi', 'score': 0.4989}, {'...</td>\n",
                            "      <td>launch</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>2020-06-18 11:59:57+00:00</td>\n",
                            "      <td>PM ‚Å¶@narendramodi‚Å© to launch Garib Kalyan Rojg...</td>\n",
                            "      <td>BJP4AnN</td>\n",
                            "      <td>PMOIndia</td>\n",
                            "      <td>bjp4ann</td>\n",
                            "      <td>pmoindia</td>\n",
                            "      <td>BJP</td>\n",
                            "      <td>2020.0</td>\n",
                            "      <td>ruling</td>\n",
                            "      <td>0.967184</td>\n",
                            "      <td>Pro Ruling</td>\n",
                            "      <td>Pro Ruling</td>\n",
                            "      <td>[{'text': 'narendramodi', 'score': 0.4989}, {'...</td>\n",
                            "      <td>livelihood</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                   timestamp  \\\n",
                            "0  2020-06-18 11:59:57+00:00   \n",
                            "1  2020-06-18 11:59:57+00:00   \n",
                            "2  2020-06-18 11:59:57+00:00   \n",
                            "\n",
                            "                                               tweet retweet_author  \\\n",
                            "0  PM ‚Å¶@narendramodi‚Å© to launch Garib Kalyan Rojg...        BJP4AnN   \n",
                            "1  PM ‚Å¶@narendramodi‚Å© to launch Garib Kalyan Rojg...        BJP4AnN   \n",
                            "2  PM ‚Å¶@narendramodi‚Å© to launch Garib Kalyan Rojg...        BJP4AnN   \n",
                            "\n",
                            "  original_author retweet_lc original_lc retweet_party    year    side  \\\n",
                            "0        PMOIndia    bjp4ann    pmoindia           BJP  2020.0  ruling   \n",
                            "1        PMOIndia    bjp4ann    pmoindia           BJP  2020.0  ruling   \n",
                            "2        PMOIndia    bjp4ann    pmoindia           BJP  2020.0  ruling   \n",
                            "\n",
                            "   polarity_avg   label_0_5 tweet_label  \\\n",
                            "0      0.967184  Pro Ruling  Pro Ruling   \n",
                            "1      0.967184  Pro Ruling  Pro Ruling   \n",
                            "2      0.967184  Pro Ruling  Pro Ruling   \n",
                            "\n",
                            "                                     subjects_scored       keyword  \n",
                            "0  [{'text': 'narendramodi', 'score': 0.4989}, {'...  narendramodi  \n",
                            "1  [{'text': 'narendramodi', 'score': 0.4989}, {'...        launch  \n",
                            "2  [{'text': 'narendramodi', 'score': 0.4989}, {'...    livelihood  "
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load the main dataset with all tweets\n",
                "print(\"Loading main dataset...\")\n",
                "main_df = pd.read_csv(MAIN_DATASET_PATH, low_memory=False)\n",
                "print(f\"Main dataset loaded: {len(main_df):,} rows\")\n",
                "print(f\"Columns: {main_df.columns.tolist()}\")\n",
                "print(f\"\\nUnique keywords in main dataset: {main_df['keyword'].nunique():,}\")\n",
                "main_df.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "list_csv_files",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 15 CSV files to process:\n",
                        "  - shaheen_bagh.csv\n",
                        "  - modi.csv\n",
                        "  - hindu.csv\n",
                        "  - hindutva.csv\n",
                        "  - congress.csv\n",
                        "  - muslim.csv\n",
                        "  - farm_laws.csv\n",
                        "  - kashmir.csv\n",
                        "  - caa.csv\n",
                        "  - farmers_protests.csv\n",
                        "  - kashmiri_pandits.csv\n",
                        "  - china.csv\n",
                        "  - rahulgandhi.csv\n",
                        "  - ram_mandir.csv\n",
                        "  - new_parliament.csv\n"
                    ]
                }
            ],
            "source": [
                "# List all CSV files in the csv_sheets folder (excluding the summary file)\n",
                "csv_files = [f for f in os.listdir(CSV_SHEETS_DIR) \n",
                "             if f.endswith('.csv') and not f.startswith('_')]\n",
                "\n",
                "print(f\"Found {len(csv_files)} CSV files to process:\")\n",
                "for f in csv_files:\n",
                "    print(f\"  - {f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "analyze_duplicates",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "DUPLICATE ANALYSIS\n",
                        "================================================================================\n",
                        "\n",
                        "üìÑ modi.csv\n",
                        "   Keyword: 'modi'\n",
                        "   Total rows: 150, Unique tweets: 146\n",
                        "   üîÑ Duplicates to replace: 4\n",
                        "\n",
                        "üìÑ hindutva.csv\n",
                        "   Keyword: 'hindutva'\n",
                        "   Total rows: 120, Unique tweets: 116\n",
                        "   üîÑ Duplicates to replace: 4\n",
                        "\n",
                        "üìÑ congress.csv\n",
                        "   Keyword: 'congress'\n",
                        "   Total rows: 120, Unique tweets: 118\n",
                        "   üîÑ Duplicates to replace: 2\n",
                        "\n",
                        "üìÑ kashmir.csv\n",
                        "   Keyword: 'kashmir'\n",
                        "   Total rows: 120, Unique tweets: 116\n",
                        "   üîÑ Duplicates to replace: 4\n",
                        "\n",
                        "üìÑ farmers_protests.csv\n",
                        "   Keyword: 'farmers protests'\n",
                        "   Total rows: 120, Unique tweets: 102\n",
                        "   üîÑ Duplicates to replace: 18\n",
                        "\n",
                        "üìÑ kashmiri_pandits.csv\n",
                        "   Keyword: 'kashmiri pandits'\n",
                        "   Total rows: 120, Unique tweets: 94\n",
                        "   üîÑ Duplicates to replace: 26\n",
                        "\n",
                        "üìÑ china.csv\n",
                        "   Keyword: 'china'\n",
                        "   Total rows: 120, Unique tweets: 119\n",
                        "   üîÑ Duplicates to replace: 1\n",
                        "\n",
                        "üìÑ rahulgandhi.csv\n",
                        "   Keyword: 'rahulgandhi'\n",
                        "   Total rows: 120, Unique tweets: 113\n",
                        "   üîÑ Duplicates to replace: 7\n",
                        "\n",
                        "================================================================================\n",
                        "TOTAL DUPLICATES TO REPLACE: 66\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "# First, let's analyze the duplicates in each file\n",
                "print(\"=\" * 80)\n",
                "print(\"DUPLICATE ANALYSIS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "analysis_results = []\n",
                "\n",
                "for csv_file in csv_files:\n",
                "    csv_path = os.path.join(CSV_SHEETS_DIR, csv_file)\n",
                "    df = pd.read_csv(csv_path)\n",
                "    \n",
                "    total_rows = len(df)\n",
                "    unique_tweets = df['tweet'].nunique()\n",
                "    duplicates_to_replace = total_rows - unique_tweets\n",
                "    \n",
                "    # Get the keyword for this sheet (from the 'keyword' or 'matched keyword' column)\n",
                "    if 'matched keyword' in df.columns:\n",
                "        keyword = df['matched keyword'].iloc[0] if len(df) > 0 else 'unknown'\n",
                "    elif 'keyword' in df.columns:\n",
                "        keyword = df['keyword'].iloc[0] if len(df) > 0 else 'unknown'\n",
                "    else:\n",
                "        keyword = csv_file.replace('.csv', '').replace('_', ' ')\n",
                "    \n",
                "    analysis_results.append({\n",
                "        'file': csv_file,\n",
                "        'keyword': keyword,\n",
                "        'total_rows': total_rows,\n",
                "        'unique_tweets': unique_tweets,\n",
                "        'duplicates_to_replace': duplicates_to_replace\n",
                "    })\n",
                "    \n",
                "    if duplicates_to_replace > 0:\n",
                "        print(f\"\\nüìÑ {csv_file}\")\n",
                "        print(f\"   Keyword: '{keyword}'\")\n",
                "        print(f\"   Total rows: {total_rows}, Unique tweets: {unique_tweets}\")\n",
                "        print(f\"   üîÑ Duplicates to replace: {duplicates_to_replace}\")\n",
                "\n",
                "# Show summary\n",
                "analysis_df = pd.DataFrame(analysis_results)\n",
                "total_replacements = analysis_df['duplicates_to_replace'].sum()\n",
                "print(f\"\\n\" + \"=\" * 80)\n",
                "print(f\"TOTAL DUPLICATES TO REPLACE: {total_replacements}\")\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "define_functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_sheet(csv_path, main_df, output_dir):\n",
                "    \"\"\"\n",
                "    Process a single annotation CSV:\n",
                "    1. Remove duplicate tweets (keep first occurrence)\n",
                "    2. Replace removed duplicates with new tweets from main dataset (matching keyword)\n",
                "    3. Save to output directory\n",
                "    \n",
                "    Returns: dict with processing statistics\n",
                "    \"\"\"\n",
                "    # Load the annotation sheet\n",
                "    sheet_df = pd.read_csv(csv_path)\n",
                "    original_count = len(sheet_df)\n",
                "    csv_file = os.path.basename(csv_path)\n",
                "    \n",
                "    # Get the keyword for this sheet\n",
                "    if 'matched keyword' in sheet_df.columns:\n",
                "        keyword = sheet_df['matched keyword'].iloc[0] if len(sheet_df) > 0 else None\n",
                "    elif 'keyword' in sheet_df.columns:\n",
                "        keyword = sheet_df['keyword'].iloc[0] if len(sheet_df) > 0 else None\n",
                "    else:\n",
                "        keyword = csv_file.replace('.csv', '').replace('_', ' ')\n",
                "    \n",
                "    # Step 1: Remove duplicates, keeping first occurrence\n",
                "    deduped_df = sheet_df.drop_duplicates(subset=['tweet'], keep='first').copy()\n",
                "    unique_count = len(deduped_df)\n",
                "    duplicates_removed = original_count - unique_count\n",
                "    \n",
                "    if duplicates_removed == 0:\n",
                "        # No duplicates, just save as is\n",
                "        output_path = os.path.join(output_dir, csv_file)\n",
                "        sheet_df.to_csv(output_path, index=False)\n",
                "        return {\n",
                "            'file': csv_file,\n",
                "            'keyword': keyword,\n",
                "            'original_count': original_count,\n",
                "            'duplicates_removed': 0,\n",
                "            'replacements_added': 0,\n",
                "            'final_count': original_count,\n",
                "            'status': 'No duplicates'\n",
                "        }\n",
                "    \n",
                "    # Step 2: Get existing tweets to exclude from replacements\n",
                "    existing_tweets = set(sheet_df['tweet'].dropna().unique())\n",
                "    \n",
                "    # Step 3: Find replacement tweets from main dataset with matching keyword\n",
                "    # The main dataset has 'keyword' column\n",
                "    matching_df = main_df[main_df['keyword'] == keyword].copy()\n",
                "    \n",
                "    # Exclude tweets already in the sheet\n",
                "    available_df = matching_df[~matching_df['tweet'].isin(existing_tweets)].copy()\n",
                "    \n",
                "    # Step 4: Sample replacement tweets\n",
                "    replacements_needed = duplicates_removed\n",
                "    available_count = len(available_df)\n",
                "    \n",
                "    if available_count < replacements_needed:\n",
                "        print(f\"  ‚ö†Ô∏è Warning: Only {available_count} replacement tweets available for keyword '{keyword}', need {replacements_needed}\")\n",
                "        replacements_to_add = available_count\n",
                "    else:\n",
                "        replacements_to_add = replacements_needed\n",
                "    \n",
                "    if replacements_to_add > 0:\n",
                "        # Sample random replacement tweets\n",
                "        replacement_tweets = available_df.sample(n=replacements_to_add, random_state=42)\n",
                "        \n",
                "        # Create new rows in the annotation format\n",
                "        new_rows = []\n",
                "        for _, row in replacement_tweets.iterrows():\n",
                "            new_row = {\n",
                "                'source_row': '',  # Will be filled manually if needed\n",
                "                'tweet': row['tweet'],\n",
                "                'tweet_label': row.get('tweet_label', ''),\n",
                "                'subjects': '',  # To be annotated\n",
                "                'subjects_scored': row.get('subjects_scored', ''),\n",
                "                'keyword': keyword,\n",
                "                'label_norm': '',  # To be annotated\n",
                "                'matched keyword': keyword,\n",
                "                'stance ': '',  # To be annotated (note the space in column name)\n",
                "                'stance reason': ''  # To be annotated\n",
                "            }\n",
                "            new_rows.append(new_row)\n",
                "        \n",
                "        # Add replacement rows to deduplicated dataframe\n",
                "        replacements_df = pd.DataFrame(new_rows)\n",
                "        final_df = pd.concat([deduped_df, replacements_df], ignore_index=True)\n",
                "    else:\n",
                "        final_df = deduped_df\n",
                "    \n",
                "    # Save to output directory\n",
                "    output_path = os.path.join(output_dir, csv_file)\n",
                "    final_df.to_csv(output_path, index=False)\n",
                "    \n",
                "    return {\n",
                "        'file': csv_file,\n",
                "        'keyword': keyword,\n",
                "        'original_count': original_count,\n",
                "        'duplicates_removed': duplicates_removed,\n",
                "        'replacements_added': replacements_to_add,\n",
                "        'final_count': len(final_df),\n",
                "        'status': 'Processed'\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "process_all",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "PROCESSING ALL SHEETS\n",
                        "================================================================================\n",
                        "\n",
                        "üìÑ Processing: shaheen_bagh.csv\n",
                        "   ‚úì No duplicates found, copied as-is\n",
                        "\n",
                        "üìÑ Processing: modi.csv\n",
                        "   ‚úì Removed 4 duplicates\n",
                        "   ‚úì Added 4 replacement tweets\n",
                        "   ‚úì Final count: 150 rows\n",
                        "\n",
                        "üìÑ Processing: hindu.csv\n",
                        "   ‚úì No duplicates found, copied as-is\n",
                        "\n",
                        "üìÑ Processing: hindutva.csv\n",
                        "   ‚úì Removed 4 duplicates\n",
                        "   ‚úì Added 4 replacement tweets\n",
                        "   ‚úì Final count: 120 rows\n",
                        "\n",
                        "üìÑ Processing: congress.csv\n",
                        "   ‚úì Removed 2 duplicates\n",
                        "   ‚úì Added 2 replacement tweets\n",
                        "   ‚úì Final count: 120 rows\n",
                        "\n",
                        "üìÑ Processing: muslim.csv\n",
                        "   ‚úì No duplicates found, copied as-is\n",
                        "\n",
                        "üìÑ Processing: farm_laws.csv\n",
                        "   ‚úì No duplicates found, copied as-is\n",
                        "\n",
                        "üìÑ Processing: kashmir.csv\n",
                        "   ‚úì Removed 4 duplicates\n",
                        "   ‚úì Added 4 replacement tweets\n",
                        "   ‚úì Final count: 120 rows\n",
                        "\n",
                        "üìÑ Processing: caa.csv\n",
                        "   ‚úì No duplicates found, copied as-is\n",
                        "\n",
                        "üìÑ Processing: farmers_protests.csv\n",
                        "  ‚ö†Ô∏è Warning: Only 0 replacement tweets available for keyword 'farmers protests', need 18\n",
                        "   ‚úì Removed 18 duplicates\n",
                        "   ‚úì Added 0 replacement tweets\n",
                        "   ‚úì Final count: 102 rows\n",
                        "\n",
                        "üìÑ Processing: kashmiri_pandits.csv\n",
                        "  ‚ö†Ô∏è Warning: Only 0 replacement tweets available for keyword 'kashmiri pandits', need 26\n",
                        "   ‚úì Removed 26 duplicates\n",
                        "   ‚úì Added 0 replacement tweets\n",
                        "   ‚úì Final count: 94 rows\n",
                        "\n",
                        "üìÑ Processing: china.csv\n",
                        "   ‚úì Removed 1 duplicates\n",
                        "   ‚úì Added 1 replacement tweets\n",
                        "   ‚úì Final count: 120 rows\n",
                        "\n",
                        "üìÑ Processing: rahulgandhi.csv\n",
                        "   ‚úì Removed 7 duplicates\n",
                        "   ‚úì Added 7 replacement tweets\n",
                        "   ‚úì Final count: 120 rows\n",
                        "\n",
                        "üìÑ Processing: ram_mandir.csv\n",
                        "   ‚úì No duplicates found, copied as-is\n",
                        "\n",
                        "üìÑ Processing: new_parliament.csv\n",
                        "   ‚úì No duplicates found, copied as-is\n",
                        "\n",
                        "================================================================================\n",
                        "PROCESSING COMPLETE\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "# Process all CSV files\n",
                "print(\"=\" * 80)\n",
                "print(\"PROCESSING ALL SHEETS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "results = []\n",
                "\n",
                "for csv_file in csv_files:\n",
                "    csv_path = os.path.join(CSV_SHEETS_DIR, csv_file)\n",
                "    print(f\"\\nüìÑ Processing: {csv_file}\")\n",
                "    \n",
                "    result = process_sheet(csv_path, main_df, OUTPUT_DIR)\n",
                "    results.append(result)\n",
                "    \n",
                "    if result['duplicates_removed'] > 0:\n",
                "        print(f\"   ‚úì Removed {result['duplicates_removed']} duplicates\")\n",
                "        print(f\"   ‚úì Added {result['replacements_added']} replacement tweets\")\n",
                "        print(f\"   ‚úì Final count: {result['final_count']} rows\")\n",
                "    else:\n",
                "        print(f\"   ‚úì No duplicates found, copied as-is\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"PROCESSING COMPLETE\")\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "summary",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üìä SUMMARY TABLE:\n",
                        "                file          keyword  original_count  duplicates_removed  replacements_added  final_count        status\n",
                        "    shaheen_bagh.csv     shaheen bagh             150                   0                   0          150 No duplicates\n",
                        "            modi.csv             modi             150                   4                   4          150     Processed\n",
                        "           hindu.csv            hindu             120                   0                   0          120 No duplicates\n",
                        "        hindutva.csv         hindutva             120                   4                   4          120     Processed\n",
                        "        congress.csv         congress             120                   2                   2          120     Processed\n",
                        "          muslim.csv           muslim             150                   0                   0          150 No duplicates\n",
                        "       farm_laws.csv        farm laws             150                   0                   0          150 No duplicates\n",
                        "         kashmir.csv          kashmir             120                   4                   4          120     Processed\n",
                        "             caa.csv              caa             150                   0                   0          150 No duplicates\n",
                        "farmers_protests.csv farmers protests             120                  18                   0          102     Processed\n",
                        "kashmiri_pandits.csv kashmiri pandits             120                  26                   0           94     Processed\n",
                        "           china.csv            china             120                   1                   1          120     Processed\n",
                        "     rahulgandhi.csv      rahulgandhi             120                   7                   7          120     Processed\n",
                        "      ram_mandir.csv       ram mandir             150                   0                   0          150 No duplicates\n",
                        "  new_parliament.csv   new parliament             150                   0                   0          150 No duplicates\n",
                        "\n",
                        "================================================================================\n",
                        "TOTALS:\n",
                        "  Total files processed: 15\n",
                        "  Total duplicates removed: 66\n",
                        "  Total replacements added: 22\n",
                        "\n",
                        "‚úÖ All deduplicated sheets saved to: deduplicated_sheets/\n"
                    ]
                }
            ],
            "source": [
                "# Summary table\n",
                "results_df = pd.DataFrame(results)\n",
                "print(\"\\nüìä SUMMARY TABLE:\")\n",
                "print(results_df[['file', 'keyword', 'original_count', 'duplicates_removed', 'replacements_added', 'final_count', 'status']].to_string(index=False))\n",
                "\n",
                "# Totals\n",
                "print(f\"\\n\" + \"=\" * 80)\n",
                "print(f\"TOTALS:\")\n",
                "print(f\"  Total files processed: {len(results_df)}\")\n",
                "print(f\"  Total duplicates removed: {results_df['duplicates_removed'].sum()}\")\n",
                "print(f\"  Total replacements added: {results_df['replacements_added'].sum()}\")\n",
                "print(f\"\\n‚úÖ All deduplicated sheets saved to: {OUTPUT_DIR}/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "verify",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "================================================================================\n",
                        "VERIFICATION: Sample of processed file\n",
                        "================================================================================\n",
                        "\n",
                        "File: modi.csv\n",
                        "Total rows: 150\n",
                        "Unique tweets: 150\n",
                        "\n",
                        "Columns: ['source_row', 'tweet', 'tweet_label', '_label_norm', 'keyword', 'subjects', 'subjects_scored', 'STANCE ', 'Reasoning ', 'label_norm', 'matched keyword', 'stance ', 'stance reason']\n",
                        "\n",
                        "Last 5 rows (should include replacement tweets):\n",
                        "                                                 tweet keyword  stance   \\\n",
                        "145  On the sidelines of the @g20org Rome Summit, P...    modi      NaN   \n",
                        "146           Thanku modi ji üôè https://t.co/4pWrUmHBcN    modi      NaN   \n",
                        "147   Modi ji ki Vikas Express https://t.co/U0kdI98oWZ    modi      NaN   \n",
                        "148     Wah Modi ji Wah ...ü§£ü§£ü§£ https://t.co/23QTJPP9SC    modi      NaN   \n",
                        "149     Short Story of Modi ji https://t.co/sik4aKk6gx    modi      NaN   \n",
                        "\n",
                        "     stance reason  \n",
                        "145            NaN  \n",
                        "146            NaN  \n",
                        "147            NaN  \n",
                        "148            NaN  \n",
                        "149            NaN  \n"
                    ]
                }
            ],
            "source": [
                "# Verification: Check one of the processed files\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"VERIFICATION: Sample of processed file\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Check a file that had duplicates\n",
                "files_with_dups = results_df[results_df['duplicates_removed'] > 0]['file'].tolist()\n",
                "if files_with_dups:\n",
                "    sample_file = files_with_dups[0]\n",
                "    sample_path = os.path.join(OUTPUT_DIR, sample_file)\n",
                "    sample_df = pd.read_csv(sample_path)\n",
                "    \n",
                "    print(f\"\\nFile: {sample_file}\")\n",
                "    print(f\"Total rows: {len(sample_df)}\")\n",
                "    print(f\"Unique tweets: {sample_df['tweet'].nunique()}\")\n",
                "    print(f\"\\nColumns: {sample_df.columns.tolist()}\")\n",
                "    \n",
                "    # Show last few rows (the new replacements)\n",
                "    print(f\"\\nLast 5 rows (should include replacement tweets):\")\n",
                "    print(sample_df.tail()[['tweet', 'keyword', 'stance ', 'stance reason']])\n",
                "else:\n",
                "    print(\"No files had duplicates to process.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "save_summary",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Processing summary saved to: deduplicated_sheets/_processing_summary.csv\n"
                    ]
                }
            ],
            "source": [
                "# Save the processing summary\n",
                "summary_path = os.path.join(OUTPUT_DIR, '_processing_summary.csv')\n",
                "results_df.to_csv(summary_path, index=False)\n",
                "print(f\"‚úÖ Processing summary saved to: {summary_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "partenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
